{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Agenda","text":"Topic Due Threads Wednesday 1/7 RD01 ACM Software Engineering Code of Ethics Thu 1/8 Ethics RD03 `git`: Core Concepts of a Repository Fri 1/9 git RD02 What is Source Code Management and `git`? Thu 1/8 git RD04 `git`: Fundamental Subcommands Sat 1/10 git RD00 Course Syllabus Thu 1/8 Course TK00 Docker, VSCode/DevContainers, Copilot, and GitHub Education Setup Fri 1/9 IDE LS00 Foundations of Software Engineering (Stream) Course Friday 1/9 RD05 Documenting Architecture Decisions Sun 1/11 ADR RD06 Review AI-generated Code Sun 1/11 Code Agent TK01 Write ADRs for Package Managers Sun 1/11 ADR LS01 Architectural Design Records (ADRs), Lifelong Learning (Stream) ADR Ethics Learning Monday 1/12 RD07 Branching and Merging Tue 1/13 git RD08 Git Collaboration: Working with Remote Repositories Tue 1/13 git TK02 Implement a Dependency Manager ADR with an Agent Thu 1/15 ADR Code Agent LS02 Dependency Managers, LLMs, and Agentic IDEs Code Review Code Agent Wednesday 1/14 TK03          `git` Diagram Practice          Thu 1/15 git LS03 Conceptually Understanding Common `git` Operations (Stream) git Friday 1/16 LS04 Testing Foundations: From Requirements to Automation (Stream) Testing Testing Wednesday 1/21 TK04 Professionalizing the Developer Environment Tue 1/27 ADR Code Agent QZ00 Unit 0 Quiz Course Friday 1/23 RD11 Human Communication and API Design: A Shared Foundation Tue 1/27 APIs RD09 Communicating in the Software Development Lifecycle Tue 1/27 APIs Communication RD10 Advancements in Communication between Computing Systems Tue 1/27 APIs LS05 Designing with Layers and Composition (Stream) APIs Testing Monday 1/26 SD00          Snow Day - Class Cancelled          Course Wednesday 1/28 RD12 MagicMock Documentation Thu 1/29 Testing LS06 Arrange-Act-Assert Testing and Mocking (Stream) Testing Friday 1/30 RD13 How AI Impacts Skill Formation Sun 2/1 Course Code Agent LS07 Testing Concepts: Patching and Fixtures (Stream) Testing Monday 2/2 TK05 Unit and Integration Testing Exercise Fri 2/6 Testing SD01          Snow Day - Class Cancelled          Course Wednesday 2/4 LS08 Testing Practice and Exercises (Stream) Testing Friday 2/6 LS09 Pydantic (Stream) Testing Wednesday 2/11 RD16 FastAPI and Pydantic Tutorial Sun 2/15 APIs RD14 Key Concepts in HTTP Thu 2/12 APIs RD15 Toward Designing and Formally Specifying APIs Thu 2/12 APIs QZ01 Unit 1 Quiz Course Friday 2/13 LS10 HTTP API Design Backend Monday 2/16 RD18 On Pair Programming Tue 2/17 Teamwork RD17 Dependency Injection Tue 2/17 Backend TK06 API Design (Part 1) Thu 2/19 API Design LS11 Dependency Injection Lab Backend Wednesday 2/18 RD19 Unit, Integration, and E2E Testing API Routes Fri 2/20 Backend LS12 API Design Working Day (Stream) Backend Friday 2/20 TK06 API and Service-layer Implementation (Part 2) Mon 2/23 API LS13          Lesson 13          Backend Monday 2/23 LS14          Lesson 14          Backend Wednesday 2/25 QZ02          Unit 2 Quiz          Course Wednesday 3/11 QZ03          Unit 3 Quiz          Course Monday 5/4 FN00          Final &amp; Final Presentations (4pm-7pm)          Course"},{"location":"demo_markdown/","title":"Demo Markdown","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"demo_markdown/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"demo_markdown/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"demo_markdown/#code-snippets","title":"Code Snippets","text":"<p>The following code snippet demonstrates line numbering as well as adding callout notes.</p> <pre><code>foo: string = \"bar\"\nfor i in range(1, 3):\n    print(i)  # (1)!\n\ndef f() -&gt; int:\n    return 42\n</code></pre> <ol> <li> printing <code>i</code> will result in <code>1</code>, <code>2</code>, and then the loop completes</li> </ol>"},{"location":"demo_markdown/#example-of-pulling-code-snippets-from-a-file-and-highlighting","title":"Example of pulling code snippets from a file (and highlighting)","text":"<ul> <li><code>linenums</code> is where line numbering starts from</li> <li><code>title</code> is added above the block of code to describe content</li> <li><code>hl_lines</code> is optional and can highlight specific lines of code (offset from 1, regardless of line numbering start)</li> </ul> foo.py<pre><code>def f() -&gt; int:\n    \"\"\"A simple function.\"\"\"\n    return 42\n</code></pre>"},{"location":"demo_markdown/#graphics-with-mermaid-to-be-explored","title":"Graphics with <code>mermaid</code> (to be explored)","text":"<pre><code>sequenceDiagram\n    participant WorkingDir as Working Directory\n    participant Changed as Changed\n    participant Staged as Staged\n    participant Committed as Commit\n\n    WorkingDir -&gt;&gt; Changed: Modify File\n    Changed -&gt;&gt; Staged: git add\n    Staged -&gt;&gt; Committed: git commit\n    Changed -&gt;&gt; WorkingDir: git restore &lt;file&gt;\n    Staged -&gt;&gt; Changed: git restore --staged &lt;file&gt;\n    Committed -&gt;&gt; WorkingDir: git checkout &lt;commit&gt; &lt;file&gt;</code></pre> Method Description <code>GET</code>      Fetch resource <code>PUT</code>  Update resource <code>DELETE</code>      Delete resource Date Topic Links W 1/9 Welcome to COMP423 F 1/11 git Repositories CC++ <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre> <p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"lessons/2026_01_07_fdoc/","title":"Foundations of Software Engineering","text":"<p>Slides</p>"},{"location":"lessons/2026_01_09_adrs_llms/","title":"Architectural Design Records (ADRs), Lifelong Learning","text":"<p>Learning Objectives:</p> <ol> <li>Gain experience applying ACM Code of Ethics Principles to a real-life scenario.</li> <li>Understand how AI can help you enhance learning or help you avoid learning depending on how you use it.</li> <li>Gain experience engaging AI with curiosity and avoiding engaging AI with laziness.</li> </ol>"},{"location":"lessons/2026_01_12_dependency_mgmt_llm_chat_agent/","title":"Dependency Managers, LLMs, and Agentic IDEs","text":"<p>Learning Objectives:</p> <ol> <li>Differentiate between a large language model (LLM) and runtime / inference engine.</li> <li>Understand that today's large language models guess likely next tokens given an input context in an inference loop until a stop token, or other condition, is encountered.</li> <li>Explain how a chat-like system is layered on top of a large language model.</li> <li>Understand how a context window impacts the  quality of output tokens emitted.</li> <li>Explain what tool calling means to today's AI agents.</li> <li>Explain, at a high level, what context engineering means in a developer agent workflow.</li> </ol>"},{"location":"lessons/2026_01_14_git_operations/","title":"Conceptually Understanding Common `git` Operations","text":"<p>TODO</p>"},{"location":"lessons/2026_01_16_testing_foundations/","title":"Testing Foundations: From Requirements to Automation","text":"<p>TODO</p>"},{"location":"lessons/2026_01_21_quiz_0/","title":"Unit 0 Quiz","text":"<p>The goal of this page is to provide you with a single source of guidance on what to review and prepare for ahead of Quiz 0. The goal of the quiz is to assess your understanding of key concepts, ideas, and skills we have covered in the first two weeks of class.</p>"},{"location":"lessons/2026_01_21_quiz_0/#task-learning-objectives","title":"Task Learning Objectives","text":""},{"location":"lessons/2026_01_21_quiz_0/#from-tk01","title":"From TK01","text":"<p>By completing TK01, you should have gained experience with:</p> <ol> <li>Writing Architectural Design Records (ADRs).</li> <li>Reading and extracting meaning from open-source documentation.</li> <li>Understanding the role of dependency/package managers in Python projects.</li> <li>Using an LLM to learn unfamiliar technical concepts and clarify trade-offs.</li> <li>Reasoning about diverging project paths where multiple choices are defensible.</li> <li>Answering the guided technical questions in the write-up.</li> </ol>"},{"location":"lessons/2026_01_21_quiz_0/#from-tk02","title":"From TK02","text":"<p>By completing this assignment, you should have gained experience with:</p> <ol> <li>Following a specific <code>git</code> branching and merging workflow.</li> <li>Commiting a markdown ADR and understanding its value in a project.</li> <li>Engaging with an AI agent to implement your architectural decisions.</li> <li>Auditing, verifying, and taking ownership of an Agent's work before committing it.</li> <li>Answering the key conceptual questions found at the end of the write-up.</li> </ol>"},{"location":"lessons/2026_01_21_quiz_0/#from-tk03","title":"From TK03","text":"<p>By completing this <code>git</code> diagram, you should have comfort diagramming:</p> <ol> <li><code>git</code> commits</li> <li><code>git</code> branch creation</li> <li><code>git</code> branch switching</li> <li><code>git</code> branch merging</li> </ol> <p>For a solution to the diagram, see this demonstration: https://youtu.be/_uw0s4NrsRY</p>"},{"location":"lessons/2026_01_21_quiz_0/#reading-learning-objectives","title":"Reading Learning Objectives","text":"<p>For readings 01 through 08, reviewing Gradescope questions and your notes is preparatory.</p>"},{"location":"lessons/2026_01_21_quiz_0/#lesson-learning-objetives","title":"Lesson Learning Objetives","text":""},{"location":"lessons/2026_01_21_quiz_0/#ls01-architectural-design-records-adrs-lifelong-learning","title":"LS01 - Architectural Design Records (ADRs), Lifelong Learning","text":"<ol> <li>Gain experience applying ACM Code of Ethics Principles to a real-life scenario.</li> <li>Understand how AI can help you enhance learning or help you avoid learning depending on how you use it.</li> <li>Gain experience engaging AI with curiosity and avoiding engaging AI with laziness.</li> </ol>"},{"location":"lessons/2026_01_21_quiz_0/#ls02-dependency-managers-llms-and-agentic-ides","title":"LS02 - Dependency Managers, LLMs, and Agentic IDEs","text":"<ol> <li>Differentiate between a large language model (LLM) and runtime / inference engine.</li> <li>Understand that today's large language models guess likely next tokens given an input context in an inference loop until a stop token, or other condition, is encountered.</li> <li>Explain how a chat-like system is layered on top of a large language model.</li> <li>Understand how a context window impacts the  quality of output tokens emitted.</li> <li>Explain what tool calling means to today's AI agents.</li> <li>Explain, at a high level, what context engineering means in a developer agent workflow.</li> </ol>"},{"location":"lessons/2026_01_21_quiz_0/#ls03-conceptually-understanding-common-git-operations","title":"LS03 - Conceptually Understanding Common <code>git</code> Operations","text":"<p>The readings on <code>git</code> provided much necessary background for this lesson.</p> <ol> <li>You should have comfort diagramming:<ol> <li><code>git</code> commits with parents</li> <li><code>git</code> branches</li> <li>The <code>HEAD</code> branch</li> <li><code>git</code> branch switching</li> <li><code>git</code> branch merging</li> <li>Fast-forwarding vs. merge commits</li> </ol> </li> <li>You should understand what happens when a branch is deleted.</li> <li>You should understand how one branch into another and be able to do so with <code>git</code> commands.</li> <li>You should understand the scenarios in which a merge is a fast-fowarding of a branch as opposed to producing a merge commit.</li> <li>You should understand the differences and relationships between <code>pull</code>, <code>fetch</code>, and <code>merge</code>. You should be able to sync a local branch with a remote branch of the same name.</li> </ol>"},{"location":"lessons/2026_01_21_quiz_0/#ls04-testing-foundations-from-requirements-to-automation","title":"LS04 - Testing Foundations: From Requirements to Automation","text":"<ol> <li>Explain what a regression is in a software project and some scenarios in which it can occur.</li> <li>Defend the utility of automated testing in a software project.</li> <li>Differentiate between unit, integration, and end-to-end tests and the values and trade-offs of each.</li> <li>Differentiate between functional requirements and testing and non-functional (\"cross-functional\") requirements and testing.</li> <li>Write unit tests that cover specific branches/paths of a subject.</li> </ol>"},{"location":"lessons/2026_01_23_designing_with_layers/","title":"Designing with Layers and Composition","text":"<p>TODO</p>"},{"location":"lessons/2026_01_26_snow_day/","title":"Snow Day - Class Cancelled","text":"<p>TODO</p>"},{"location":"lessons/2026_01_28_aaa_testing_and_mocking/","title":"Arrange-Act-Assert Testing and Mocking","text":"<p>TODO</p>"},{"location":"lessons/2026_01_30_testing_with_patching_and_fixtures/","title":"Testing Concepts: Patching and Fixtures","text":"<p>TODO</p>"},{"location":"lessons/2026_02_02_snow_day/","title":"Snow Day - Class Cancelled","text":"<p>TODO</p>"},{"location":"lessons/2026_02_04_test_practice_and_modules/","title":"Testing Practice and Exercises","text":"<p>TODO</p>"},{"location":"lessons/2026_02_06_pydantic/","title":"Pydantic","text":"<p>TODO</p>"},{"location":"lessons/2026_02_11_qz01/","title":"Unit 1 Quiz","text":"<p>The goal of this page is to provide you with a single source of guidance on what to review and prepare for ahead of Quiz 1. The goal of the quiz is to assess your understanding of key concepts, ideas, and skills we have covered in the first two weeks of class.</p>"},{"location":"lessons/2026_02_11_qz01/#task-learning-objectives","title":"Task Learning Objectives","text":""},{"location":"lessons/2026_02_11_qz01/#from-tk04-professionalizing-the-developer-environment","title":"From TK04 - Professionalizing the Developer Environment","text":"<ol> <li>What is a code formatter?</li> <li>What is a linter?</li> <li>What value does each uniquely provide to a software engineering team?</li> <li>How do you extend your IDE (such as VSCode) to support developer tools like formatting and linting?</li> <li>What is a static type checker?</li> <li>Why is a static type checker useful in Python?</li> <li>Why don't you need a separate tool for this in a language like Java?</li> <li>Why is \"ceremony\" required to write a test in a testing framework?</li> <li>What is coverage in the context of automated testing?</li> <li>What are fixtures and why are they important in testing?</li> </ol>"},{"location":"lessons/2026_02_11_qz01/#from-tk05-unit-and-integration-testing-exercise","title":"From TK05 - Unit and Integration Testing Exercise","text":"<p>After successfully completing this exercise, you should be able to:</p> <ol> <li>Read and navigate a new codebase, understanding its structure and developer tooling.</li> <li>Explain the value of test-driven development (TDD) and apply the Red \u2192 Green \u2192 Refactor cycle.</li> <li>Write unit tests that verify individual components in isolation.</li> <li>Use mocks to isolate the subject under test from its composed dependencies.</li> <li>Use patches to intercept built-in library calls and test logic without side effects.</li> <li>Write integration tests that verify components work together with real dependencies.</li> </ol> <p>These skills are foundational to the upcoming quiz and will be assessed in the context of your work on this task.</p>"},{"location":"lessons/2026_02_11_qz01/#lesson-learning-objectives","title":"Lesson Learning Objectives","text":""},{"location":"lessons/2026_02_11_qz01/#from-ls05-designing-with-layers-and-composition","title":"From LS05 - Designing with Layers and Composition","text":"<p>After lecture, you should be able to address:</p> <ol> <li>What is a strict layered architecture?</li> <li>What are common motivations and trade-offs in layered systems?<ul> <li>Separation of Concerns</li> <li>Clear interfaces and contracts</li> <li>Dependency direction</li> <li>Replaceability</li> <li>Maintainability</li> <li>Testability</li> </ul> </li> <li>The role object-oriented composition plays in layered software architectures.</li> <li>The role of Fake implementations via subclassing or interfaces and trade-offs.</li> </ol>"},{"location":"lessons/2026_02_11_qz01/#from-ls06-aaa-arrange-act-assert-testing-and-mocking","title":"From LS06 - AAA (Arrange, Act, Assert) Testing and Mocking","text":"<p>After lecture, you should be able to address:</p> <ol> <li>What do the steps of Arrange, Act, and Assert mean in practice? Why is it a valuable test design pattern?</li> <li>What is a test double? How is a Mock different from a Fake?</li> <li>What are some fundamental facilities of a MagicMock that support automated testing?</li> </ol>"},{"location":"lessons/2026_02_11_qz01/#from-ls07-testing-with-patching-and-fixtures","title":"From LS07 - Testing with Patching and Fixtures","text":"<p>After lecture, you should be able to address:</p> <ol> <li>What is patching? When is it necessary? How does it relate to test doubles (fakes, mocks)?</li> <li>What is the utility of a fixture as it relates to automated testing?</li> </ol>"},{"location":"lessons/2026_02_11_qz01/#from-ls08-patching-and-testing-practice","title":"From LS08 - Patching and Testing Practice","text":"<p>After lecture, you should be able to address:</p> <ol> <li>How do fixtures factor into the lifecycle of running one test? Multiple tests?</li> <li>When patching a name in a dynamic language like Python, why must you patch the name where it is used in Python and not where it is defined?</li> </ol>"},{"location":"lessons/2026_02_11_qz01/#from-ls09-models-and-data-vadidation","title":"From LS09 - Models and Data Vadidation","text":"<p>After lecture, you should be able to address:</p> <ol> <li>What is a data model?</li> <li>What is data validation? Why is it important?</li> <li>What is the difference between declarative style programming vs. imperative? How is this exemplified in a data modeling library like Pydantic?</li> </ol>"},{"location":"lessons/2026_02_11_qz01/#from-rd13-how-ai-impacts-skill-formation","title":"From RD13 - How AI Impacts Skill Formation","text":"<p>After reading, you should be able to address:</p> <ol> <li>What is a danger in over-reliance on AI generated code when learning new concepts?</li> <li>Why did conceptual inquiry, hybrid code-explanation, and generation-then-comprehension techniques seem to improve quiz scores over the other AI-involved methods?</li> </ol>"},{"location":"lessons/2026_02_13_api_design/","title":"HTTP API Design","text":"<p>TODO</p>"},{"location":"lessons/2026_02_16_di_lab/","title":"Dependency Injection Lab","text":"<p>TODO</p>"},{"location":"lessons/2026_02_18_api_design_work_day/","title":"API Design Working Day","text":"<p>TODO</p>"},{"location":"lessons/2026_02_20_tbd/","title":"Lesson 13","text":"<p>TODO</p>"},{"location":"lessons/2026_02_23_tbd/","title":"Lesson 14","text":"<p>TODO</p>"},{"location":"lessons/2026_02_25/","title":"Unit 2 Quiz","text":"<p>TODO</p>"},{"location":"lessons/2026_03_11/","title":"Unit 3 Quiz","text":"<p>TODO</p>"},{"location":"lessons/2026_05_04/","title":"Final & Final Presentations (4pm-7pm)","text":"<p>TODO</p>"},{"location":"people/team/","title":"COMP423 Spring 2026 Teaching Team","text":"<ul> <li> <p>Alex Tang</p> </li> <li> <p> </p> <p>Benny Rakower</p> </li> <li> <p> </p> <p>Charlotte Tsui</p> </li> <li> <p> </p> <p>Hamzah Yousuf</p> </li> <li> <p> </p> <p>Jade Keegan</p> </li> <li> <p> </p> <p>Julia Guzzo</p> </li> <li> <p> </p> <p>Kamal Vasireddy</p> </li> <li> <p> </p> <p>Katie Brown</p> </li> <li> <p> </p> <p>Kris Jordan</p> </li> <li> <p> </p> <p>Lizzie Coats</p> </li> <li> <p> </p> <p>Lucy Good</p> </li> <li> <p> </p> <p>Muhammad Fouly</p> </li> <li> <p> </p> <p>Natalie Ammerman</p> </li> </ul>"},{"location":"people/bios/alext_about/","title":"Alex Tang","text":"<p>Edited: 1/5/2026</p> <p></p>"},{"location":"people/bios/alext_about/#about","title":"About","text":"<p>Hi! I'm Alex Tang, and I'm a junior double majoring in Computer Science and Statistics from Charlotte, NC. </p> <p>I spent this past summer as a software engineer intern for CommScope, a network infrastructure company, working with agentic AI systems and Model Context Protocol to develop in-house solutions. </p> <p>In my free time, I like watching movies and playing video games. My favorite movie is Everything Everywhere All At Once, and I'm currently playing through Pokemon Legends Z-A. This semester, my goal is to add at least 20 movies to my Letterboxd.</p> <p>I'm very excited to be one of your UTAs, and I'm looking forward to a great semester!</p>"},{"location":"people/bios/alext_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/brakower_about/","title":"Benny Rakower","text":"<p>Edited: 1/6/2026</p> <p></p>"},{"location":"people/bios/brakower_about/#about","title":"About","text":"<p>Hey everyone! I\u2019m Benny Rakower, a senior at UNC majoring in Computer Science and minoring in Business Administration from Westchester, NY.</p> <p>This past summer I was a Software Engineer intern at Major League Baseball, working on some of the components found on MLB.com.</p> <p>I\u2019m really into building things \u2014 especially web apps that have impact and can be used by the people around me. When I\u2019m not coding, you can find me playing poker, playing/watching sports, practicing guitar, or traveling. </p> <p>I\u2019m excited to be a TA for this class and looking forward to a fun and interesting semester!</p>"},{"location":"people/bios/brakower_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/chtsui_about/","title":"Charlotte Tsui","text":""},{"location":"people/bios/chtsui_about/#about","title":"About","text":"<p>Hello! I'm Charlotte, and I'm a junior from Charlotte, North Carolina. I am currently studying Computer and Data Science student. Previous to 423, I have TA'ed for COMP 110 and COMP 426. This past summer, I interned at SAS as a software engineer intern where I got to apply many of the skills and technology I learned throughout 426 and 423.</p> <p>Aside from academics, I enjoy climbing, hiking, video games, and crocheting!</p>"},{"location":"people/bios/chtsui_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/escoats_about/","title":"Lizzie Coats","text":""},{"location":"people/bios/escoats_about/#about","title":"About","text":"<p>Hi! I'm Lizzie Coats, and I'm a junior from Raleigh, North Carolina. I'm a Computer Science and Information Science double major, and prior to joining Team 423 I spent three semesters as a UTA for COMP110. </p> <p>I recently returned from an awesome semester abroad in Cape Town, South Africa, where I worked as a software engineer intern at RLabs, an international social enterprise. There, I helped develop a web app for a United Nations-recognized platform using many of the tools and skills I learned when I took COMP423 last year!</p> <p>This semester I'm trying to train for a 10k, find some cool hikes close to Chapel Hill, and start an herb garden in my backyard.</p>"},{"location":"people/bios/escoats_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/hamzahyous_about/","title":"Hamzah Yousuf","text":"<p>Edited: 1/7/2026</p> <p></p>"},{"location":"people/bios/hamzahyous_about/#about","title":"About","text":"<p>Hi, my name is Hamzah Yousuf and I am a junior at Chapel Hill studying Computer Science. I took this course in Spring 2025 as a sophomore and enjoyed learning the industry grade SWE tooling and best practices. When I interned over the summer as a software engineer, the learning from this class made the onboarding process much smoother. </p> <p>This is my first semester as a UTA, and I could not be more excited to dig through some of the core content again, help students with any questions they may have, and learn the new content this course will introduce this semester!</p> <p>Outside of school, I enjoy biking (biked the 32-mile Tobacco trail over winter break!), playing soccer, as well as tennis.</p>"},{"location":"people/bios/hamzahyous_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/jadek_about/","title":"Jade Keegan","text":"<p>Edited: 1/5/2026</p> <p></p>"},{"location":"people/bios/jadek_about/#about","title":"About","text":"<p>Hi! My name is Jade Keegan. I'm an MS student in my third and final semester. I completed my undergrad in CS here at UNC as well, where I worked under Kris on the COMP 590 pilot version of this course! Although software engineering is my main interest, this semester I'm excited to also be involved in cryptography research with Saba Eskandarian.</p> <p>Outside of work and school, I enjoy hiking, baking/cooking, crocheting, drawing, and playing video games with my friends.</p>"},{"location":"people/bios/jadek_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/juliag_about/","title":"Julia Guzzo","text":"<p>Edited: 1/6/2026</p> <p></p>"},{"location":"people/bios/juliag_about/#about","title":"About","text":"<p>Hi! My name is Julia Guzzo, and I am a senior studying Computer Science with a Data Science minor. This will be my fourth semester as as Undergraduate Teaching Assistant for the CS Department. Before this semester, I was a UTA for COMP 110 and COMP 433.</p> <p>This past summer, I was a SWE intern at Apple, where I worked on an internal macOS app designed to reduce friction in the code integration workflow. I have also been a member of App Team Carolina for 4 semesters, where I learned iOS development, served as an iOS developer for the Carolina Food Pantry app, and led the iOS Apprenticeship Learning Team. </p> <p>COMP 423 has been my favorite class I've taken here at UNC, so I'm really excited to finish out my time at Carolina as a COMP 423 UTA! </p>"},{"location":"people/bios/juliag_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/kamalv_about/","title":"Kamal Vasireddy","text":"<p>Edited: 1/6/2026</p> <p></p>"},{"location":"people/bios/kamalv_about/#about","title":"About","text":"<p>Hi! I'm Kamal, a third year student at UNC-Chapel Hill from Charlotte, NC pursuing a double major in Computer Science and Applied Mathematics.</p> <p>This past semester, I was an software engineering intern at Cognizant, a tech consulting company, where I helped build a automated tool to write tests for products that we were working on. Outside of school, I like to play tennis, go rock climbing, and binge tv shows.</p> <p>Looking forward to a great semester working with all of you!</p>"},{"location":"people/bios/kamalv_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/kgbro_about/","title":"Katie Brown","text":"<p>Edited: 1/6/2026</p> <p></p>"},{"location":"people/bios/kgbro_about/#about","title":"About","text":"<p>Hi, my name is Katie Brown! I'm a junior from Montgomery, AL studying computer science and information science. I'm interested in full stack development, pursuing software engineering, and before joining 423, I TA'd for COMP110!  </p> <p>Outside of class, I enjoy hiking, painting, live music, and hanging out with my friends! </p>"},{"location":"people/bios/kgbro_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/krisj_about/","title":"Kris Jordan","text":"<p>Edited: 1/4/2026</p> <p></p>"},{"location":"people/bios/krisj_about/#about","title":"About","text":"<p>Hello! My name is Kris Jordan. I'm in my 11th year teaching at UNC. Prior to that I was a founder and technical director of New Media Campaigns, attended Brown University for graduate school, and earned my Bachelors of Science in Computer Science at... UNC Chapel Hill!</p> <p>I just returned from a semester teaching abroad in Cape Town South Africa (as pictured) and am so excited to be back in COMP423 Foundations of Software Engineering to introduce you to an ever exciting field. The tools software engineers have available to them today are much more powerful than they were... one year ago! It is awesome time to be in an area where we can continue to do things that were impossible a few short years ago.</p> <p>In the past couple of years I have picked up sailing as a hobby and think it's a great outdoor activity for the engineering-minded.</p>"},{"location":"people/bios/krisj_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/lucykgood_about/","title":"Lucy Good","text":""},{"location":"people/bios/lucykgood_about/#about","title":"About","text":"<p>Hey everybody! My name is Lucy Good and I'm a junior from Chattanooga, TN. I'm double-majoring in CS and Philosophy and minoring in Spanish--I actually spent the fall semester abroad in Seville, Spain living with a host family, taking classes in Spanish, and volunteering as an assistant girls volleyball coach!</p> <p>Last summer, I worked for a tech startup in my hometown where I built them an internal administrative web application for customer management. I owe much of my ability to take on such a project to what I learned in this class last spring! So excited about learning more alongside you all this semester! If you have any questions at all, please feel free to come chat!</p>"},{"location":"people/bios/lucykgood_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/mfouly_about/","title":"Muhammad Fouly","text":"<p>Edited: 1/4/2026</p> <p></p>"},{"location":"people/bios/mfouly_about/#about","title":"About","text":"<p>Hi everyone! My name is Muhammad Fouly, and I am in my final semester at UNC. I\u2019m from Charlotte, NC, but was originally born and raised in Egypt. I\u2019m majoring in Computer Science (B.S.) and will be returning to Microsoft as a full-time Software Engineer after graduating in May.</p> <p>In my free time, I enjoy traveling, exploring new places, and experiencing the beauty the world has to offer.</p> <p>I\u2019m very excited to be a TA for this class and learn new things along the way!</p>"},{"location":"people/bios/mfouly_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"people/bios/natamm_about/","title":"Natalie Ammerman","text":"<p>Edited: 1/8/2026</p> <p></p>"},{"location":"people/bios/natamm_about/#about","title":"About","text":"<p>Hello!! My name is Natalie Ammerman. I'm a senior at UNC majoring in Computer Science and minoring in Neuroscience and Environmental Science and Studies. I took the pilot course for COMP 423 in Fall 2024, so I'm looking forward to seeing how much the course has grown since, and how much it will grow this semester!</p> <p>This past summer I worked as a Software Engineer intern at Red Hat, where I worked on the OpenShift platform, and I'm very excited to be returning to my team to work full-time after graduation.</p> <p>Outside of the professional realm, I enjoy going outside in pretty much any format (hiking, biking, running, etc.), traveling, hanging out with my dog, and partaking in music listening, movie watching, and video game playing.</p>"},{"location":"people/bios/natamm_about/#projects-links","title":"Projects &amp; Links","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#core-content","title":"Core Content","text":"<ul> <li>Backend Architecture and Testing</li> <li>Introduction to HTTP, RESTful APIs, and FastAPI</li> <li>Collaborating with <code>git</code></li> <li>Static Documentation Sites with <code>MkDocs</code></li> </ul>"},{"location":"resources/#exercises","title":"Exercises","text":"<ul> <li>EX00 - Collaborating on Technical Documentation</li> <li>EX01 - API Design with FastAPI</li> <li>EX02 - Angular Front-end</li> </ul>"},{"location":"resources/#course-content","title":"Course Content","text":"<ul> <li>Syllabus</li> <li>Team</li> <li>Playlists</li> </ul>"},{"location":"resources/playlist/","title":"Semester Playlists","text":"<ul> <li>Spotify</li> <li>YouTube</li> </ul>"},{"location":"resources/syllabus/","title":"Course Syllabus","text":""},{"location":"resources/syllabus/#general-course-info","title":"General Course Info","text":"<ul> <li>Title: Foundations of Software Engineering</li> <li>Term: Spring 2026</li> <li>Department: Computer Science (COMP)</li> <li>Course Number: 423</li> <li>Section: 001 In-person MWF - 1:25pm to 2:15pm</li> <li>Instructor: Kris Jordan<ul> <li>E-mail: kris+comp423.26s@cs.unc.edu</li> <li>LinkedIn: https://www.linkedin.com/in/krisjordan/</li> </ul> </li> </ul>"},{"location":"resources/syllabus/#course-description-target-audience-and-prerequisites","title":"Course Description, Target Audience, and Prerequisites","text":"<p>This course introduces the fundamentals of software engineering. You will gain experience with technical communication, teamwork, design, project management, production environments, automation, and code review, using modern tools and best practices.</p> <p>This course is for undergraduate Computer Science majors who want experience in software engineering.</p> <p>Prerequisite courses: C or better in both COMP301 and COMP211.</p>"},{"location":"resources/syllabus/#course-objectives","title":"Course Objectives","text":"<p>The goal of this course is to prepare you for technical leadership by focusing on how teams design, build, and deliver software for people.</p> <p>Software engineering is a fast-moving field that covers everything from ethics and communication to code and tools. The course is organized around six threads:</p> <ol> <li>Career and Professional Life<ul> <li>You will learn the software engineering code of ethics and how to make and justify decisions ethically.</li> <li>You will practice teaching yourself by evaluating tools, reading documentation, using AI assistants, and experimenting.</li> <li>You will learn about common professional roles and career paths in software development.</li> <li>You will reflect on your goals and practice habits to help you achieve them.</li> <li>You will build a portfolio-grade project with a team to show to future employers.</li> </ul> </li> <li>Collaboration<ul> <li>You will practice technical communication for different audiences, especially your teammates.</li> <li>You will practice using professional collaboration workflows.</li> <li>You will practice using tools designed to help teams build software.</li> <li>You will practice working on a shared codebase with a small team.</li> </ul> </li> <li>Design Process<ul> <li>You will practice solving engineering challenges through a design lens.</li> <li>You will practice analyzing requirements, brainstorming, and making trade-offs.</li> <li>You will practice writing requirements, design documents, and creating wireframes.</li> <li>You will practice working with user personas, stories, and epics.</li> </ul> </li> <li>Production System Design and Operation <ul> <li>You will learn the main subsystems of a modern, production-ready application that integrates with databases and AI.</li> <li>You will learn how system components communicate using APIs and protocols.</li> <li>You will practice deploying a production-grade cloud system automatically.</li> <li>You will practice reasoning about cross-functional system concerns like scalability and reliability.</li> </ul> </li> <li>Software Engineering Skills <ul> <li>You will practice building software that meets design requirements.</li> <li>You will practice making and documenting architectural decisions.</li> <li>You will practice acceptance verification using techniques such as automated testing, continuous integration, and static analysis.</li> <li>You will practice code reviews and pair programming.</li> <li>You will practice contributing to a large, existing codebase.</li> </ul> </li> <li>Tool Bench<ul> <li>You will learn to use fundamental tools such as source code management, dependency managers, test harnesses, and linters.</li> <li>You will learn to use AI tools like coding agents and cloud code reviewers.</li> <li>You will practice setting up a professional development environment.</li> <li>You will practice evaluating and choosing the right tools for a job.</li> </ul> </li> </ol> <p>These threads will be woven together throughout the semester. We will label activities by thread to help you see how they fit into the big picture.</p>"},{"location":"resources/syllabus/#textbooks-and-resources","title":"Textbooks and Resources","text":"<p>The course website (https://comp423-26s.github.io/) and Canvas are your main resources. There is no required textbook; we will provide all readings and tutorials online.</p> <p>We may source chapters from the following books. You do not need to buy them, but they are excellent resources for your career:</p> <ul> <li>The Mythical Man-month, Fred Brooks</li> <li>Code Complete 2nd Edition, Steve McConnell</li> <li>The Pragmatic Programmer, Dave Thomas and Andrew Hunt</li> <li>Clean Code and Clean Agile, Robert Martin</li> <li>A Philosophy of Software Design, John Ousterhout</li> </ul>"},{"location":"resources/syllabus/#disclaimer","title":"Disclaimer","text":"<p>The instructor may change the syllabus and will announce any changes as early as possible. This semester represents a significant iteration over previous offerings to incorporate AI-enabled software engineering workflows and modern production systems. Expect more frequent updates than in typical courses. Check the course site regularly for changes.</p>"},{"location":"resources/syllabus/#modality-in-person","title":"Modality: In-person","text":"<p>Lecture and office hours are your best resources. Both are in-person only; remote participation is not possible because we focus on group work.</p> <p>Unlike many CS courses, software engineering is as much about communication and teamwork as it is about programming. You will work in pairs and teams all semester. Being a reliable teammate is critical to your success.</p>"},{"location":"resources/syllabus/#course-requirements-and-policies","title":"Course Requirements and Policies","text":"<p>You should attend all lectures and check the course page for updates. Complete all assignments on time.</p> <p>Please arrive at least five minutes early so class can start on time. Place your bag in the basket under your seat.</p>"},{"location":"resources/syllabus/#course-load-expectation","title":"Course Load Expectation","text":"<p>This course is rigorous. Communicating with teammates, reading, designing, and debugging will take significant time. You should expect to spend 3 hours in lecture and about 9 hours per week working outside of class.</p> <p>We do not recommend taking COMP423 if you are enrolled in 17 or more credit hours.</p>"},{"location":"resources/syllabus/#grading-criteria","title":"Grading Criteria","text":"<p>To do well, you must participate in class, be a professional and productive teammate, keep up with assignments, and write verifiably acceptable software.</p> <p>Your final grade will depend heavily on peer evaluations of your work on the team project and paired tasks.</p> <p>Final grades are weighted as follows:</p> <ul> <li>10% - (LS) Lesson Attendance and Participation</li> <li>25% - Homework<ul> <li>10% - (RD) Readings</li> <li>15% - (TK) Tasks</li> </ul> </li> <li>15% - (QZ) Quizzes</li> <li>35% - Project Sprints (SP) / Final Project (FN)<ul> <li>10% - SP00 - Sprint 0</li> <li>10% - SP01 - Sprint 1</li> <li>10% - SP02 - Sprint 2</li> <li>5% - Final Presentation and Hand-in</li> </ul> </li> <li>15% - (FN) Final Exam</li> </ul>"},{"location":"resources/syllabus/#lesson-class-attendance-policy","title":"Lesson (Class Attendance) Policy","text":"<p>We follow UNC's official Attendance, Grading, and Examination policies.</p> <p>You can miss up to four (4) classes without penalty or needing approval. After four absences, further missed classes will lower your participation grade unless they are university-approved.</p>"},{"location":"resources/syllabus/#regrade-requests","title":"Regrade Requests","text":"<p>You have 48 hours to request a regrade after a grade is posted. Review your work first to understand any mistakes. If we graded something incorrectly, submit a request on Gradescope for that specific question. Do not use regrade requests to ask why something is wrong; come to office hours instead.</p>"},{"location":"resources/syllabus/#late-policies","title":"Late policies","text":"<p>Class assignments are due by the end of the lesson on Gradescope.</p> <p>Other assignments are due at 10:00pm on their due date.</p> <p>Assignments with 10:00pm deadlines have a 2-hour grace period. After that, a 15% penalty applies. The late period lasts 48 hours.</p>"},{"location":"resources/syllabus/#slip-days","title":"Slip Days","text":"<p>We will drop the late penalties for your first four late assignments. This only removes the penalty; it does not count for assignments you don't turn in. We expect you to hand everything in!</p>"},{"location":"resources/syllabus/#grading-scale-breakdown","title":"Grading Scale Breakdown","text":"<ul> <li>A: 94-100</li> <li>A-: 90-94</li> <li>B+: 87-89</li> <li>B: 83-86</li> <li>B-: 80-82</li> <li>C+: 77-79</li> <li>C: 73-76</li> <li>C-: 70-72</li> <li>D: 60-69</li> <li>F: 59 or below</li> </ul> <p>Grades will be rounded up if the fraction is greater than 0.499.</p>"},{"location":"resources/syllabus/#office-hour-expectations","title":"Office Hour Expectations","text":"<p>Office hours are a great way to get help. We use the CSXL app to manage the queue; you will be enrolled automatically.</p> <p>Office hours are in-person in Sitterson Hall SN136. Check the CSXL app for the current schedule.</p> <p>To help our TAs support everyone, please follow these guidelines:</p> <ul> <li> <p>TAs cannot help you with code you are working on that you cannot explain yourself. This is especially important to understand for AI-generated code: it is your responsibility to be able to understand and explain code you are responsible for and you are responsible for all code that you write yourself or generate.</p> </li> <li> <p>TAs can help you for up to 20 minutes per day. This ensures everyone gets help and encourages you to try solving problems on your own or with teammates first. Solving problems yourself is a vital skill for any engineer.</p> </li> <li> <p>Please write thoughtful help tickets. Explain the problem in detail and what you have already tried. We will help you make progress, but we may ask you to try reasoning through new problems on your own before returning.</p> </li> </ul>"},{"location":"resources/syllabus/#final-project-presentations","title":"Final Project Presentations","text":"<p>Instead addition to an abbreviated final exam, teams will present and demo their projects during the scheduled final exam period.</p>"},{"location":"resources/syllabus/#final-project-licensing-agreement","title":"Final Project Licensing Agreement","text":"<p>You will work in a team of four to build a feature for an open-source project used by UNC CS students. We hope to merge the best projects into the production codebase. Because of this, your work will be licensed under the MIT License, and you will receive proper credit for your contributions.</p>"},{"location":"resources/syllabus/#honor-code-and-collaboration-policy","title":"Honor Code and Collaboration Policy","text":"<p>Software engineering is about teamwork. We encourage you to collaborate with classmates and staff unless an assignment is explicitly marked as individual (like quizzes). The largest part of your grade is a group project. Quizzes are the only part of the course where you cannot collaborate.</p> <p>Always cite your collaborators clearly on any work you turn in.</p> <p>Do not use AI tools to write your original text drafts. You may use them to get feedback on grammar or clarity, but we want to see your own writing and thinking,  especially for self-reflections.</p> <p>If you use AI tools for code, you must cite the tools you use in your commits. In some tasks where we expect AI usage, we will require your prompt contexts to be committed to the repository as files of record and specify <code>git</code> workflows to show proof-of-work. You must be able to explain any code you turn in. You don't need to cite official documentation or standard library examples.</p> <p>In earlier offerings of the course we had a \"Google Rule\": no more than 20% of the lines of code you add to an assignment should be written by AI. The world of software engineering is quickly changing, however. This semester there is no limit to AI code generation in projects. You are responsible for understanding and ensuring the correctness of the code you generate and submit, though. The more you generate, the more you need to understand and assume responsibility for.</p>"},{"location":"resources/syllabus/#code-review-test","title":"Code Review Test","text":"<p>We may ask you to explain any line of code you turned in. Submitting code you do not understand is intellectually dishonest. If you cannot explain your work, your grade will be penalized, and you may be reported to the honor court.</p>"},{"location":"resources/syllabus/#feedback","title":"Feedback","text":"<p>If you have suggestions or want to thank the TAs, please give us feedback. We are happy to make improvements when we can!</p>"},{"location":"resources/syllabus/#university-statements-and-resources","title":"University Statements and Resources","text":""},{"location":"resources/syllabus/#acceptable-use-policy","title":"Acceptable Use Policy","text":"<p>By attending the University of North Carolina at Chapel Hill, you agree to abide by the University of North Carolina at Chapel Hill policies related to the acceptable use of IT systems and services. The Acceptable Use Policy (AUP) sets the expectation that you will use the University\u2019s technology resources responsibly, consistent with the University\u2019s mission. In the context of a class, it\u2019s quite likely you will participate in online activities that could include personal information about you or your peers, and the AUP addresses your obligations to protect the privacy of class participants. In addition, the AUP addresses matters of others\u2019 intellectual property, including copyright. These are only a couple of typical examples, so you should consult the full Information Technology Acceptable Use Policy, which covers topics related to using digital resources, such as privacy, confidentiality and intellectual property.</p> <p>Additionally, consult the Safe Computing at UNC website for information about data security policies, updates, and tips on keeping your identity, information, and devices safe.</p>"},{"location":"resources/syllabus/#data-security-and-privacy","title":"Data Security and Privacy","text":"<p>UNC-Chapel Hill is committed to fulfilling its responsibilities of transparency as a state-sponsored institution of higher learning, protecting certain types of information, and using information Carolina collects only for appropriate purposes. Consult the UNC-Chapel Hill Privacy Statement for additional information.</p>"},{"location":"resources/syllabus/#university-compliance-office-accommodations","title":"University Compliance Office \u2013 Accommodations","text":"<p>University Compliance Office (UCO) Accommodations Team (Accommodations \u2013 UNC Compliance) receives requests for accommodations for disability, pregnancy and related conditions, and sincerely held religious beliefs and practices through the University\u2019s Policy on Accommodations. UCO Accommodations team determines eligibility and reasonable accommodations consistent with state and federal laws.</p>"},{"location":"resources/syllabus/#counseling-and-psychological-services","title":"Counseling and Psychological Services","text":"<p>UNC-Chapel Hill is strongly committed to addressing the mental health needs of a diverse student body. The Heels Care Network website is a place to access the many mental health resources at Carolina. CAPS is the primary mental health provider for students, offering timely access to consultation and connection to clinically appropriate services. Go to the CAPS website or visit their facilities on the third floor of the Campus Health building for an initial evaluation to learn more. Students can also call CAPS 24/7 at 919-966-3658 for immediate assistance.</p>"},{"location":"resources/syllabus/#title-ix-resources","title":"Title IX Resources","text":"<p>Any student who is impacted by discrimination, harassment, interpersonal (relationship) violence, sexual violence, sexual exploitation, or stalking is encouraged to seek resources on campus or in the community. Reports can be made online to the UCO or by contacting the University\u2019s Title IX Coordinator, Elizabeth Hall, or the Report and Response Managers in the University Compliance Office (UCO) (formerly the Equal Opportunity and Compliance Office).  Please note that I am designated as a Responsible Employee, which means I must report to the UCO any information I receive about the forms of misconduct listed in this paragraph.  If you\u2019d like to speak with a confidential resource, those include Counseling and Psychological Services, the University\u2019s Ombuds Office, and the Gender Violence Services Coordinators. Additional resources are available at safe.unc.edu.</p>"},{"location":"resources/syllabus/#policy-on-non-discrimination","title":"Policy on Non-Discrimination","text":"<p>As set out in the University\u2019s Policy Statement on Non-Discrimination, the University is committed to providing an environment where all members of our community can learn, work, and thrive. Consistent with these principles and applicable laws, it is therefore the University\u2019s policy not to discriminate on the basis of age, color, disability, gender, gender expression, gender identity, genetic information, national origin, race, religion, sex, sexual orientation or veteran status as consistent with the University\u2019s Policy on Prohibited Discrimination, Harassment and Related Misconduct. No person, on the basis of protected status, shall be excluded from participation in, be denied the benefits of, or be subjected to unlawful discrimination, harassment, or retaliation under any University program or activity, including with respect to employment terms and conditions. The University will consider only relevant factors such as individual abilities and qualifications in admissions, hiring, disciplinary action, and all other decisions and will apply consistent standards of conduct and performance.</p> <p>If you are experiencing harassment or discrimination, you can seek assistance and file a report through the Report and Response Coordinators (email reportandresponse@unc.edu or see additional contact info at safe.unc.edu) or the University Compliance Office. Please note that I am designated as a Responsible Employee, which means that I must report to the UCO any information I receive about harassment or discrimination.  If you\u2019d like to speak with a confidential resource, those include Counseling and Psychological Services and the University\u2019s Ombuds Office.</p> <p>All students are expected to adhere to University policy and follow the guidelines of the UNC Student Code of Conduct. Additional information can be found at https://studentconduct.unc.edu/.</p>"},{"location":"resources/syllabus/#writing-center","title":"Writing Center","text":"<p>For free feedback on any course writing projects, check out UNC\u2019s Writing Center. Writing Center coaches can assist with any writing project, including multimedia projects and application essays, at any stage of the writing process. You don\u2019t even need a draft to come visit. To schedule a 45-minute appointment, review quick tips, or request written feedback online, visit UNC\u2019s Writing Center online.</p>"},{"location":"resources/syllabus/#syllabus-changes","title":"Syllabus Changes","text":"<p>The instructor reserves the right to make changes to the syllabus including project due dates and test dates. These changes will be announced as early as possible.</p>"},{"location":"resources/MkDocs/ex00/","title":"EX00 - Collaborating on Technical Documentation","text":""},{"location":"resources/MkDocs/ex00/#learning-objectives","title":"Learning Objectives","text":"<p>Welcome to your first pair project! This exercise aims to build your skills in several key areas:</p> <ul> <li>Starting Projects: Learn how to create a new project using today\u2019s development tools and be empowered to start your own!</li> <li>Using Technical Documentation: Get better at finding, reading, and using guides and manuals to achieve your goals on your own.</li> <li>Writing Technical Documentation: Improve at writing clear instructions for others on how to do technical tasks.</li> <li>Teamwork with Git: Develop your ability to work with others using git, focusing on how to manage changes and updates in your work effectively.</li> </ul>"},{"location":"resources/MkDocs/ex00/#workflow-expectations","title":"Workflow Expectations","text":"<p>We are prescribing, and grading on, some very specific workflow expectations in this exercise:</p> <ul> <li>All merges in this exercise must produce merge commits, thus always use the <code>--no-ff</code> flag when merging in this exercise. The <code>git merge --no-ff &lt;source-branch&gt;</code> command is short for no fast-forward and produces a merge commit where fast-forwarding was otherwise possible. This is how we will see your history of branch use.</li> <li>Before merging locally, be sure you have pushed your branch to your repository.</li> <li>Only you are permitted to merge into <code>main</code> on your own repository. You can push branches to your partner's repository.</li> <li>We will NOT use Pull Requests yet in order to apply our understanding of <code>git</code>'s fundamental collaboration commands (<code>fetch</code>, <code>branch</code>, <code>switch</code>, <code>merge</code>, <code>pull</code>, and <code>push</code>)</li> </ul>"},{"location":"resources/MkDocs/ex00/#deliverable-overview","title":"Deliverable Overview","text":"<p>You and your partner will work together to produce:</p> <ol> <li>Tutorial Guides (50 points): Each of you will write a tutorial for setting up a basic project from scratch, one of you for the Go programming language and the other for the Rust programming language. You\u2019ll publish your guide on your personal MkDocs site. Your partner will help by reviewing and suggesting improvements to your guide.<ul> <li>20 points - Well fomatted markdown and use of appropriate MkDocs for Material features (e.g. text formatting, bullets/enumerations where meaningful, subheadings, code blocks, and admonitions). Hint: Markdown reference.</li> <li>30 points - Tutorial includes required components (brief intro, git repo setup, dev container setup, initialize a project, add a hello world example, compile and run)</li> </ul> </li> <li>Project Repositories (20 points): Host the starter projects resulting from your tutorials on GitHub and make them public. Project repositories will need a <code>README.md</code> in the root directory that link out to your tutorial on your course notes site.</li> <li>EX00 Git Workflow Adherence (30 points): You will be asked to show how you followed the specific <code>git</code> workflow expectations we have in this exercise. This will be preserved in your <code>git</code> repository's history (e.g. via <code>log</code>) and we will show you how to share it via your GitHub repository. Read Workflow Expectations Above BEFORE Beginning!</li> </ol>"},{"location":"resources/MkDocs/ex00/#tutorial-anti-patterns","title":"Tutorial Anti-patterns","text":"<ul> <li>Besides VSCode, Docker, and git, users should not install any software directly to their host machine. That is the role of the dev container! (Instructions which install Go or Rust to the host machine will be a -25pt penalty.)</li> <li>When compiling Go or Rust in the Dev Container, since it is running Linux, you do not expect to see <code>.exe</code> file extensions anywhere in this project. That is a sign you are running something on the host machine.</li> </ul>"},{"location":"resources/MkDocs/ex00/#initial-setup-partner-repository-access-and-main-ruleset","title":"Initial Setup: Partner Repository Access and <code>main</code> Ruleset","text":"<p>You need to give your partner access to your repository and get access to theirs.</p>"},{"location":"resources/MkDocs/ex00/#setting-up-repository-access","title":"Setting Up Repository Access","text":"<p>Navigate to your <code>comp423-course-notes</code> repository on GitHub (the one you created in the previous tutorial) and follow these steps:</p> <ol> <li>Go to Settings &gt; Collaborators</li> <li>Click \"Add people\"</li> <li>Enter your partner's GitHub username</li> <li>Select their account and send the invitation</li> </ol> <p>Your partner will need to accept the invitation via email or through GitHub's interface before they can contribute to your repository. Make sure you both complete this step before proceeding.</p>"},{"location":"resources/MkDocs/ex00/#protecting-your-repository-with-rulesets","title":"Protecting Your Repository with Rulesets","text":"<p>Even though you've added your partner as a collaborator, you'll want to protect your <code>main</code> branch through a ruleset. Rulesets are GitHub's approach to repository security, allowing you to define who can make what kinds of changes to your codebase. Let's set up a simple one and understand what each setting does.</p> <p>Navigate to your repository's Settings page on GitHub and follow these steps:</p> <ol> <li>Click on \"Rules\" &gt; \"Rulesets\" in the left sidebar under \"Code and automation\"</li> <li>Click \"New ruleset\" &gt; \"New branch ruleset\" to create a ruleset</li> <li>Configure your ruleset with these settings:</li> </ol> <p>Basic Settings:</p> <ul> <li>Name your ruleset <code>main</code> </li> <li>For \"Enforcement status,\" select \"Active\"</li> <li>Bypass list: Add \"Repository admin\" (this is you, the repository owner)</li> <li>Under \"Target branches\" choose \"Add target\" and select \"Include default branch\" (this is <code>main</code>)</li> </ul> <p>Rules:</p> <ul> <li>Enable \"Restrict creations\"</li> <li>Enable \"Restrict updates\"</li> <li>Leave everything else as is. The following two settings should be checked by default:<ul> <li>Enable \"Restrict deletions\"</li> <li>Block force pushes</li> </ul> </li> </ul> <p>Let's understand what each of these settings means and why they're important:</p> <p>Targeting the Default Branch: Your default branch (in your project, this is <code>main</code>) is the primary branch of your repository.</p> <p>Restrict Creations, Updates, and Deletions: These three settings work together to control who can make changes to your default branch. When enabled, only people in the bypass list (in this case, just you) can create new commits on the primary branch, update existing commits, overwrite the branch (force push) or delete the branch entirely.</p> <p>These rules reflect a simplified real-world setup. Your partner can create branches, make changes, and push their branches to share with you - they just can't modify the protected <code>main</code> branch directly. That's your job!</p>"},{"location":"resources/MkDocs/ex00/#cloning-your-partners-repository","title":"Cloning Your Partner's Repository","text":"<p>Since you already have your own repository set up locally from the previous tutorial, you just need to clone your partner's repository. Because both repositories have the same name, you'll want to specify a different directory name when cloning to avoid conflicts. The target directory name is added as an extra command-line argument following the repository URL:</p> <pre><code># Clone your partner's repository with a descriptive directory name\ngit clone https://github.com/&lt;partner&gt;/comp423-course-notes.git partner-notes\n</code></pre> <p>After cloning, open your partner's notes site in the new directory in VS Code and then reopen the workspace in a dev container, just as you did with your own repository in the previous tutorial.</p>"},{"location":"resources/MkDocs/ex00/#pay-attention-to-which-project-you-are-working-on","title":"Pay Attention To Which Project You Are Working On","text":"<p>When working on this exercise, be sure you know whose project repository you are working in! In this exercise each of you owns your own repository and will be practicing contributing to your partner's repository.</p> <p>As a rule of thumb, when returning to work in VS Code, go to your \"File\" menu, \"Open Recent\", and select the directory with \"[Dev Container]\" that you intend to work on. If you're already in it, nothing happens. If you're switching between projects, your the new dev container will open.</p> <p>Finally, it is possible to have both dev containers running simultaneously. However, you cannot have both MkDocs dev servers available simultaneously on the same port. The most straightforward way around this is to start <code>mkdocs serve</code> with the <code>-a</code> (address) flag and provide an alternate port (e.g. <code>mkdocs serve -a localhost:4230</code>). In the event you forget to to do this, Docker will try to avoid the conflict by forwarding a different port on your host machine to your container port. To see these mappings in VS Code, go to the Ports tab (typically next to Terminal) or open up the Command Palatte and search for Focus on Ports View. Look for the forwarded address.</p>"},{"location":"resources/MkDocs/ex00/#direct-git-collaboration-workflow","title":"Direct Git Collaboration Workflow","text":"<p>In this exercise, you will rely on Git's fundamental commands to share and integrate changes. This approach helps you practice your understanding of Git's branching model. Here's the basic flow:</p> <ol> <li>You work on changes in your own repository</li> <li>Your partner clones your repository, makes suggestions in a branch, and they push their branch to your repo</li> <li>You fetch their suggestions locally and switch to their branch to consider the work</li> <li>After reviewing and testing, you merge changes into your <code>main</code> branch (with <code>--no-ff</code> to preserve the history!)</li> <li>You push the updated <code>main</code> branch back to GitHub to deploy your site</li> </ol> <p>This workflow requires careful coordination and communication between team members and a solid understanding of Git's core commands. Let's explore how to use these commands effectively.</p>"},{"location":"resources/MkDocs/ex00/#do-all-of-your-work-in-branches","title":"Do All of Your Work in Branches!","text":"<p>Always create a new branch for work you're about to embark upon:</p> <pre><code># Do you have any uncommitted work?\n#     Yes? Decide what to do with it!\n#     No? Great!\n# Are you on the right starting branch for new work?\n#     No? switch to where you want to branch from (usually main)\n#     Yes? Great!\ngit status\n\n# Create and switch to a new branch with a meaningful name\n# Of course, substitute &lt;meaningful-name&gt; with something... meaningful\ngit switch -c &lt;meaningful-name&gt;\n\n# As you work, commit regularly with meaningful messages\ngit add &lt;directory or file(s)&gt;\ngit commit -m \"&lt;message&gt;\"\n</code></pre>"},{"location":"resources/MkDocs/ex00/#share-your-work","title":"Share Your Work","text":"<p>When ready to share your changes for your partner to check out:</p> <pre><code># Push your meaningfully named branch to GitHub\n# Note: Use the same branch name locally as remotely!\ngit push -u origin &lt;meaningful-name&gt;\n</code></pre> <p>Go convince yourself the branch push worked by opening GitHub, clicking the Branch Dropdown (it should show <code>main</code>) and selecting your branch. Let your partner know the branch you shared by its name and/or by sharing a link directly to the branch on Github.</p>"},{"location":"resources/MkDocs/ex00/#reviewing-partners-work","title":"Reviewing Partner's Work","text":"<p>When your partner has pushed changes for review, open your copy of your partner's dev container project and then:</p> <ol> <li> <p>Fetch the latest changes from the project's repository:    <pre><code>git fetch origin\n</code></pre></p> </li> <li> <p>Switch to the branch they just pushed    <pre><code>git switch &lt;branch-name-partner-pushed&gt;\n</code></pre></p> </li> <li> <p>Review their changes locally, running their tutorial to verify it works. If you'd like to see the specific changes made relative to your <code>main</code> branch, or any other branch, you can use the <code>diff</code> subcommand of <code>git</code>:</p> </li> <li>In the terminal: <code>git diff main</code> - press <code>Space</code> to move forward page-by-page and <code>q</code> to exit. The diff shows additions (in green) and removals (in red).</li> <li> <p>On GitHub: View the branch, look for the Contribute button drop down, click it and select Compare (not Pull Request). You will see a diff of the changes.</p> </li> <li> <p>To make your own suggested edits to their work, go ahead and start a branch:    <pre><code>git switch -c edits/&lt;branch-name-partner-pushed&gt;\n</code></pre></p> </li> <li> <p>Make your changes, add commit(s) with meaningful commit messages, then push your edits:    <pre><code>git push -u origin edits/&lt;branch-name-partner-pushed&gt;\n</code></pre></p> </li> </ol> <p>Let your partner know the name of the branch you pushed edits to their work on.</p>"},{"location":"resources/MkDocs/ex00/#merging-a-partners-changes","title":"Merging a Partner's Changes","text":"<p>Ready to review the edits your partner pushed to your repo? Be sure you have your dev container open, then follow the same steps as reviewing partner work above, but use the branch name they pushed.</p> <p>Once you're ready to merge their work into your branch, or <code>main</code>, or your own work into <code>main</code>, the steps are as you already know with one minor twist: for this exercise, always use the <code>--no-ff</code> flag to preserve the merge history:</p> <pre><code># First, switch to the target branch you're merging into\ngit switch &lt;target branch&gt;\n\n# Best practice: be sure your branch is up-to-date\ngit pull origin &lt;target branch&gt;\n\n# Merge your source branch and force a merge commit \ngit merge --no-ff &lt;source branch being merged in&gt;\n\n# Push the merged changes to GitHub\ngit push origin &lt;target branch&gt;\n</code></pre>"},{"location":"resources/MkDocs/ex00/#class-lesson-04-practice-exchange","title":"Class Lesson 04 - Practice Exchange","text":"<p>Decide which of you will do a tutorial on Go and the other on Rust. If your partner is not in class, you get first dibs on either!</p> <ol> <li>Create and switch to a branch in your project named <code>mt04-setup</code></li> <li>Create a directory named <code>docs/tutorials</code></li> <li>Add a file named <code>docs/tutorials/rust-setup.md</code> or <code>docs/tutorials/go-setup.md</code>, whichever you are going to be working on, with the following contents:    <pre><code># Setting up a dev container for &lt;Insert: Go or Rust depending on which you are doing&gt;\n\n* Primary author: [&lt;Your Name&gt;](https://YourGitHubProfileLink)\n</code></pre></li> <li>Verify your work in your local development server (<code>mkdocs serve</code>)</li> <li>Push the branch to your repo (see Share Your Work above)</li> <li>Let your partner know and they should follow the steps of Reviewing Partner's Work (above)</li> <li>Your partner should create a new branch named <code>edits/mt04-setup</code> and add the following changes to your setup markdown file below the primary author line:    <pre><code>* Reviewer: [&lt;Partner name&gt;](https://PartnerGithubProfileLink)\n</code></pre></li> <li>Your partner should commit and push their branch to your repo</li> <li>You should fetch changes, then switch to their branch and see their contribution</li> <li>You should switch back to your work branch (<code>mt04-setup</code>) and merge their work in with <code>--no-ff</code>. Then push to your <code>mt04-branch</code>.</li> <li>You should switch back to your <code>main</code> branch and merge your work in <code>mt04-setup</code> with <code>--no-ff</code>. Then push your main branch. Your site should publish successfully with these changes!</li> </ol> <p>Review the commit graph history: </p> <ol> <li>In your dev container terminal <code>git log --graph --oneline</code> and notice the merge commits and where branches are.</li> <li>In your GitHub repository, go to the Insights tab, select Network. You should see your commit graph with tags here.</li> </ol> <p>Ultimately, when proving that you are following the workflow expectations with your partner for this assignment, you will submit a screenshot of these graphs as a record of the branches and merges made.</p>"},{"location":"resources/MkDocs/ex00/#assignment-structure","title":"Assignment Structure","text":"<p>You and your partner will each create a tutorial about setting up a new DevContainer project. One of you will focus on Go, while the other will work on Rust. Here's how to organize the work:</p> <ol> <li>Update your <code>mkdocs.yml</code> to include the required markdown features. You can enable whatever other features you would like, too!</li> <li>Write your tutorial in either <code>docs/tutorials/go-setup.md</code> or <code>docs/tutorials/rust-setup.md</code> (one per team mate course notes site)</li> <li>Share your tutorial branch with your team mate, they should follow along and add helpful additions, corrections, or suggestiosn along the way. They will push their brach back to your repo.</li> <li>You iterate off of their pushed branch and incorporate back into your work.</li> <li>You finalize your tutorial and merge into <code>main</code></li> <li>You follow your tutorial from start to finish and publish the resulting repository in a public repo on your GitHub account. Its <code>README.md</code> should link back to your course notes site's tutorial.</li> </ol>"},{"location":"resources/MkDocs/ex00/#required-material-for-mkdocs-features-to-enable","title":"Required Material for MkDocs Features to Enable","text":"<p>Before diving head first into your tutorial, there are a few helpful markdown features to enable in Material for MkDocs with your partner. Find how to enable these on the official website. Hint: try searching! Once you find reference for a particular feature, look at the sidebar for both configuration and usage examples.</p> <p>Start a branch for <code>mkdocs-extensions</code> and enable the following features. After enabling each feature, try adding an example of how to use them in markdown to your tutorial draft.</p> <p>Workflow Expectation: You setup code blocks in the branch. Your partner will check out your work and then incorporate admonitions in a branch off of your branch. You will then be responsible for merging all and pushing. If you've already individually enabled both features, let your partner find another feature to enable on your site (and do the same in exchange)!</p> <ol> <li>Code Blocks - for syntax highlighting of code</li> <li>Admonitions - for useful call-outs to draw attention to ideas or asides</li> </ol>"},{"location":"resources/MkDocs/ex00/#tutorial-content-requirements","title":"Tutorial Content Requirements","text":"<p>Your tutorials should include:</p> <ol> <li>Prerequisites</li> <li>Step-by-step instructions for creating a new Dev Container project for your language</li> <li>Should start from a blank directory and include git initialization</li> <li>Dev Container configuration file explanations</li> <li>Steps to create a new project, write a basic \"Hello COMP423\" program, compile, and run</li> <li>The program's requirement is that it simply prints \"Hello COMP423\" out to standard output</li> </ol> <p>Make use of Material for MkDocs features to enhance your documentation:</p> <ul> <li>Code blocks with syntax highlighting for configuration files and commands</li> <li>Admonitions for important notes and warnings</li> </ul> <p>You can cite and reuse instructions from the 423 MkDocs tutorial if useful.</p>"},{"location":"resources/MkDocs/ex00/#need-help","title":"Need Help?","text":"<ul> <li>If you encounter Git issues, try running <code>git status</code> and <code>git log</code> to understand your current state</li> <li>When in doubt, communicate with your partner about the current state of changes</li> <li>For language-specific questions, consult the official language documentation and Google</li> <li>Go documentation</li> <li>Rust / Cargo documentation</li> </ul> <p>Remember to commit early and often, and always use the <code>--no-ff</code> flag when merging to maintain a clear history of collaboration.</p>"},{"location":"resources/MkDocs/ex00/#go-tutorial-expectations","title":"Go Tutorial Expectations","text":"<ul> <li>Dev container should use a base image from Microsoft (Hint: refer back to the MkDocs tutorial)</li> <li>Be sure the Dev Container Installs the official Go VSCode Plugin (Made by the Go Team at Google)</li> <li>Show the <code>go version</code> subcommand to prove a recent version of Go</li> <li>Use the <code>mod</code> subcommand</li> <li>Use the <code>run</code> subcommand</li> <li>Use the <code>build</code> subcommand (discuss this in the context of COMP211's <code>gcc</code> command), run the built binary directly, and discuss the difference from <code>run</code></li> </ul>"},{"location":"resources/MkDocs/ex00/#rust-tutorial-expectations","title":"Rust Tutorial Expectations","text":"<ul> <li>Dev container should use a base image from Microsoft (Hint: refer back to the MkDocs tutorial)</li> <li>Be sure the Dev Container Installs the official <code>rust-analyzer</code> VSCode plugin by the Rust Programming Language Group</li> <li>Show the <code>rustc --version</code> to prove a recent version of rust</li> <li>Use the <code>cargo new</code> command to create a binary project (use the flag that does not create a new <code>git</code> repository automatically on your behalf: <code>--vcs none</code> (Version Control System))</li> <li>Use the <code>cargo build</code> command and show how to run the built file (discuss in terms of COMP211's <code>gcc</code> command)</li> <li>Use the <code>cargo run</code> subcommand and discuss difference with <code>build</code></li> </ul>"},{"location":"resources/MkDocs/tutorial/","title":"Starting a Static Website Project with MkDocs","text":"<p>Welcome! In this tutorial, you'll learn how to build a static website to organize your course notes using GitHub Pages and the powerful static site generator, Material for MkDocs. By the end of this guide, you'll have a fully functional website hosted online, starting from a blank repository. Along the way, you'll also set up a basic Python development container (dev container) in Visual Studio Code (VS Code) and configure GitHub Actions for continuous integration and deployment (CI/CD) and learn what all of this useful jargon means.</p> <p>Why Material for MkDocs?</p> <p>MkDocs is the de facto documentation site generator for today's most popular, modern Python projects, including a few we'll in this course: FastAPI and Pydantic. Those sites are made and maintained with this documentation tool. Material for MkDocs is one of the most popular themes for MkDocs, offering a sleek design, responsive layout, and tons of features out of the box. As an added endorsement, I, Kris, claim this is the best and easiest to use, static site generator tool I've ever seen. In fact, this course site you're reading right now is built using MkDocs, too! (There's a recursion joke somewhere in here.)</p>"},{"location":"resources/MkDocs/tutorial/#why-this-matters","title":"Why This Matters","text":"<p>Static websites are an essential part of software engineering and open-source projects. Many teams and individuals use them to document software, share knowledge, and create personal portfolios. Learning to build and manage one not only enhances your technical skillset but also sets you up for creating your own portfolio and blogging website.</p>"},{"location":"resources/MkDocs/tutorial/#what-you-will-learn","title":"What You Will Learn","text":"<p>By completing this tutorial, you will:</p> <ul> <li>Set up a basic Python Development Container in VS Code to streamline development.</li> <li>Initialize and configure a GitHub repository for a static site.</li> <li>Use Material for MkDocs to generate a clean, professional website.</li> <li>Deploy your site to GitHub Pages with GitHub Actions for CI/CD.</li> <li>Gain insight into the tools and practices used in open-source and professional software projects.</li> </ul>"},{"location":"resources/MkDocs/tutorial/#prerequisites","title":"Prerequisites","text":"<p>Before we dive in, make sure you have:</p> <ol> <li>A GitHub account: If you don\u2019t have one yet, sign up at GitHub.</li> <li>Git installed: Install Git if you don\u2019t already have it.</li> <li>Visual Studio Code (VS Code): Download and install it from here.</li> <li>Docker installed: Required to run the dev container. Get Docker here.</li> <li>Command-line basics: Your COMP211 command-line knowledge will serve you well here. If in doubt, review the Learn a CLI text!</li> </ol>"},{"location":"resources/MkDocs/tutorial/#part-1-project-setup-creating-the-repository","title":"Part 1. Project Setup: Creating the Repository","text":""},{"location":"resources/MkDocs/tutorial/#step-1-create-a-local-directory-and-initialize-git","title":"Step 1. Create a Local Directory and Initialize Git","text":"<p>(A) Open your terminal or command prompt.</p> <p>(B) Create a new directory for your project. (Note: Of course, if you'd like to organize this tutorial somewhere else on your machine, go ahead and change into that parent directory first. By default this will be in your user's home directory.):</p> <pre><code>mkdir comp423-course-notes\ncd comp423-course-notes\n</code></pre> <p>(C) Initialize a new Git repository:</p> <pre><code>git init\n</code></pre> <p>What is the effect of running the <code>init</code> subcommand?</p> <p>You should know what happens when you run this command at this point in the course! If you do not, please refer back to the chapter on Fundamental git Subcommands.  </p> <p>(D) Create a README file:</p> <pre><code>echo \"# COMP423 Course Notes\" &gt; README.md\ngit add README.md\ngit commit -m \"Initial commit with README\"\n</code></pre>"},{"location":"resources/MkDocs/tutorial/#step-2-create-a-remote-repository-on-github","title":"Step 2. Create a Remote Repository on GitHub","text":"<p>(1) Log in to your GitHub account and navigate to the Create a New Repository page.</p> <p>(2) Fill in the details as follows:</p> <ul> <li>Repository Name: <code>comp423-course-notes</code></li> <li>Description: \"Course notes organized as a static website using Material for MkDocs.\"</li> <li>Visibility: Public</li> </ul> <p>(3) Do not initialize the repository with a README, .gitignore, or license.</p> <p>(4) Click Create Repository.</p>"},{"location":"resources/MkDocs/tutorial/#step-3-link-your-local-repository-to-github","title":"Step 3. Link your Local Repository to GitHub","text":"<p>(1) Add the GitHub repository as a remote:</p> <pre><code>git remote add origin https://github.com/&lt;your-username&gt;/comp423-course-notes.git\n</code></pre> <p>Replace <code>&lt;your-username&gt;</code> with your GitHub username.</p> <p>(2) Check your default branch name with the subcommand <code>git branch</code>. If it's not <code>main</code>, rename it to <code>main</code> with the following command: <code>git branch -M main</code>. Old versions of <code>git</code> choose the name <code>master</code> for the primary branch, but these days <code>main</code> is the standard primary branch name.</p> <p>(3) Push your local commits to the GitHub repository:</p> <pre><code>git push --set-upstream origin main\n</code></pre> <p>Understanding the --set-upstream Flag</p> <ul> <li><code>git push --set-upstream origin main</code>: This command pushes the main branch to the remote repository origin. The <code>--set-upstream</code> flag sets up the main branch to track the remote branch, meaning future pushes and pulls can be done without specifying the branch name and just writing <code>git push origin</code> when working on your local <code>main</code> branch. This long flag has a corresponding <code>-u</code> short flag.</li> </ul> <p>(4) Back in your web browser, refresh your GitHub repository to see that the same commit you made locally has now been pushed to remote. You can use <code>git log</code> locally to see the commit ID and message which should match the ID of the most recent commit on GitHub. This is the result of pushing your changes to your remote repository.</p>"},{"location":"resources/MkDocs/tutorial/#part-2-setting-up-the-development-environment","title":"Part 2. Setting Up the Development Environment","text":""},{"location":"resources/MkDocs/tutorial/#what-is-a-development-dev-container","title":"What is a Development (Dev) Container?","text":"<p>A dev container ensures that your development environment is consistent and works across different machines. At its core, a dev container is a preconfigured environment defined by a set of files, typically leveraging Docker to create isolated, consistent setups for development. Think of it as a \"mini computer\" inside your computer that includes everything you need to work on a specific project\u2014like the right programming language, tools, libraries, and dependencies.</p> <p>Why is this valuable? In the technology industry, teams often work on complex projects that require a specific set of tools and dependencies to function correctly. Without a dev container, each developer must manually set up their environment, leading to errors, wasted time, and inconsistencies. With a dev container, everyone works in an identical environment, reducing bugs caused by \"it works on my machine\" issues. It also simplifies onboarding new team members since they can start coding with just a few steps.</p>"},{"location":"resources/MkDocs/tutorial/#how-are-software-project-dependencies-managed","title":"How are software project dependencies managed?","text":"<p>To effectively manage software dependencies, it's important to understand package and dependency management. In most software projects, you rely on external libraries or packages to save time and leverage work that has already been done by others. Managing these dependencies ensures that your project has access to the correct versions of these libraries, avoiding compatibility issues.</p> <p>In this project, our primary dependency is <code>mkdocs-material</code>, which enables us to build and style our static site. This package is available on PyPi, the Python Package Index, which is a repository of software for the Python programming language. PyPi hosts thousands of free, open source, third-party libraries that developers can use to add functionality to their projects. These libraries are installed using <code>pip</code>, a package manager for Python. Similar tools and repositories exist for other programming languages\u2014like <code>npm</code> for JavaScript, <code>cargo</code> for Rust, or <code>maven</code> for Java.</p> <p>To ensure your dependencies are always correctly installed and available, in standard Python projects relying on <code>pip</code>, requirements are traditionally listed out in a <code>requirements.txt</code> file in the project's root directory. This file is committed to your project's version control history so that as your project adds or updates dependencies, it is reflected in the project's history. This allows anyone working on the project to quickly set up their environment by installing the necessary dependencies with the <code>pip install</code> command. The dev container configuration you setup will automatically install dependencies from <code>requirements.txt</code> when the container is created. This allows anyone working on the project to have a complete environment setup in one step: starting a dev container.</p> <p>In summary, the the <code>devcontainer.json</code> file specifies configuration for a consistent development environment using a Docker image. The <code>requirements.txt</code> file ensures all needed Python package for our project are installed when the container is created. Together, these files automate the process of setting up a developer environment, making it easier for you and others to work on the project. </p> <p>Lets establish your static website development environment:</p>"},{"location":"resources/MkDocs/tutorial/#step-1-add-development-container-configuration","title":"Step 1. Add Development Container Configuration","text":"<ol> <li>In VS Code, open the <code>comp423-course-notes</code> directory. You can do this via: File &gt; Open Folder.</li> <li>Install the Dev Containers extension for VS Code.</li> <li>Create a <code>.devcontainer</code> directory in the root of your project with the following file inside of this \"hidden\" configuration directory:</li> </ol> <p><code>.devcontainer/devcontainer.json</code></p> <p>The <code>devcontainer.json</code> file defines the configuration for your development environment. Here, we're specifying the following:</p> <ul> <li><code>name</code>: A descriptive name for your dev container.</li> <li><code>image</code>: The Docker image to use, in this case, the latest version of a Python environment. Microsoft maintains a collection of base images for many programming language environments, but you can also create your own!</li> <li><code>customizations</code>: Adds useful configurations to VS Code, like installing the Python extension. When you search for VSCode extensions on the marketplace, you will find the string identifier of each extension in its sidebar. Adding extensions here ensures other developers on your project have them installed in their dev containers automatically.</li> <li><code>postCreateCommand</code>: A command to run after the container is created. In our case, it will use <code>pip</code> to install the dependencies listed in <code>requirements.txt</code>.</li> </ul> <pre><code>{\n  \"name\": \"COMP423 Course Notes\",\n  \"image\": \"mcr.microsoft.com/devcontainers/python:latest\",\n  \"customizations\": {\n    \"vscode\": {\n      \"settings\": {},\n      \"extensions\": [\"ms-python.python\"]\n    }\n  },\n  \"postCreateCommand\": \"pip install -r requirements.txt\"\n}\n</code></pre>"},{"location":"resources/MkDocs/tutorial/#step-2-add-requirementstxt-python-dependency-configuration","title":"Step 2. Add <code>requirements.txt</code> Python Dependency Configuration","text":"<p><code>requirements.txt</code></p> <p>The <code>requirements.txt</code> file lists the Python dependencies needed for the project. It should be in your project's root directory. Here, you only need to include <code>mkdocs-material</code> pinned to a specific minor version its current release:</p> <pre><code>mkdocs-material~=9.5\n</code></pre> <p>Pinning a dependency to a minor version ensures that the project will use the latest fixes, called patch releases, within that version. For example, <code>~=9.5</code> allows automatic upgrades from <code>9.5.49</code> to <code>9.5.50</code>, but prevents upgrades to <code>9.6</code> or beyond, ensuring stability while still benefiting from bug fixes and minor improvements. In larger software projects, this practice is valuable for maintaining a stable and reproducible development environment.</p>"},{"location":"resources/MkDocs/tutorial/#step-3-reopen-the-project-in-a-vscode-dev-container","title":"Step 3. Reopen the Project in a VSCode Dev Container","text":"<p>Reopen the project in the container by pressing <code>Ctrl+Shift+P</code> (or <code>Cmd+Shift+P</code> on Mac), typing \"Dev Containers: Reopen in Container,\" and selecting the option. This may take a few minutes while the image is downloaded and the requirements are installed.</p> <p>Once your dev container setup completes, close the current terminal tab (trash can), open a new terminal pane within VSCode, and try running <code>python --version</code> to see your dev container is running a recent version of Python without much effort! (As of this writing: 3.13 released in October of 2024.)</p>"},{"location":"resources/MkDocs/tutorial/#part-3-creating-the-static-site-with-material-for-mkdocs","title":"Part 3. Creating the Static Site with Material for MkDocs","text":""},{"location":"resources/MkDocs/tutorial/#step-1-initialize-mkdocs","title":"Step 1. Initialize MkDocs","text":"<p>Run the following commands in your terminal (inside the container):</p> <pre><code>mkdocs new .\n</code></pre> <p>This command works because <code>mkdocs</code> is installed in the container as part of the <code>requirements.txt</code> setup in the previous section. The <code>mkdocs</code> subcommand creates the basic file structure for your site, including a default <code>mkdocs.yml</code> configuration file and a <code>docs</code> folder.</p> <p>What is the <code>.</code> in that command?</p> <p>You should recall this from COMP211! It refers to the current working directory the terminal's shell process is in.</p> <p>The official documentation for creating a new site, which goes into more depth than this tutorial, can be found in the official documenation for Material for MkDocs, if you are interested in additional coverage.</p>"},{"location":"resources/MkDocs/tutorial/#step-2-configure-your-site","title":"Step 2. Configure Your Site","text":"<p>The <code>mkdocs.yml</code> file is a YAML configuration file. YAML stands for \"Yet Another Markup Language\" and is commonly used in software projects for configuration due to its simplicity and human-readable format. Unlike JSON, which was used in the <code>.devcontainer.json</code> file, YAML doesn\u2019t require brackets or quotes for every element, making it cleaner and easier to read for large configurations. While JSON is more rigid and structured, YAML's flexibility and readability make it popular for configuration files, especially in tools like MkDocs, and cloud deployment tools you will soon learn about, such as GitHub Actions and Kubernetes.</p> <p>Edit <code>mkdocs.yml</code> to look like this:</p> <pre><code>site_name: COMP423 Course Notes\ntheme:\n  name: material\n</code></pre> <p>Edit the <code>index.md</code> markdown file in the <code>docs</code> directory:</p> <p><code>docs/index.md</code></p> <pre><code># Welcome to Your Name's Course Notes\n\nThis is my home page. I will use it to organize and share my course notes.\n</code></pre> <p>Of course, substitute your name in the title.</p> <p>Markdown is a lightweight markup language that allows you to format text using simple, readable syntax. Files written in Markdown typically use the .md extension. It\u2019s widely used in the software industry for documentation, readme files, blogs, and more due to its simplicity and flexibility.</p> <p>In a MkDocs project, all your pages will be written in Markdown and then processed by MkDocs to generate HTML files for your website. This means you can write plain text and use Markdown\u2019s syntax for headings, lists, links, and more, without dealing directly with HTML.</p> <p>Here are some key benefits of Markdown:</p> <ul> <li>It\u2019s easy to read and write.</li> <li>It works well with version control systems like Git.</li> <li>It\u2019s supported by many platforms, including GitHub, where Markdown is used for README files.</li> </ul> <p>To learn more about Markdown, check out this comprehensive guide.</p>"},{"location":"resources/MkDocs/tutorial/#step-3-preview-your-site-locally","title":"Step 3. Preview Your Site Locally","text":"<p>Run the following command to start a local development server:</p> <pre><code>mkdocs serve\n</code></pre> <p>The <code>serve</code> subcommand launches a local development web server that monitors your files for changes. This means if you edit any of the files, the server will automatically refresh the page in your browser to reflect those changes. </p> <p>Open your browser and navigate to <code>http://127.0.0.1:8000</code> to see your site. Woo! You should see the contents of your <code>docs/index.md</code> file rendered in beautiful HTML.</p> <p>What is <code>127.0.0.1</code>?</p> <p><code>127.0.0.1</code> is the \"loopback\" address, meaning it always refers to your own computer. This address is commonly used for testing web applications locally. It also means that you cannot share your local development work directly (without others without using some additional tricks beyond our concerns right now). In the next section, you will learn how to deploy a production grade version of this site live on the web.</p> <p>To stop the server, return to your terminal and press <code>Ctrl+C</code>. This will terminate the <code>mkdocs serve</code> process.</p>"},{"location":"resources/MkDocs/tutorial/#part-4-deploying-with-github-pages","title":"Part 4. Deploying with GitHub Pages","text":"<p>In this section, you will set up an automated deployment process using GitHub Actions, a tool for Continuous Integration and Continuous Deployment (CI/CD). Setting up CI/CD for your project means that every time you make changes and push them to GitHub, a series of automated steps will run to \"test\", build, and deploy your website. This ensures your site is always up-to-date with the latest changes you make.</p> <p>Here\u2019s what you're about to do to configure this repository's CI/CD pipeline:</p> <ol> <li>Add a GitHub Actions Workflow: You'll create a workflow file that defines the automated steps for building and deploying your site on GitHub.</li> <li>Push Changes and Test the Workflow: Once everything is set up, you'll push your changes to GitHub and observe the deployment process in action.</li> </ol>"},{"location":"resources/MkDocs/tutorial/#setting-up-a-github-action-for-cicd","title":"Setting up a GitHub Action for CI/CD","text":""},{"location":"resources/MkDocs/tutorial/#step-1-add-a-github-action-for-cicd","title":"Step 1. Add a GitHub Action for CI/CD","text":"<p>Create a <code>.github/workflows/ci.yml</code> file in your repository. You can do this from a Terminal in your dev container with the commands:</p> <pre><code>mkdir -p .github/workflows\ncode .github/workflows/ci.yml\n</code></pre> <p>This file defines the steps for the CI/CD workflow.</p> <p><code>.github/workflows/ci.yml</code></p> <pre><code>name: Deploy to GitHub Pages\n\non:\n  push:\n    branches:\n      - main\n\npermissions:\n  contents: write\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.x'\n      - name: Install dependencies\n        run: pip install -r requirements.txt\n      - name: Build and Deploy\n        run: mkdocs gh-deploy --force\n</code></pre> <ul> <li><code>on.push.branches</code>: Specifies that this workflow should run whenever changes are pushed to the <code>main</code> branch.</li> <li><code>permisisons.contents</code>: Specifies the GitHub Action will be able to write back to this repository, needed to update the <code>gh-pages</code> branch the static site content will generate to.</li> <li><code>steps</code>: These define the tasks to perform, such as checking out the code, setting up Python, installing dependencies, building the site, and deploying it. The details of GitHub action configuration are beyond our scope, but you can find the full GitHub Action documentation here.</li> </ul> <p>What does the step running <code>mkdocs gh-deploy --force</code> do?</p> <p>The <code>mkdocs gh-deploy --force</code> command builds your site into static files and pushes them to the <code>gh-pages</code> branch of your repository. This is a subcomand built into <code>mkdocs</code>. The <code>--force</code> flag ensures that any existing content in the <code>gh-pages</code> branch is overwritten, guaranteeing that the latest version of your site is deployed.</p>"},{"location":"resources/MkDocs/tutorial/#step-2-push-changes-and-deploy","title":"Step 2. Push Changes and Deploy","text":"<p>Now that your configuration is ready, let\u2019s test it out:</p> <p>(1) Add and commit your changes:</p> <pre><code>git add .\ngit commit -m \"Set up Material for MkDocs and GitHub Action\"\n</code></pre> <p>(2) Push the changes to GitHub:</p> <pre><code>git push origin main\n</code></pre> <p>(3) Navigate to the Actions tab in your GitHub repository to watch the workflow run. You\u2019ll see each step execute, and once the process completes, your site will be deployed to GitHub Pages.</p> <p>(4) After the GitHub Action completes, navigate to the <code>gh-pages</code> branch in your GitHub repository to view the deployed static files. These files represent your website as hosted by GitHub Pages.</p> <p>(5) To ensure your repository's Github Site is served from the <code>gh-pages</code> branch, go to your repository's Settings &gt; Pages, and select the <code>gh-pages</code> branch as the source for your GitHub Pages site. Once set, your site will soon be live at the URL below.</p> <p>(6) Visit your site at <code>https://&lt;your-username&gt;.github.io/comp423-course-notes</code> to see the changes live. Sometimes this can take a minute to update.</p> <p>Congratulations! You\u2019ve automated your deployment process with CI/CD. Now, every time you push to <code>main</code>, or merge Pull Requests into <code>main</code>, these steps will automatically be carried out resulting in your static website being regenerated and deployed. This \"push-to-deploy\"/\"merge-to-deploy\" git workflow is common in many industrial settings.</p>"},{"location":"resources/MkDocs/tutorial/#understanding-your-cicd-workflow","title":"Understanding your CI/CD Workflow","text":"<p>Now that your deployment is automated, let\u2019s break down what happen step by step when you make changes to this project in the future:</p> <ol> <li> <p>Commit Changes Locally: You edit your site\u2019s files and save your changes in your local Git repository by creating a commit.</p> </li> <li> <p>Push to GitHub: When you push your changes to the <code>main</code> branch on GitHub, the CI/CD pipeline is triggered automatically.</p> </li> <li> <p>GitHub Action Starts: GitHub detects the new commit and starts running the workflow you defined in <code>.github/workflows/ci.yml</code>.</p> </li> <li> <p>Workflow Steps Execute:</p> <ul> <li>The repository code is checked out to the runner environment.</li> <li>A Python environment is set up, and the dependencies listed in <code>requirements.txt</code> are installed.</li> <li>MkDocs builds the static site, converting your Markdown files into an HTML website.</li> <li>The <code>mkdocs gh-deploy</code> command deploys the site to the <code>gh-pages</code> branch.</li> </ul> </li> <li> <p>Site is Updated: Once the workflow completes, your GitHub Pages site automatically reflects the latest changes.</p> </li> </ol>"},{"location":"resources/MkDocs/tutorial/#why-this-matters-beyond-this-tutorial","title":"Why This Matters Beyond This Tutorial","text":"<p>CI/CD is not just for static sites\u2014it\u2019s a critical practice in modern software development. It ensures your code is tested, built, and delivered efficiently and consistently. This approach reduces manual work, catches errors early, and speeds up development.</p> <p>For example:</p> <ul> <li>In large software projects, CI/CD pipelines run tests on every commit to ensure code quality.</li> <li>For applications, pipelines can build and deploy to staging or production environments automatically.</li> </ul> <p>By setting up this workflow, you\u2019ve taken a first step toward understanding these industry-standard practices and how automation can enhance the reliability of your projects.</p>"},{"location":"resources/MkDocs/tutorial/#conclusion","title":"Conclusion","text":"<p>Congratulations! You\u2019ve successfully created a static website for your course notes using Material for MkDocs, configured a development environment, and set up automated deployment. This foundational skill can be applied to many open-source and professional projects. </p> <p>In the first exercise, you will expand your site with more pages and customizations!</p>"},{"location":"resources/backend-architecture/0-layered-architecture/","title":"0. Layered Architecture: Separating Concerns in Software Design","text":""},{"location":"resources/backend-architecture/0-layered-architecture/#introduction","title":"Introduction","text":"<p>One of the oldest and most enduring patterns in software engineering is a layered architecture. This pattern enforces a structured separation of concerns, ensuring that each layer of a system only depends on the layer directly beneath it. This approach provides a clean interface between layers and serves as a barrier of abstraction, helping to manage complexity in large systems.</p> <p>A crucial characteristic of layered architecture is that lower layers should have no explicit awareness of higher layers. This means that a foundational layer does not (and should not) know of the higher-level layer that is built on top of it.</p> <p>In this chapter, we will explore layered architecture in the context of building a RESTful backend API using FastAPI. Our goal is to introduce a new layer: the business logic services layer, and understand why it exists and how it interacts with the other layers. Our HTTP API layer will sit above your services layer.</p>"},{"location":"resources/backend-architecture/0-layered-architecture/#what-is-business-logic-and-why-is-it-called-that","title":"What is Business Logic and Why is it Called That?","text":"<p>The services layer is commonly referred to as business logic because it encapsulates the domain-specific rules and workflows that define how an application operates. The term \"business logic\" stems from the idea that this layer contains the core operations that are specific to the application's purpose or domain, regardless of the technical details of how the system is accessed or stored.</p> <p>More precisely, business logic models the application's concerns using simpler, domain-specific objects rather than exposing technical implementation details. By defining these rules in a dedicated layer, we create a more maintainable and adaptable system. This separation allows us to reason about and modify the business rules independently from concerns like HTTP, databases, or external APIs.</p> <p>The HTTP routing layer serves as a facade or translator between the external world and the business logic. This means that it handles incoming requests, extracts necessary parameters, calls the relevant business logic methods, and then formats the output for delivery as an HTTP response. This layer should contain minimal logic of its own, acting only as an interface to the core functionality contained within the business logic layer.</p>"},{"location":"resources/backend-architecture/0-layered-architecture/#motivation-for-separating-concerns","title":"Motivation for Separating Concerns","text":"<p>So far, we have learned about FastAPI routes and how they handle HTTP requests. However, if we write all of our application logic directly inside route handlers, several problems arise:</p> <ol> <li>Tightly Coupled Code \u2013 Routes become entangled with business logic, making the system difficult to modify or extend.</li> <li>Lack of Reusability \u2013 Business logic cannot be easily reused in different contexts, such as CLI tools or background tasks.</li> <li>Difficult Testing \u2013 Testing business rules independently becomes challenging when they are mixed with HTTP concerns.</li> <li>Error Handling Complexity \u2013 If errors are raised directly within route handlers, managing appropriate HTTP responses becomes messy and inconsistent.</li> </ol> <p>To address these issues, we introduce a business logic services layer, which abstracts core application logic away from the HTTP concerns handled by FastAPI routes.</p>"},{"location":"resources/backend-architecture/0-layered-architecture/#layered-architecture-in-our-api","title":"Layered Architecture in Our API","text":"<p>Our API will follow a three-layered architecture:</p> <ol> <li>Routes Layer (Facade/Translator Layer): Responsible for handling HTTP requests and responses. Routes gather inputs, call services, and return results.</li> <li>Service Layer (Business Logic Layer): Contains core domain logic, independent of FastAPI. This layer operates on domain-specific objects and ensures that all application logic is encapsulated here.</li> <li>Persistence Layer (Coming Soon): Handles persistence and retrieval of data, usually via a database. Services interact with this layer but do not concern themselves with storage details. At this stage in the course, we will not concern ourselves with persistence. However, know that it is on the horizon and will be another layer!</li> </ol>"},{"location":"resources/backend-architecture/0-layered-architecture/#simplified-example-user-lookup","title":"Simplified Example: User Lookup","text":"<p>Consider the following, hand-waving example of what this architectural strategy is trying to avoid:</p>"},{"location":"resources/backend-architecture/0-layered-architecture/#mainpy-without-a-services-layer-bad-practice","title":"<code>main.py</code> Without a Services Layer (Bad Practice)","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom models import User\n\napp = FastAPI()\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    user = database.find_user_by_id(user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n    return user\n</code></pre> <p>In this example, the route directly accesses the database, performs a lookup, and raises an HTTPException. This mixes business logic (checking if the user exists) with FastAPI-specific concerns (HTTPException and status codes).</p>"},{"location":"resources/backend-architecture/0-layered-architecture/#mainpy-with-a-services-layer-better-practice","title":"<code>main.py</code> with a Services Layer (Better Practice)","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom services import UserService, UserDoesNotExistError\nfrom models import User\n\napp = FastAPI()\nuser_service = UserService()\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    try:\n        return user_service.get_user(user_id)\n    except UserDoesNotExistError:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n</code></pre>"},{"location":"resources/backend-architecture/0-layered-architecture/#servicespy-the-service-layer-better-practice","title":"<code>services.py</code> the Service Layer (Better Practice)","text":"<pre><code>from models import User\n\nclass UserDoesNotExistError(Exception):\n    pass\n\nclass UserService:\n    def get_user(self, user_id: int) -&gt; User:\n        user = database.find_user_by_id(user_id)\n        if not user:\n            raise UserDoesNotExistError(\"User does not exist\")  # Raises a domain-specific error\n        return user\n</code></pre>"},{"location":"resources/backend-architecture/0-layered-architecture/#key-improvements","title":"Key Improvements:","text":"<ol> <li>The service layer has no knowledge of HTTP. It operates purely on domain-specific objects and raises domain-specific errors (e.g., <code>UserDoesNotExistError</code>).</li> <li>The route handler translates errors into HTTP responses, keeping the API\u2019s interface clean and predictable.</li> <li>Business logic is reusable. The UserService could be used elsewhere, such as in a background job, without modification.</li> </ol>"},{"location":"resources/backend-architecture/0-layered-architecture/#sequence-diagram-of-layers","title":"Sequence Diagram of Layers","text":"<p>Study the following diagram to understand how these layers serve specific purposes and separate concerns.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant API Routing\n    participant Service\n\n    Client-&gt;&gt;API Routing: HTTP GET /users/1\n    API Routing-&gt;&gt;Service: UserService.get_user(1) \n    Service--&gt;&gt;API Routing: User Data Model or throws UserDoesNotExistError\n    API Routing--&gt;&gt;Client: 200 OK (JSON user data) or 404 Not Found</code></pre>"},{"location":"resources/backend-architecture/0-layered-architecture/#characteristics-of-a-well-separated-service-layer","title":"Characteristics of a Well-Separated Service Layer","text":"<p>When designing a services layer, keep the following principles in mind:</p> <ul> <li>No FastAPI-specific imports: The service layer should not reference HTTP concerns (e.g., <code>HTTPException</code>).</li> <li>Works with domain models: Services should receive and return domain-specific objects (e.g., Pydantic models) instead of raw dictionaries or ORM objects.</li> <li>Handles business rules: Any domain-specific logic (e.g., checking if a user has permission to perform an action) should reside in the service layer.</li> <li>Raises domain-specific errors, not HTTP exceptions: The service layer should raise application-specific exceptions, like <code>UserDoesNotExistError</code> or <code>ValueError</code>. The routes layer should handle them and translate them into proper HTTP responses.</li> </ul>"},{"location":"resources/backend-architecture/0-layered-architecture/#looking-ahead-dependency-injection","title":"Looking Ahead: Dependency Injection","text":"<p>So far, our <code>UserService</code> is instantiated directly inside our routes file. This works but is not scalable. In the next reading, you will be introduced to Dependency Injection, a technique that allows us to cleanly manage dependencies like <code>UserService</code> while keeping our code modular and testable.</p>"},{"location":"resources/backend-architecture/1-dependency-injection/","title":"1. Introduction to Dependency Injection in FastAPI","text":""},{"location":"resources/backend-architecture/1-dependency-injection/#what-is-dependency-injection","title":"What is Dependency Injection?","text":"<p>Dependency Injection (DI) is a widely used design pattern that promotes modular, testable, and maintainable code. It is a core principle in many modern application frameworks across various programming languages, including Java (Spring), Python (FastAPI), and TypeScript (Angular). The primary idea behind DI is instead of you constructing dependencies inside a function or class body, you declare them as special parameters. When the application framework calls your function, such as a route, its DI system constructs the argument values behind the scenes and \"injects\" them as arguments. This process is called dependency injection.</p> <p>This concept plays a role in modern layered architectures like you just read about. We previously introduced a business logic services layer that encapsulates domain-specific logic and separates it from the routing layer (which handles HTTP requests and responses). Dependency Injection provides a clean and structured way to introduce and manage dependencies between these layers, keeping them loosely coupled and testable.</p>"},{"location":"resources/backend-architecture/1-dependency-injection/#why-use-dependency-injection","title":"Why Use Dependency Injection?","text":"<p>Dependency Injection helps solve common software design challenges, making applications:</p> <ul> <li>More Maintainable: By loosely coupling dependencies between parts of a system, changes in one part of the application don\u2019t require modifying other parts. Loosely coupled components make it easier to replace or upgrade individual parts of a system without affecting the rest. This reduces the risk of unintended side effects and promotes a more modular and extensible architecture.</li> <li>Easier to Test: With DI, dependencies can be replaced with mock implementations, making unit tests isolated and reliable.</li> <li>More Flexible: By programming to an interface rather than a concrete implementation, different implementations of a dependency can be injected dynamically, allowing for easy configuration changes.</li> <li>Reduces Code Duplication: Centralizing dependency management prevents repeated instantiation of services throughout the codebase.</li> </ul> <p>You\u2019ve actually already encountered DI in FastAPI! Every time a request includes path parameters, query parameters, or request bodies... where did the argument values come from? FastAPI injected them into your route handlers automatically! Now, let\u2019s take this one step further: what if we wanted to inject a custom service into our application to handle business logic?</p>"},{"location":"resources/backend-architecture/1-dependency-injection/#how-does-dependency-injection-work-in-fastapi-routing","title":"How Does Dependency Injection Work in FastAPI Routing?","text":"<p>When a request is received in a FastAPI application, the following steps occur:</p> <ol> <li>Request Routing: FastAPI matches the incoming request's URL and HTTP method to the appropriate route handler function.</li> <li>Dependency Resolution: Before calling the route function, FastAPI checks for any declared dependencies using <code>Depends()</code>. It determines what dependencies are needed and resolves how to instantiate them in a correct order. An injected dependency may have its own injected dependencies that need to be resolved and instantiated first!</li> <li>Dependency Instantiation: If a dependency is a class or function, FastAPI instantiates it (if needed) and injects it into the route function.</li> <li>Function Execution: The route function is called with the injected dependencies passed in as arguments to the routed function's parameters.</li> </ol>"},{"location":"resources/backend-architecture/1-dependency-injection/#tutorial-dependency-injection-in-fastapi","title":"Tutorial: Dependency Injection in FastAPI","text":"<p>To follow along with this quick tutorial on dependency injection, from your host machine's terminal clone the course FastAPI Tutorial repository again, but name the cloned directory <code>di-tutorial</code>:</p> <pre><code>git clone https://github.com/comp423-26s/fastapi-tutorial di-tutorial\n</code></pre> <p>The last argument of <code>di-tutorial</code> is what causes <code>git</code> to clone to a specific directory name on your machine.</p> <p>GitHub's <code>gh</code> CLI Program</p> <p>Now that you are comfortable with fundamental <code>git</code> commands, you may want to install GitHub's <code>gh</code> tool on your host machine. Instructions here: https://cli.github.com</p> <p>The <code>gh</code> tool allows you to interact with GitHub's REST API from your command line. You can do things like create new GitHub repositories, list issues, and nearly anything you can do from the GitHub web page.</p> <p>Once you have <code>gh</code>, you can achieve the clone command above with:</p> <pre><code>gh repo clone comp423-26s/fastapi-tutorial di-tutorial\n</code></pre> <p>Open the repo directory in a VS Code Dev Container.</p>"},{"location":"resources/backend-architecture/1-dependency-injection/#step-1-defining-models","title":"Step 1: Defining Models","text":"<p>Let's implement a Rock, Paper, Scissors API! Create a new file in the project's root directory named <code>models.py</code>. We'll define our Pydantic data models here. Review the code below and then copy it into <code>models.py</code>:</p> models.py<pre><code>from enum import Enum\nfrom datetime import datetime\nfrom typing import Annotated, TypeAlias\nfrom pydantic import BaseModel, Field\n\n\nclass Choice(str, Enum):\n    rock = \"rock\"\n    paper = \"paper\"\n    scissors = \"scissors\"\n\n\nChoiceField: TypeAlias = Annotated[\n    Choice,\n    Field(\n        description=\"Choice of rock, paper, or scissors.\",\n        examples=[\"rock\", \"paper\", \"scissors\"],\n    ),\n]\n\n\nclass GamePlay(BaseModel):\n    user_choice: ChoiceField\n\n\nclass GameResult(BaseModel):\n    timestamp: Annotated[datetime, Field(description=\"When the game was played.\")]\n    user_choice: ChoiceField\n    api_choice: ChoiceField\n    user_wins: Annotated[bool, Field(description=\"Did the user win the game?\")]\n</code></pre> <p>Using a <code>TypeAlias</code> for repeated type annotations</p> <p>Notice <code>ChoiceField</code> is defined as a <code>TypeAlias</code> for the annotated type of <code>Choice</code> such that it contains the <code>Field</code> information with an API description.</p> <p>When you find an annotated type is repeated in multiple places in your Pydantic models or FastAPI routes, using a <code>TypeAlias</code> to cut down on the repetition and make the code more readable is a best practice.</p> <p>In Python, a protocol is similar to an interface in Java. It defines a contract that a class must follow without enforcing inheritance. This allows for better flexibility and testability.</p>"},{"location":"resources/backend-architecture/1-dependency-injection/#step-2-defining-a-game-service","title":"Step 2: Defining a <code>Game</code> Service","text":"<p>Now let's define a service class that handles the serious \"business logic\" of rock paper scissors. Review the contents below and copy the contents to a new file named <code>services.py</code>:</p> services.py<pre><code>from datetime import datetime\nfrom random import choice as random_choice\nfrom models import GamePlay, GameResult, Choice\n\n\nclass GameService:\n    \"\"\"Service for processing game plays.\n\n    This class provides functionality to simulate a game between a user and the API.\n    \"\"\"\n\n    def play(self, gameplay: GamePlay) -&gt; GameResult:\n        \"\"\"Play a game round.\n\n        Args:\n            gameplay (GamePlay): An object encapsulating the user's choice.\n\n        Returns:\n            GameResult: The outcome of the game including user and API choices, and win flag.\n        \"\"\"\n        api_choice: Choice = self._random_choice()\n\n        return GameResult(\n            timestamp=datetime.now(),\n            user_choice=gameplay.user_choice,\n            api_choice=api_choice,\n            user_wins=self._does_user_win(gameplay.user_choice, api_choice),\n        )\n\n    def _random_choice(self) -&gt; Choice:\n        \"\"\"Select a random choice for the API.\n\n        Returns:\n            Choice: A randomly chosen game option.\n        \"\"\"\n        return random_choice(list(Choice))\n\n    def _does_user_win(self, user_choice: Choice, api_choice: Choice) -&gt; bool:\n        \"\"\"Determine if the user wins based on choices.\n\n        Args:\n            user_choice (Choice): The user's chosen option.\n            api_choice (Choice): The API's chosen option.\n\n        Returns:\n            bool: True if the user wins, False otherwise.\n        \"\"\"\n        result: tuple[Choice, Choice] = (user_choice, api_choice)\n        winning_results: set[tuple[Choice, Choice]] = {\n            (Choice.rock, Choice.scissors),\n            (Choice.paper, Choice.rock),\n            (Choice.scissors, Choice.paper),\n        }\n        return result in winning_results\n</code></pre> <p>Notice that this <code>services.py</code> module knows nothing about HTTP or FastAPI. Its imports are data models and some library functionality for randomization. This is just plain-old Python! This is the \"core\" logic of our little app, though, and you can easily imagine how writing unit tests for it would be straightforward.</p>"},{"location":"resources/backend-architecture/1-dependency-injection/#step-3-establishing-a-fastapi-route-to-play-the-game","title":"Step 3: Establishing a FastAPI Route to Play the Game!","text":"<p>Now that we have a service defined, how do we add a route that uses dependency injection to utilize it? Similar to how we declare parameters of routes that are populated by dynamic <code>Path</code> parts, <code>Query</code> parameters, or <code>Body</code> payloads.</p> <p>What HTTP method would you choose for the game playing REST API endpoint?</p> <p>We will model playing a round of this game with a <code>POST</code> method. Even though we are not (yet) storing a history of games or creating anything, playing a game is not idempotent: we get back a new result each time we play. Not only do the <code>api_choice</code> and <code>user_wins</code> fields update, the <code>timestamp</code> reflects the latest game play.</p>"},{"location":"resources/backend-architecture/1-dependency-injection/#starting-without-dependency-injection","title":"Starting Without Dependency Injection","text":"<p>Update your <code>main.py</code> file to reflect the following. Note: this example does not yet rely upon dependency injection! We will refactor this to make use of dependency injection next.</p> main.py<pre><code>\"\"\"FastAPI main entrypoint file.\"\"\"\n\nfrom typing import Annotated, TypeAlias\nfrom fastapi import FastAPI, Body, Depends\nfrom models import GamePlay, GameResult\nfrom services import GameService\n\napp = FastAPI()\n\n\n@app.post(\"/play\")\ndef play(\n    user_choice: Annotated[\n        GamePlay,\n        Body(description=\"User's choice of rock, paper, or scissors.\"),\n    ],\n) -&gt; GameResult:\n    # Here we construct a GameService *without* dependency injection...\n    game_svc: GameService = GameService() # (1)!\n    return game_svc.play(user_choice)\n</code></pre> <ol> <li>Notice that fully inside the body of this function is where we declare a local variable of type <code>GameService</code> and construct it. If we later wanted to use a different object, which conformed to the interface <code>GameService</code> implements, how would we do so? We couldn't without changing this source code! Being able to swap out implementations is very useful in one common software engineering practice we will soon embrace: unit testing. In unit testing, to isolate the behavior of a single function, dependency injection gives you the ability to substitute fake dependencies in such that you are only testing the unit(s) of code you care about.</li> </ol> <p>Check for understanding: Why is line 19 problematic? Why do we want to use dependency injection instead?</p> <p>Try to answer this question for yourself before clicking the annotation symbol at the end of line 19 to reveal the answer.</p>"},{"location":"resources/backend-architecture/1-dependency-injection/#refactor-to-dependency-injection","title":"Refactor to Dependency Injection","text":"<p>The updated definition of the <code>play</code> function provides an example with FastAPI's dependency injection utilized:</p> main.py<pre><code>@app.post(\"/play\")\ndef play(\n    user_choice: Annotated[\n        GamePlay,\n        Body(description=\"User's choice of rock, paper, or scissors.\"),\n    ],\n    game_svc: Annotated[GameService, Depends()],\n) -&gt; GameResult:\n    return game_svc.play(user_choice)\n</code></pre> <p>Be sure to run the FastAPI server and try out the route from the OpenAPI <code>/docs</code> user interface!</p> <p>Notice on line 17 we added an additional parameter to the <code>play</code> function definition. Its type is <code>Annotated[GameService, Depends()]</code>. The <code>Depends()</code> call is what declaratively signals to FastAPI this is a dependency injected parameter. How does it know to construct an instance of <code>GameService</code>? Because it's annotating the type <code>GameService</code>. </p> <p>There are other ways of using <code>Depends</code>, too, like giving it a factory function and specifying the construction elsewhere. You can also specify the annotated type to be a <code>Protocol</code> (similar to a Java interface) and giving a concrete classname as an argument to <code>Depends</code>. That's how COMP301 should have taught you to approach a similar problem. However, we will adhere to a software engineering goal: don't overengineer until you have good reason to!</p> <p>This is dependency injection! There is a HUGE win here: your dependency is now a parameter passed in, or injected, from the outside. It is not hardwired in to the route body. Thus, if you wanted to unit test this function, you could easily supply a mock instance of a <code>GameService</code> and isolate the function's behavior. That said, this example is so trivial that the notion of isolating it for a unit test is a bit silly. </p>"},{"location":"resources/backend-architecture/1-dependency-injection/#step-4-adding-functionality","title":"Step 4. Adding Functionality","text":"<p>Let's record a history of games played since the service was last restarted. We will use global module memory for this, but realize this is only a stopgap solution until we learn more about data persistence.</p> services.py<pre><code># ... the import statements above remain the same ...\n\n# This is *NOT* a database, just a hack for now...\n_db: list[GameResult] = []\n\nclass GameService:\n    \"\"\"Service for processing game plays.\n\n    This class provides functionality to simulate a game between a user and the API.\n    \"\"\"\n\n    def play(self, gameplay: GamePlay) -&gt; GameResult:\n        \"\"\"Play a game round.\n\n        Args:\n            gameplay (GamePlay): An object encapsulating the user's choice.\n\n        Returns:\n            GameResult: The outcome of the game including user and API choices, and win flag.\n        \"\"\"\n        api_choice: Choice = self._random_choice()\n        result = GameResult(\n            timestamp=datetime.now(),\n            user_choice=gameplay.user_choice,\n            api_choice=api_choice,\n            user_wins=self._does_user_win(gameplay.user_choice, api_choice),\n        )\n        _db.append(result)\n        return result\n\n    def get_results(self) -&gt; list[GameResult]:\n        \"\"\"Get all game results.\n\n        Returns:\n            list[GameResult]: A list of all game results.\n        \"\"\"\n        return _db\n\n    # ... the \"private\" helper methods remain the same ...\n</code></pre> <p>Notice, we are using a simple global variable in the module to store results. Why not use an instance variable in the <code>GameService</code>? FastAPI's dependency injection system constructs a new instance of <code>GameService</code> on each request. With a little more effort we could get around this with something like the singleton design pattern, to ensure only one instance of <code>GameService</code> is shared across all requests, but that's beyond the scope of this tutorial.</p> <p>Let's add a route for listing the history of games played to <code>main.py</code>.</p> main.py<pre><code>@app.get(\"/results\")\ndef log(game_svc: Annotated[GameService, Depends()]) -&gt; list[GameResult]:\n    return game_svc.get_results()\n</code></pre> <p>After saving, your FastAPI server reloads so global memory is cleared. Try playing a few games and then trying out your <code>/results</code> route. You can access it both from the <code>/docs</code> UI as well as from the browser directly, since the route's method is <code>GET</code>.</p>"},{"location":"resources/backend-architecture/1-dependency-injection/#cleaning-up-the-types","title":"Cleaning Up the Types","text":"<p>There's one last minor tweak to make to help clean this up. You will find this useful as you write many routes which depend on the same service. Let's use a <code>TypeAlias</code> for our dependency injected <code>GameService</code> rather than repeat this annotated type everywhere. Try making the following changes in <code>main.py</code>:</p> main.py<pre><code>\"\"\"FastAPI main entrypoint file.\"\"\"\n\nfrom typing import Annotated, TypeAlias\nfrom fastapi import FastAPI, Body, Depends\nfrom models import GamePlay, GameResult\nfrom services import GameService\n\napp = FastAPI()\n\nGameServiceDI: TypeAlias = Annotated[GameService, Depends()]\n\n\n@app.post(\"/play\")\ndef play(\n    user_choice: Annotated[\n        GamePlay,\n        Body(description=\"User's choice of rock, paper, or scissors.\"),\n    ],\n    game_svc: GameServiceDI,\n) -&gt; GameResult:\n    return game_svc.play(user_choice)\n\n\n@app.get(\"/results\")\ndef log(game_svc: GameServiceDI) -&gt; list[GameResult]:\n    return game_svc.get_results()\n</code></pre> <p>Ah, that's not only a little easier on the eyes, but since the <code>TypeAlias</code> is defined in one place we could now customize <code>Depends()</code> and have more control over how the <code>GameService</code> dependency gets injected across all of these routes. If we wanted to move toward a singleton pattern, for example, we could do so here.</p>"},{"location":"resources/backend-architecture/1-dependency-injection/#the-power-of-dependency-injection-in-fastapi","title":"The Power of Dependency Injection in FastAPI","text":"<p>Congratulations on completing a first foray into dependency injection (DI) in FastAPI\u2014starting from its core principles and working through a hands-on example with a Rock, Paper, Scissors. You\u2019ve now seen how DI promotes modularity, testability, and maintainability in your applications. Instead of hardwiring dependencies, we leveraged FastAPI's <code>Depends()</code> function to keep our code clean and flexible.</p> <p>Through this tutorial, you've learned how to:</p> <ul> <li>Define business logic services that remain independent of the HTTP framework.</li> <li>Use FastAPI\u2019s DI system to inject dependencies in a structured way.</li> <li>Improve testability by allowing easy substitution of dependencies.</li> <li>Reduce code duplication and enhance maintainability.</li> </ul> <p>In time, you will learn some more advanced uses of DI in FastAPI:</p> <ul> <li>Services which inject other services into their constructors. This works just like you'd expect, but still feels magical! Since our services will ultimately depend on a database layer, we will inject the database dependencies into the service.</li> <li>A nicer way of declaring routes which have many query parameters using <code>Depends</code>. Read more here.</li> <li>Singleton dependencies which have only one instance shared across all requests.</li> </ul> <p>By adopting dependency injection, you're setting yourself up for scalable and maintainable application development. Whether you're working on a small personal project or a large-scale system, mastering DI ensures that your code remains clean, modular, and future-proof. Now, take what you\u2019ve learned, and complete exercise 01's service layer using dependency injection best practices!</p>"},{"location":"resources/backend-architecture/2-testing/","title":"2. Introduction to Testing","text":""},{"location":"resources/backend-architecture/2-testing/#why-is-testing-important","title":"Why is Testing Important?","text":"<p>Software can be fragile. One small change can break something else without warning. Testing brings stability and trust. It confirms that your code works as expected, your bug fixes stick, and that new features don\u2019t undo old ones. There are many reasons for testing software, but some of the top reasons for testing are:</p> <ul> <li>Acceptance and Verification: Tests show that your software meets user needs and business goals. You can point to a test and say, \u201cYes, this works as intended.\u201d</li> <li>Regression Prevention: With automated tests, you\u2019ll know if new code breaks existing functionality. This saves you from introducing the same bug again and again. When you fix a bug, writing a test locks in the fix. If that bug ever shows up again, the test fails and alerts you.</li> <li>Confidence in Code Changes: With good test coverage, refactoring or adding new features feels safer. You don\u2019t have to worry as much about sneaky breakage.</li> </ul> <p>A great test suite isn\u2019t just about catching bugs\u2014it transforms the way you write code. When you have fast, reliable tests, you can experiment freely, refactor fearlessly, and build new features with confidence. Instead of worrying about whether a change might break something unexpected, you get immediate feedback. Green tests? You\u2019re good to go. Red tests? You know exactly where to look. This is especially valuable in a team setting where you might not have written the original code or its tests. With a strong test suite, you don\u2019t have to rely on gut instinct or deep dives into unfamiliar code; the tests will tell you if your changes introduce problems.</p> <p>Good tests also reinforce clean abstraction layers. When a module or API has solid test coverage, you can trust its interface without constantly checking its implementation. This means you can operate at a higher level, focusing on solving the problem at hand rather than getting bogged down in lower-level details. It\u2019s a massive boost to developer productivity and experience\u2014when the foundational layers are proven to work well with testing, and they \"just work\" in practice, you can build on top of them with confidence.</p>"},{"location":"resources/backend-architecture/2-testing/#types-of-software-tests","title":"Types of Software Tests","text":"<p>Not all tests are created equal, and choosing the right type of test for a given scenario is crucial. A well-balanced test suite includes different types of tests, each serving a specific purpose. Understanding their strengths, limitations, and trade-offs allows you to write intentional tests\u2014tests that prove something meaningful and valuable about your codebase.</p>"},{"location":"resources/backend-architecture/2-testing/#the-importance-of-intentional-testing","title":"The Importance of Intentional Testing","text":"<p>Testing isn't just about writing tests for the sake of it; it's about proving that your code behaves correctly under the conditions that matter most. Before writing a test, ask yourself:</p> <ul> <li>What am I trying to prove?   Are you checking that a function returns the right output? That two components communicate correctly? That your system handles heavy load?  </li> <li>Why is this test valuable?   Does it provide meaningful feedback? Will it catch real-world issues?  </li> <li>Where should I set the test\u2019s boundaries?   Should I isolate a single function, or does the value come from testing multiple pieces together?  </li> </ul> <p>When you\u2019re intentional about testing, you avoid redundant or low-value tests and focus on what truly increases confidence in your software.</p>"},{"location":"resources/backend-architecture/2-testing/#common-types-of-tests","title":"Common Types of Tests","text":"<p>Each type of test provides different insights into your software. Here\u2019s a breakdown of the most common ones:</p> <ul> <li> <p>Unit Tests Unit tests focus on individual functions or methods in isolation. They are fast, reliable, and help confirm that small pieces of logic work correctly. If a unit test fails, you know exactly where the problem is. </p> <ul> <li>Strengths: Quick to run, easy to debug, useful for pinpointing issues in logic.  </li> <li>Limitations: They only test small pieces of the system and don\u2019t guarantee that components work well together.  </li> <li>Best for: Business logic, calculations, pure functions, and methods with clear inputs and outputs.  </li> </ul> </li> <li> <p>Integration Tests These verify that multiple parts of the system work together as expected. An integration test might check how your API interacts with a service, or how a service interacts with a database, or, generally whether two or more units of code coordinate correctly.  </p> <ul> <li>Strengths: Helps catch real-world interaction issues that unit tests miss.  </li> <li>Limitations: If integration test includes external systems, they are slower than unit tests. Failures can be harder to diagnose since scope is inclusive of many parts of the system.  </li> <li>Best for: Dependencies between services, database interactions, API calls, authentication flows, and so on.  </li> </ul> </li> <li> <p>End-to-End (E2E) Tests E2E tests simulate a user\u2019s experience through the entire system, from frontend clicks to backend responses and database updates. These tests confirm that everything works together in a real-world scenario.  </p> <ul> <li>Strengths: Provides high confidence that the entire system functions correctly.  </li> <li>Limitations: Slow, complex to set up, and brittle\u2014small UI changes can break them.  </li> <li>Best for: Critical user flows, such as signups, payments, and login/logout sequences.  </li> </ul> </li> <li> <p>Performance Tests / Profiling Performance tests, often technically referred to as profiling, measure how fast the system responds and how efficiently it handles requests. These tests help ensure that the software remains performant under normal and peak conditions.  </p> <ul> <li>Strengths: Helps detect slow response times, memory leaks, and bottlenecks.  </li> <li>Limitations: Requires specialized tools and environments to get accurate results.  </li> <li>Best for: Response time benchmarks, caching strategies, and database query optimization.  </li> </ul> </li> <li> <p>Load Tests A type of performance test that determines how well the system handles high levels of traffic. Load tests simulate many users interacting with the system simultaneously to expose potential crashes, slowdowns, race conditions and more.  </p> <ul> <li>Strengths: Helps anticipate scaling issues before they impact real users.  </li> <li>Limitations: Can be expensive and time-consuming to run effectively.  </li> <li>Best for: Large-scale web applications, APIs, and cloud services.  </li> </ul> </li> <li> <p>Security Tests Security tests probe the system for vulnerabilities like SQL injection, cross-site scripting (XSS), and authentication flaws.  </p> <ul> <li>Strengths: Helps identify potential security risks before attackers do.  </li> <li>Limitations: Often requires specialized security expertise and tools.  </li> <li>Best for: Web applications, authentication systems, and applications that handle sensitive data.</li> </ul> </li> </ul>"},{"location":"resources/backend-architecture/2-testing/#making-testing-trade-offs","title":"Making Testing Trade-offs","text":"<p>There\u2019s no one-size-fits-all approach to testing. Each type has trade-offs: unit tests are fast but limited in scope, while E2E tests are comprehensive but slow. Writing too many of the wrong kinds of tests can be as bad as writing none at all.</p> <p>In this tutorial, we focus on unit and integration tests because they provide a strong balance of speed, reliability, and practical value. They help you validate business logic and confirm that your system components interact correctly without the overhead of full E2E testing.</p> <p>The key takeaway? Test intentionally. Every test you write should prove something important about your software.</p>"},{"location":"resources/backend-architecture/2-testing/#integration-testing-a-fastapi-backend","title":"Integration Testing a FastAPI Backend","text":"<p>Integration tests verify that different parts of your system work together correctly. For our FastAPI application, this means testing the complete request flow: from HTTP request handling through routing, dependency injection, and down to service implementation.</p>"},{"location":"resources/backend-architecture/2-testing/#understanding-pytest","title":"Understanding <code>pytest</code>","text":"<p>Before diving into FastAPI testing, let's understand pytest - Python's premier testing framework. pytest uses simple conventions to discover and run tests:</p> <ul> <li>Test files must be named <code>test_*.py</code> or <code>*_test.py</code></li> <li>Test functions must start with <code>test_</code></li> </ul> <p>The <code>pytest</code> module is installed standard on Microsoft's Dev Container, but it's just <code>pip</code> package you can install and add to <code>requirements.txt</code>, on systems that do not bundle it. You can run <code>pytest</code> from the terminal in several ways:</p> <pre><code># Run all tests in current directory and subdirectories\npytest\n\n# Run tests with detailed output\npytest -v\n\n# Run tests in a specific file\npytest test_main.py\n\n# Run a specific test\npytest test_main.py::test_play_route\n</code></pre>"},{"location":"resources/backend-architecture/2-testing/#basic-fastapi-integration-test-setup","title":"Basic FastAPI Integration Test Setup","text":"<p>Let's start with a basic integration test for our Rock, Paper, Scissors game. Create a file named <code>test_main.py</code>:</p> test_main.py<pre><code>from fastapi.testclient import TestClient\nfrom main import app\n\nclient = TestClient(app)\n\ndef test_play_route_integration():\n    \"\"\"Test that the /play endpoint handles basic gameplay correctly.\"\"\"\n    response = client.post(\"/play\", json={\"user_choice\": \"rock\"})\n\n    # Verify HTTP-level details\n    assert response.status_code == 200\n    assert response.headers[\"content-type\"] == \"application/json\"\n\n    # Verify response structure\n    data = response.json()\n    assert \"user_choice\" in data\n    assert \"api_choice\" in data\n    assert \"user_wins\" in data\n    assert \"timestamp\" in data\n\n    # Verify data types and constraints\n    assert data[\"user_choice\"] == \"rock\"  # Our input is preserved\n    assert data[\"api_choice\"] in [\"rock\", \"paper\", \"scissors\"]\n    assert isinstance(data[\"user_wins\"], bool)\n</code></pre> <p>This test verifies several integration points:</p> <ol> <li>FastAPI correctly routes the <code>POST</code> request to the <code>/play</code> endpoint</li> <li>The endpoint successfully deserializes JSON into our <code>GamePlay</code> model</li> <li>The dependency injection system provides a <code>GameService</code> instance</li> <li>The service processes the game and returns a valid result</li> <li>FastAPI successfully serializes the <code>GameResult</code> back to JSON</li> </ol> <p>Notice something this test does not prove: the logic of who wins. This illustrates one of the key benefits and downsides of an integration test versus a unit test: the benefit is you have confidence everything comes together from request-to-response in one test. The downside is it would be very cumbersome to try and fully test logic in this way. Sure, you could write a loop that plays the game enough times and re-encode the winning logic to test a winner, but that's thinking about an integration test at the wrong level of abstraction. That style of test is more suited for a unit test, which we will explore shortly.</p>"},{"location":"resources/backend-architecture/2-testing/#unit-testing","title":"Unit Testing","text":"<p>Let's dive into unit testing! While integration tests give us confidence that all the pieces work together, unit tests help us verify that individual components work correctly in isolation. This granular approach makes it easier to pinpoint issues when tests fail and often leads to better designed components.</p> <p>Typically, you will write unit tests before writing integration tests, but since we will introduce some new techniques for isolating behavior in unit tests, we wanted to start with the bigger picture and then zoom in to emphasize the contrasts before getting into the details.</p>"},{"location":"resources/backend-architecture/2-testing/#unit-testing-the-game-service","title":"Unit Testing the Game Service","text":"<p>Let's start by testing the core game logic in our <code>GameService</code> class. Since this service needs to make random choices, we'll use Python's <code>unittest.mock.patch</code> to temporarily replace the random choice behavior during our tests and control it ourselves:</p> test_services.py<pre><code>from unittest.mock import MagicMock\nfrom services import GameService\nfrom models import Choice, GamePlay, GameResult\nfrom datetime import datetime, UTC\n\n\ndef create_mock_game_service(choice_to_return: Choice) -&gt; GameService:\n    \"\"\"Create a GameService with a mocked _random_choice method\"\"\"\n    service = GameService()\n    service._random_choice = MagicMock(return_value=choice_to_return)\n    return service\n\n\ndef test_game_service_rock_beats_scissors():\n    # Create a service that will return scissors\n    service = create_mock_game_service(Choice.scissors)\n    result = service.play(GamePlay(user_choice=Choice.rock))\n\n    assert result.user_choice == Choice.rock\n    assert result.api_choice == Choice.scissors\n    assert result.user_wins is True\n    service._random_choice.assert_called_once()\n\n\ndef test_game_service_scissors_loses_to_rock():\n    service = create_mock_game_service(Choice.rock)\n    result = service.play(GamePlay(user_choice=Choice.scissors))\n\n    assert result.user_choice == Choice.scissors\n    assert result.api_choice == Choice.rock\n    assert result.user_wins is False\n    service._random_choice.assert_called_once()\n\n\ndef test_game_service_draw():\n    service = create_mock_game_service(Choice.paper)\n    result = service.play(GamePlay(user_choice=Choice.paper))\n\n    assert result.user_choice == Choice.paper\n    assert result.api_choice == Choice.paper\n    assert result.user_wins is False  # Draws count as API wins\n    service._random_choice.assert_called_once()\n\n\ndef test_game_service_all_combinations():\n    \"\"\"Test all possible game combinations systematically\"\"\"\n    # Define all possible combinations and expected results\n    test_cases = [\n        (Choice.rock, Choice.scissors, True),  # Rock beats scissors\n        (Choice.rock, Choice.paper, False),  # Rock loses to paper\n        (Choice.rock, Choice.rock, False),  # Rock ties rock (API wins)\n        (Choice.paper, Choice.rock, True),  # Paper beats rock\n        (Choice.paper, Choice.scissors, False),  # Paper loses to scissors\n        (Choice.paper, Choice.paper, False),  # Paper ties paper (API wins)\n        (Choice.scissors, Choice.paper, True),  # Scissors beats paper\n        (Choice.scissors, Choice.rock, False),  # Scissors loses to rock\n        (Choice.scissors, Choice.scissors, False),  # Scissors ties scissors (API wins)\n    ]\n\n    for user_choice, api_choice, expected_win in test_cases:\n        # Create a service that will return the API choice we want to test\n        service = create_mock_game_service(api_choice)\n        result = service.play(GamePlay(user_choice=user_choice))\n\n        assert result.user_choice == user_choice\n        assert result.api_choice == api_choice\n        assert result.user_wins == expected_win, (\n            f\"Failed when user played {user_choice.value} \"\n            f\"against API's {api_choice.value}\"\n        )\n        service._random_choice.assert_called_once()\n</code></pre>"},{"location":"resources/backend-architecture/2-testing/#understanding-patchobject","title":"Understanding patch.object","text":"<p>The <code>patch.object</code> decorator/context manager is a powerful feature in Python's <code>unittest.mock</code> library that temporarily replaces attributes or methods during testing. Let's break down what's happening:</p> <pre><code>with patch.object(GameService, '_random_choice', return_value=Choice.scissors):\n    service = GameService()\n    # ... test code ...\n</code></pre> <p>This code:</p> <ol> <li>Temporarily replaces, or patches, the <code>_random_choice</code> method on the <code>GameService</code> class</li> <li>Any instance of <code>GameService</code> created within the <code>with</code> block will use the patched version</li> <li>The patched version always returns <code>Choice.scissors</code> (or whatever we specify as the <code>return_value</code> of the method)</li> <li>When the <code>with</code> block ends, the original method is restored</li> </ol> <p>Patching is particularly useful when testing code that has external dependencies or non-deterministic behavior (like randomization). By patching, we make the behavior predictable during testing while preserving the actual implementation for normal use.</p>"},{"location":"resources/backend-architecture/2-testing/#unit-testing-route-functions","title":"Unit Testing Route Functions","text":"<p>Now let's look at unit testing the FastAPI route functions. These tests focus on the route function itself, isolated from both HTTP concerns and service implementation. We will isolate the routing concerns by calling the functions directly and manually controlling the arguments (FastAPI routes are just plain-old functions, after all!). Additionally, we will isolate the service by mocking it, a technique best seen and explained with some real usage:</p> test_main_unit.py<pre><code>    # Verify how the service was used\n    mock_service.play.assert_called_once_with(GamePlay(user_choice=Choice.rock))\n</code></pre>"},{"location":"resources/backend-architecture/2-testing/#understanding-magicmock","title":"Understanding MagicMock","text":"<p>MagicMock is a powerful class in Python's <code>unittest.mock</code> library that creates objects that can pretend to be anything. Here's what makes it special:</p> <ol> <li> <p>Automatic Method Creation: MagicMock automatically creates mock methods and attributes as you try to use them. When we access <code>mock_service.play</code>, MagicMock creates a play attribute that is itself a MagicMock. In doing so, as shown, you can also control the value returned by calling this mock method.</p> </li> <li> <p>Call Tracking: MagicMock records all calls made to it, including:</p> <ul> <li>How many times it was called</li> <li>What arguments were used</li> <li>In what order calls occurred</li> </ul> </li> <li> <p>Verification Methods: MagicMock provides methods to verify how it was used:</p> <ul> <li><code>assert_called()</code> - Was it called at all?</li> <li><code>assert_called_once()</code> - Was it called exactly once?</li> <li><code>assert_called_with(args)</code> - Was it called with specific arguments?</li> <li><code>assert_called_once_with(args)</code> - Was it called once with specific arguments?</li> </ul> </li> </ol> <p>In our route test, <code>mock_service.play.assert_called_once_with(GamePlay(user_choice=Choice.rock))</code> proves that:</p> <ol> <li>The route called the service's play method exactly once</li> <li>It passed exactly the Choice.rock argument</li> <li>It didn't call any other methods on the service</li> </ol> <p>This verification is valuable because it proves that:</p> <ul> <li>The route correctly forwards the user's choice to the service</li> <li>It doesn't call the service multiple times</li> <li>It doesn't modify the choice before passing it to the service</li> <li>It doesn't call any other service methods it shouldn't</li> </ul> <p>Admittedly, given how simple this route function is, unit testing it can feel a bit silly. Indeed, it's far more code to isolate the unit test than the actual implementation itself. The earlier tests proved more valuable things to our system. So, why write unit tests for such simple functions? If you had the integration test we started with, there probably isn't a valid reason to in this case! We didn't really prove anything useful other than more tightly encoding dependencies which already exist in our system. In large enough teams, though, policies of \"every public function or method must be tested\" is an axiom that helps prevent code bases from slipping into the dangerous territory of being untested.</p> <p>So, when does unit testing a route function make more sense? Primarily when there is actual logic in the route function. This is common when you are returning a non-200 status code and helps verify expected responses. Or perhaps there's some additional logic the route is doing to pre-process inputs. For our purposes, if a route function just returns a method call and you already have an integration test: don't worry about the unit test.</p>"},{"location":"resources/backend-architecture/2-testing/#different-approaches-for-different-needs","title":"Different Approaches for Different Needs","text":"<p>Notice we're using different mocking approaches for different parts of our system:</p> <ol> <li> <p>We used <code>patch</code> to:</p> <ul> <li>Override some internal method (such as randomization)</li> <li>The patched method is an implementation detail</li> <li>We want to isolate the service's game logic</li> </ul> </li> <li> <p>We used <code>MagicMock</code> to:</p> <ul> <li>Verify interaction patterns between route and service</li> <li>Isolate the route from the service that is normally dependency injected</li> <li>We care about the interface of <code>GameService</code> in the unit tests for the route, not implementation. The implementation is tested in <code>GameService</code> unit tests.</li> </ul> </li> </ol> <p>This illustrates an important testing principle: choose your testing tools based on what you're trying to prove about your code. Sometimes you need to control behavior (patch), sometimes you need to verify interactions (MagicMock), and sometimes you might need both, which MagicMock can also do.</p>"},{"location":"resources/backend-architecture/2-testing/#the-limitations-of-tests","title":"The Limitations of Tests","text":"<p>While testing is essential for software quality, we must understand its fundamental limitations. Tests provide confidence, not certainty, and even the most comprehensive test suite cannot guarantee bug-free software. The sheer number of possible input combinations, environmental factors, and user behaviors makes complete testing impossible. Tests themselves can be flawed, suffering from false positives that pass when they should fail, or false negatives that fail when they should pass. Perhaps most insidiously, tests might continue passing while no longer validating what they were intended to check due to changing assumptions about system behavior or business rules.</p>"},{"location":"resources/backend-architecture/2-testing/#test-driven-development-and-practical-strategies","title":"Test-Driven Development and Practical Strategies","text":"<p>Test-Driven Development (TDD) offers a structured approach to address many testing challenges through its Red-Green-Refactor cycle:</p> <ol> <li>Red: Write a failing test that defines the desired behavior</li> <li>Green: Write just enough code to make the test pass</li> <li>Refactor: Improve the code while maintaining passing tests</li> </ol> <p>This methodology helps prevent over-engineering while ensuring code meets requirements. Different projects demand different testing approaches - critical systems require comprehensive testing at all levels, while rapid prototypes might focus only on key functionality.</p> <p>When working with AI tools for testing, treat them as helpful starting points rather than complete solutions. While AI can quickly generate test cases and identify edge cases, human oversight remains crucial for ensuring tests are meaningful and align with project requirements.</p>"},{"location":"resources/backend-architecture/2-testing/#the-economics-and-culture-of-testing","title":"The Economics and Culture of Testing","text":"<p>Testing represents a significant investment in time, technical infrastructure, and knowledge. Not every piece of code needs the same level of testing, and not every test provides equal value. Success requires building a culture that values testing while remaining pragmatic about testing efforts.</p>"},{"location":"resources/backend-architecture/2-testing/#summary","title":"Summary","text":"<p>Testing is a fundamental skill. If you invest time and energy into learning how to test well it will pay dividends in your career. Throughout this chapter, we've explored how different types of tests\u2014from unit tests that verify individual components to integration tests that ensure systems work together\u2014serve distinct but complementary purposes. We've seen how testing transforms development itself: with a strong test suite, you can refactor confidently, catch regressions early, and build more reliable software. More importantly, we've learned that effective testing isn't about achieving perfect coverage or following rigid rules\u2014it's about writing intentional tests that prove something meaningful about your code. As you apply these testing practices in your work, you'll find yourself writing more maintainable code, catching issues earlier, and delivering features with greater confidence.</p>"},{"location":"resources/backend-architecture/3-ci-cd/","title":"3. Introduction to CI/CD","text":"<p>Modern software development moves fast. Teams need to deliver features quickly, fix bugs rapidly, and maintain high-quality code\u2014all while ensuring deployments are smooth and reliable. This is where CI/CD comes in.  </p> <ul> <li>Continuous Integration (CI) automates the process of running tests on new code changes whenever they are pushed to a central repository. This ensures that issues are caught early and developers get fast feedback. It can also be used during a Pull Request workflow to prevent a branch from being merged into <code>main</code> until it passes all tests.</li> <li>Continuous Deployment (CD) takes things a step further by automating the deployment of every successfully tested change directly into production\u2014without manual intervention. This allows teams to ship updates multiple times a day with confidence.  </li> </ul> <p>Continuous Delivery vs. Continuous Deployment</p> <p>Continuous Delivery ensures that every tested change is ready for deployment but still requires a manual approval step before going live. Continuous Deployment, on the other hand, removes this manual step and automatically pushes changes to production once they pass all verification checks.  </p> <p>In this tutorial, we\u2019re focusing on Continuous Deployment, where code moves to production immediately after passing CI tests.  </p>"},{"location":"resources/backend-architecture/3-ci-cd/#why-cicd-matters","title":"Why CI/CD Matters","text":"<p>Manually running tests and deploying software can be slow, inconsistent, and prone to human error. CI/CD automates these critical steps, leading to:  </p> <ul> <li>Faster development cycles \u2013 Developers can push changes more frequently without worrying about breaking things.  </li> <li>Less anxiety \u2013 Automated testing and deployment remove the guesswork, making releases predictable and repeatable.  </li> <li>Higher confidence \u2013 If every change is tested and verified before it reaches production, teams can move forward with fewer worries about stability.  </li> </ul> <p>By embracing automation, teams shift from a culture of hesitation and uncertainty to one of confidence and speed. Instead of fearing deployment days, developers can focus on building great software.  </p>"},{"location":"resources/backend-architecture/3-ci-cd/#how-continuous-integration-ci-works","title":"How Continuous Integration (CI) Works","text":"<p>CI ensures that every code change is automatically tested before being merged or deployed. When a developer pushes code, a CI system:  </p> <ol> <li>Detects the change  </li> <li>Automatically runs tests  </li> <li>Reports results, allowing developers to catch and fix issues early  </li> <li>Prevents merging into the main branch (if using pull requests) or deployment if tests fail  </li> </ol> <p>A tool like GitHub Actions can be used to define workflows that trigger these tests on every push or pull request. If tests fail, CI can block the code from being merged or stop the deployment process, ensuring that only properly tested changes move forward.  </p>"},{"location":"resources/backend-architecture/3-ci-cd/#how-continuous-deployment-cd-works","title":"How Continuous Deployment (CD) Works","text":"<p>Once CI verifies that code changes pass all tests, CD ensures those changes are automatically deployed to production without human intervention. A system like OKD (OpenShift Kubernetes Distribution) or Kubernetes can handle deployment by:  </p> <ol> <li>Being notified of <code>CI</code> successfully passing all tests on <code>main</code> and receiving a webhook callback</li> <li>A <code>BuildConfig</code> begins a new <code>Build</code> by pulling repository and building a Docker image</li> <li>The Docker image is pushed to an <code>ImageStream</code></li> <li>The <code>ImageStream</code> notifies a <code>Deployment</code> that a new image is available</li> <li>The <code>Deployment</code> spins up a new <code>Pod</code> (Container) based on the image</li> <li>Once the new <code>Pod</code> is available, it turns off the old <code>Pod</code> running the previous version</li> </ol> <p>With Continuous Deployment, every change that passes CI and merged to <code>main</code> is deployed to production and live, allowing teams to ship updates faster, reduce the risk of large releases, and catch issues early.  </p>"},{"location":"resources/backend-architecture/3-ci-cd/#cicd-demo-pipeline-tutorial","title":"CI/CD Demo Pipeline Tutorial","text":"<p>In the next tutorial, you\u2019ll configure a full CI/CD pipeline for a sample repository, using GitHub Actions for CI and Kubernetes (via OKD) for CD. Get ready to see automation in action!</p>"},{"location":"resources/backend-architecture/3-ci-cd/#cloning-the-starter-repository","title":"Cloning the Starter Repository","text":"<p>Individually, accept the following GitHub Classroom assignment: https://classroom.github.com/a/7izSQo1P</p> <p>Then, on your host machine outside of any other project, clone your repository. Open this repository in VSCode and then open it in a VS Code Dev Container.</p> <p>If you are on Windows and the build fails, open <code>.devcontainer/post-create.sh</code> and check the line ending setting for this file. It needs to be <code>LF</code> not <code>CRLF</code>. If you see <code>CRLF</code> in the bottom right of the screen, click it, select <code>LF</code> and then save the file and rebuild the container.</p> <p>Before continuing, a few things to open and check out:</p> <ol> <li>In the <code>.devcontainer/devcontainer.json</code> the <code>postCreateCommand</code> runs the <code>bash</code> script named <code>post-create.sh</code></li> <li>In the <code>.devcontainer/post-create.sh</code> script, you will notice the first set of steps \"Installs the <code>oc</code> CLI tool.\" This <code>oc</code> tool is what we will use to communicate with your production setup in the cloud. This script also installs our Python packages from <code>requirements.txt</code></li> <li>Open a new terminal in VSCode and run <code>pytest</code> to see that tests pass.</li> <li>Open up <code>main.py</code> to see that this app simply produces the current times in two timezones. You can try running the app locally with <code>fastapi dev --reload</code>. Navigate to the root URL and <code>/docs</code> to see the app in question is very simple.</li> </ol> <p>Now you are ready to setup continuous integration and continuous deployment!</p>"},{"location":"resources/backend-architecture/3-ci-cd/#continuous-integration-demo","title":"Continuous Integration Demo","text":"<p>Continuous Integration is controlled by a GitHub Action. Since our project's test are written in Pytest, we want the CI system to run <code>pytest</code> as part of its workflow. We've already set you up well for this, normally you'd have to create the following directory structure and <code>yml</code> file, but for your part to get this going you need to:</p> <ol> <li>Open <code>.github/workflows/test.yml</code></li> <li>Read the names of each step to see how the worklflow builds up</li> <li>Find the commented lines:     <pre><code>- name: Test with pytest (CI)\n    run: pytest\n</code></pre></li> <li>Uncomment those lines.</li> </ol> <p>Save the file then make a git commit, on <code>main</code>, with these changes. Then, push your changes to <code>origin</code>.. For this tutorial, to keep the focus on what's important, we will make all commits and pushes to <code>main</code>. In a full industrial CI/CD pipeline, you would take this further and be sure pushes are only happening on branches and merged in to <code>main</code>, following CI success, via pull requests.</p> <p>Now, go open the repository you just setup and pushed to in your web browser on GitHub. Look for the Actions tab. Click it and look to see the most recent workflow run. It may still be running or have finished with a green check. Click it. Click through to <code>test</code>. Then take a look at the steps of this workflow looking specifically for <code>Test with pytest (CI)</code>. Expand that step. Here you can see the 3 tests passed! GitHub actions ran your tests on their machines as part of this CI process. Since everything passed, <code>pytest</code>'s process exited with status code 0, and GitHub Actions uses that as a signal that your CI workflow succeeded. Once it does, and any steps following succeed, you will see a green check!</p> <p>If you navigate back to the \"Code\" tab in GitHub, then you will also see a small green check just above your list of files, following your commit message. Click it and you can see this check is part of the indication that continuous integration succeeded thanks to our \"Run Python Tests\" workflow succeeding. (Notice: that name came from the name we gave the workflow at the top of the <code>test.yml</code> action specification.)</p>"},{"location":"resources/backend-architecture/3-ci-cd/#continuous-deployment-demo","title":"Continuous Deployment Demo","text":"<p>Setting up deployment takes some more effort because we need to stand up a production cloud environment. Our production environment will be UNC Cloud Apps' \"OKD\" cluster, set up just for our course!</p> <p>You need to be connected to Eduroam, or connected to UNC VPN (instructions here), in order to successfully use OKD. If you are on a home network, UNC guest, or other network, be sure to connect via VPN.</p> <p>Login to <code>OKD</code> by going to: https://console.apps.unc.edu</p> <p>(If your login does not succeed, it's likely because you did not previously register for Cloud Apps. You can do so by going to https://cloudapps.unc.edu/ and following the Sign Up steps. It can take up to 15 minutes following Sign Up for the OKD link above to work correctly. In the interim, feel free to follow along with your neighbor.)</p> <p>Once logged in you sould see OKD in the upper-left corner. If you see \"Red Hat\", be sure you opened the link above.</p> <p>Now that you are logged in, go to the upper-right corner and click your ONYEN and go to the \"Copy Login Command\" link. Click Display Token. In this, copy the command in the first text box. Paste it into your dev container's terminal (which has the <code>oc</code> command-line application for interfacing with a Red Hat Open-Shift Kubernetes cluster installed).</p> <p>Before proceeding, switch to your personal OKD project using your ONYEN. For example, if your ONYEN is \"jdoe\", run: <pre><code>oc project comp590-140-26sp-jdoe\n</code></pre></p> <p>If the above command fails, restart the steps above! The following will not work until you are able to access your project via <code>oc</code>.</p>"},{"location":"resources/backend-architecture/3-ci-cd/#deploying-to-okd-using-cicd-integrated-deploymentconfig","title":"Deploying to OKD using CI/CD (Integrated DeploymentConfig)","text":"<p>This guide shows you how to deploy this FastAPI project to OKD following a successful CI/CD test run using an integrated DeploymentConfig that includes its own BuildConfig and ImageStream. In this setup, after tests pass in GitHub Actions, the build is automatically triggered on OKD. The BuildConfig uses the <code>Dockerfile</code> from the repository, and the resulting image is deployed automatically with the app name set to <code>comp423-cicd-demo</code>.</p>"},{"location":"resources/backend-architecture/3-ci-cd/#1-generating-a-personal-access-token-for-your-private-repository","title":"1. Generating a Personal Access Token for Your Private Repository","text":"<p>OKD needs to be able to clone your private repository from GitHub. In order to do so, you will generate a Personal Access Token (legacy) and configure your OKD project to use this access token.</p> <p>Follow these steps to create a Personal Access Token (PAT) with read access for your repository:</p> <ol> <li> <p>Sign in to GitHub </p> <ul> <li>Go to GitHub and log in with your account.</li> </ul> </li> <li> <p>Navigate to Developer Settings </p> <ul> <li>Click on your profile picture (top-right corner) and select Settings.</li> <li>In the left sidebar, click on Developer settings.</li> </ul> </li> <li> <p>Access Personal Access Tokens </p> <ul> <li>Click on Personal access tokens.</li> <li>Select Tokens (classic).</li> </ul> </li> <li> <p>Generate a New Token </p> <ul> <li>Click the \"Generate new token\" button.</li> <li>For classic tokens, click \"Generate new token (classic)\".</li> <li>Provide a descriptive name for the token (e.g., \"OKD-Repo-ReadAccess\").</li> </ul> </li> <li> <p>Select the Scope </p> <ul> <li>Under \"Select scopes\", check the <code>repo</code> scope.</li> </ul> </li> <li> <p>Generate and Copy the Token </p> <ul> <li>Click \"Generate token\".</li> <li>Copy the generated token to your clipboard immediately. You won't be able to see it again later.</li> </ul> </li> </ol>"},{"location":"resources/backend-architecture/3-ci-cd/#2-register-your-github-pat-with-a-secret-stored-in-okd","title":"2. Register your GitHub PAT with a Secret Stored in OKD","text":"<p>From the built-in terminal in your dev container:</p> <ol> <li>Create a GitHub access secret    Create a secret in OKD that contains your GitHub username and a personal access token (PAT) with appropriate repo rights. Remove the surrounding less than and greater than signs when substituting your personal GitHub username and PAT in the command below:    <pre><code>oc create secret generic comp423-cicd-git-credentials \\\n    --from-literal=username=&lt;your-github-username&gt; \\\n    --from-literal=password=&lt;your-github-pat&gt; \\\n    --type=kubernetes.io/basic-auth\n</code></pre></li> <li>Label the Secret    Add the label \"app=comp423-cicd-demo\" to make it easier to delete everything related to this demo later on:    <pre><code>oc label secret comp423-cicd-git-credentials app=comp423-cicd-demo\n</code></pre></li> </ol>"},{"location":"resources/backend-architecture/3-ci-cd/#3-create-an-integrated-deployment-with-buildconfig-imagestream-and-source-secret","title":"3. Create an Integrated Deployment (with BuildConfig, ImageStream, and Source Secret)","text":"<p>OKD's <code>new-app</code> command is a handy all-in-one command to create an application from a repository that handles setting up the app's <code>BuildConfig</code>, <code>ImageStream</code>, and <code>Deployment</code> behind the scenes.</p> <pre><code>oc new-app . \\\n--name=comp423-cicd-demo \\\n--source-secret=comp423-cicd-git-credentials \\\n--strategy=docker \\\n--labels=app=comp423-cicd-demo\n</code></pre> <p>Explanation:  </p> <ul> <li>The <code>--source-secret=comp423-cicd-git-credentials</code> flag directs OKD to use the secret you created to clone the private repo.</li> <li>The <code>--labels=app=comp423-cicd-demo</code> parameter tags the created BuildConfig, ImageStream, and DeploymentConfig with a common label, \"app=comp423-cicd-demo\".</li> </ul> <p>Over in OKD, in the web browser, you should look to see the application appear in your project. You should find its build status and track the build.</p>"},{"location":"resources/backend-architecture/3-ci-cd/#4-expose-the-service","title":"4. Expose the Service","text":"<p>Your OKD pods are securely only accessible to you, or other users you give access to, and not the general public. To begin the process of exposing a pod to the public, we need to expose a route to it in OKD. The following <code>create route</code> will automatically generate a route URL securely connecting your service to the outside world:</p> <pre><code>oc create route edge --service=comp423-cicd-demo\n</code></pre> <p>Routes can also be created with custom hostnames, but the automatic name is sufficient for this tutorial.</p> <p>After doing so, once your project successfully builds, you can run the following command:</p> <pre><code>oc get route comp423-cicd-demo\n</code></pre> <p>What this will do is show you the public route to your app running in production. Try opening this URL in your browser or your phone. This is live on the public internet!</p>"},{"location":"resources/backend-architecture/3-ci-cd/#5-setting-up-continuous-deployment-using-a-webhook-callback-from-github-actions-to-okd","title":"5. Setting up Continuous Deployment Using a Webhook Callback from GitHub Actions to OKD","text":"<p>A webhook callback is a mechanism by which OKD can be triggered to start a new build when it receives an HTTP POST request. In this setup, after your tests pass on GitHub Actions, a POST request is sent to the webhook URL configured in your OKD BuildConfig. This webhook URL contains a secret token that ensures only authorized calls (i.e. from your GitHub Actions) can trigger a build. </p> <p>To configure this:</p> <ol> <li>To get the webhook URL:    <pre><code>oc describe bc/comp423-cicd-demo | grep -C 1 generic\n</code></pre></li> <li>To get the webhook secret token:    <pre><code>oc get bc comp423-cicd-demo -o yaml | grep -C 1 generic\n</code></pre>    Look for the \"generic\" trigger section and note the secret token.</li> <li>In your GitHub repository, add this full URL as a secret named <code>WEBHOOK_URL</code>:</li> <li>Go to Settings &gt; Security &gt; Secrets and Variables &gt; Actions &gt; Repository Secrets &gt; New Repository Secret.</li> <li>Set the name to <code>WEBHOOK_URL</code> and paste in the full URL. The full URL is comprised of the URL template found in the command above and substituting the secret into the path. This secret in your repository is what will be referenced in the next GitHub Action variable. Note: you will NOT put the URL directly into the action definition!</li> <li>In your GitHub Actions workflow, uncomment the following step to use <code>curl</code> to send a POST request to trigger a new build:</li> </ol> <pre><code>- name: Trigger OKD Build via Webhook\n  if: success()\n  run: |\n    curl -X POST \"${{ secrets.WEBHOOK_URL }}\"\n</code></pre> <p>This step uses the webhook URL to trigger a build only when prior steps succeed, thanks to the <code>if</code> condition where <code>success()</code> is established by the workflow steps prior.</p> <p>You can try testing this by modifying something simple in your application that will not break the tests. For example, perhaps just switch the ordering in <code>main.py</code> of the two timezones in the <code>TIMEZONES</code> constant. Make a commit with the changes to your app and <code>test.yml</code>. Push the commit to <code>main</code> (we can circumvent best practices about branching for educational purposes here!) When you open your GitHub repository, you can see the action flow through the steps. In one of the final steps you will see \"Deploy to production\". If you then go look at OKD, you will see a new build is kicked off and working to build a new production image. Once it completes, the image is deployed to your app and it is live on the web! This is continuous integration (via testing) and continuous deployment (contingent on tests passing)! These flows are common in industrial software engineering settings.</p>"},{"location":"resources/backend-architecture/3-ci-cd/#summary","title":"Summary","text":"<p>This setup allows a robust CI/CD pipeline where any push or PR to the main branch runs tests via GitHub Actions. After successful tests, a secure webhook callback is sent to OKD to start a build. The configuration ensures the Dockerfile (from the \".production\" directory) is used and the application (tagged with \"app=comp423-cicd-demo\") is deployed with minimal manual intervention, while keeping sensitive tokens secure within GitHub Secrets.</p> <p>Happy deploying!</p>"},{"location":"resources/backend-architecture/3-ci-cd/#cleaning-up-cicd-demo-components-on-okd","title":"Cleaning Up CI/CD Demo Components on OKD","text":"<p>When you're ready to clean up all the components created by this deployment, you can delete them all in one step by using the label selector:</p> <pre><code>oc delete all -l app=comp423-cicd-demo\noc delete secret -l app=comp423-cicd-demo\n</code></pre> <p>These commands will remove all resources (Deployment, BuildConfig, ImageStream, Service, Route, and secrets) tagged with \"app=comp423-cicd-demo\" while leaving your OKD project intact for future work.</p>"},{"location":"resources/database/1-sql/","title":"1. Introduction to Relational Databases &amp; SQL","text":""},{"location":"resources/database/1-sql/#what-is-a-relational-database","title":"What is a Relational Database?","text":"<p>A relational database is a way to store and organize data in a structured format using tables. Each table consists of rows (records) and columns (fields). The power of relational databases comes from their ability to establish relationships between tables, allowing for efficient data retrieval and consistency.</p> <p>Unlike temporary data storage solutions, relational databases persist data to disk, meaning information is retained even after the database system shuts down. This persistence makes relational databases essential for applications that require long-term data storage, such as banking systems, e-commerce platforms, and content management systems.</p>"},{"location":"resources/database/1-sql/#a-brief-history-of-relational-databases-and-sql","title":"A Brief History of Relational Databases and SQL","text":"<p>The concept of relational databases was introduced by Edgar F. Codd in 1970 while working at IBM. He proposed the relational model, which organizes data into structured tables and uses mathematical relational algebra for querying and manipulating data.</p> <p>Before relational databases, most systems used hierarchical or network databases, which were rigid and difficult to scale. Codd\u2019s relational model revolutionized database management by making data more flexible and easier to query.</p> <p>The Structured Query Language (SQL) was later developed in the 1970s at IBM as a way to interact with relational databases. It became the standard language for querying and managing relational data and was eventually adopted by major database systems.</p>"},{"location":"resources/database/1-sql/#modern-popular-relational-databases","title":"Modern, Popular Relational Databases","text":"<p>Today's most popular relational databases include:</p> <ul> <li>PostgreSQL (open-source, widely used for modern applications)</li> <li>MySQL (common in web applications)</li> <li>SQLite (lightweight, used in mobile and embedded systems)</li> <li>Microsoft SQL Server (enterprise-level database solution)</li> <li>Oracle Database (widely used in large corporations)</li> </ul>"},{"location":"resources/database/1-sql/#key-abstractions-in-a-relational-database","title":"Key Abstractions in a Relational Database","text":"<p>A relational database management system (RDBMS) provides a structured way to store and manipulate data. At the core of an RDBMS are the following key abstractions:</p>"},{"location":"resources/database/1-sql/#tables","title":"Tables","text":"<p>A table is the fundamental structure in an RDBMS. It represents a collection of related data and is analogous to a spreadsheet or a well-organized list. Each table consists of columns (fields) that define the types of data stored and rows (records) that contain actual data entries.</p>"},{"location":"resources/database/1-sql/#columns-fields","title":"Columns (Fields)","text":"<p>A column defines the type of data that a table can store for a particular attribute. Each column has a data type that constrains the values it can hold, ensuring consistency and efficiency.</p> <p>Columns can have constraints such as: - NOT NULL \u2013 Ensures the column cannot have empty values. - UNIQUE \u2013 Guarantees that all values in the column are distinct. - CHECK \u2013 Enforces a condition on the column's values. - DEFAULT \u2013 Assigns a default value if no explicit value is provided.</p> <p>Some columns also have fixed maximum widths for storage efficiency. For example, a <code>VARCHAR(255)</code> column ensures that no value stored exceeds 255 characters. Limiting column widths helps optimize performance because it enables more efficient indexing, improves retrieval speed, and reduces overall memory consumption.</p> <p>Here is a table of common PostgreSQL data types:</p> Data Type Description <code>INTEGER</code> Whole numbers (e.g., 1, 2, 3) <code>SERIAL</code> Auto-incrementing integer (commonly used for primary keys) <code>TEXT</code> Variable-length string (unlimited size) <code>VARCHAR(n)</code> String with a maximum length of <code>n</code> characters <code>BOOLEAN</code> True or False values <code>DATE</code> Stores dates (YYYY-MM-DD) <code>TIMESTAMP</code> Stores date and time information <code>DECIMAL(p, s)</code> Precise fixed-point decimal numbers <code>REAL</code> / <code>DOUBLE PRECISION</code> Floating-point numbers"},{"location":"resources/database/1-sql/#rows-records","title":"Rows (Records)","text":"<p>A row represents a single entry in a table, storing a unique combination of values across all the columns. Each row corresponds to a distinct instance of the entity the table models.</p> <p>An analogy to Object-Oriented Programming (OOP) can be helpful here: - A table is like a class definition in OOP. - Columns define the attributes of the class. - Rows are like instances (objects) created from the class.</p> <p>For example, consider a <code>users</code> table: <pre><code>CREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    email TEXT UNIQUE NOT NULL\n);\n</code></pre> This is similar to defining a <code>User</code> class in OOP: <pre><code>class User:\n    def __init__(self, id, name, email):\n        self.id = id\n        self.name = name\n        self.email = email\n</code></pre> Each row in the <code>users</code> table represents a different <code>User</code> object with unique values for <code>id</code>, <code>name</code>, and <code>email</code>.</p>"},{"location":"resources/database/1-sql/#primary-keys","title":"Primary Keys","text":"<p>A primary key uniquely identifies each row in a table, ensuring that no two rows have the same identifier. The choice of primary key depends on the scale and nature of the data.</p> <p>For smaller datasets, using an <code>INTEGER</code> or <code>SERIAL</code> column as a unique ID is common. However, for very large-scale databases\u2014such as those storing tweets, Instagram posts, or large-scale event logs\u2014alternative schemes like UUIDs (Universally Unique Identifiers) or timestamp-based IDs are often used. These approaches provide a much larger range of unique identifiers and offer benefits such as distributed uniqueness and time-ordering properties.</p> <p>For our purposes, since we are working with small datasets, we will primarily use serial integers as primary keys.</p>"},{"location":"resources/database/1-sql/#understanding-sql-a-domain-specific-language-for-databases","title":"Understanding SQL: A Domain-Specific Language for Databases","text":"<p>SQL (Structured Query Language) is a programming language designed specifically for interacting with relational databases. Unlike general-purpose programming languages like Python or Java, SQL is domain-specific, meaning it is specialized for tasks related to storing, retrieving, and manipulating structured data.</p> <p>SQL enables users to:</p> <ul> <li>Define and modify database structures.</li> <li>Insert, update, and delete data within those structures.</li> <li>Query and retrieve meaningful information efficiently.</li> </ul> <p>SQL is divided into two main categories:</p>"},{"location":"resources/database/1-sql/#1-ddl-data-definition-language","title":"1. DDL (Data Definition Language)","text":"<p>DDL is responsible for defining and managing the structure of a database. Commands in this category affect the schema of a database, such as creating, modifying, and deleting tables.</p>"},{"location":"resources/database/1-sql/#common-ddl-commands","title":"Common DDL Commands:","text":"<ul> <li><code>CREATE TABLE</code> \u2013 Defines a new table structure.</li> <li><code>ALTER TABLE</code> \u2013 Modifies an existing table.</li> <li><code>DROP TABLE</code> \u2013 Deletes a table and all its data.</li> </ul> <p>Example:</p> <pre><code>CREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    email TEXT UNIQUE NOT NULL\n);\n</code></pre>"},{"location":"resources/database/1-sql/#2-dml-data-manipulation-language","title":"2. DML (Data Manipulation Language)","text":"<p>DML is used to work with the data inside database tables. These commands allow users to insert new records, update existing ones, delete data, and query stored information.</p>"},{"location":"resources/database/1-sql/#common-dml-commands","title":"Common DML Commands:","text":"<ul> <li><code>INSERT INTO</code> \u2013 Adds new records to a table.</li> <li><code>UPDATE</code> \u2013 Modifies existing records.</li> <li><code>DELETE</code> \u2013 Removes records.</li> <li><code>SELECT</code> \u2013 Retrieves data from tables.</li> </ul> <p>Example:</p> <pre><code>INSERT INTO users (name, email) VALUES ('Alice', 'alice@email.com');\nSELECT * FROM users;\n</code></pre> <p>By mastering both DDL and DML, you gain the ability to design, manage, and interact with relational databases effectively. Understanding these distinctions helps clarify how SQL serves both structural and operational purposes in database management.</p>"},{"location":"resources/database/1-sql/#experimenting-with-sql-in-a-postgresql-docker-container","title":"Experimenting with SQL in a PostgreSQL Docker Container","text":"<p>To follow along with this tutorial, you can run PostgreSQL inside a Docker container.</p>"},{"location":"resources/database/1-sql/#step-1-pull-and-run-postgresql","title":"Step 1: Pull and Run PostgreSQL","text":"<p>From your host machine's terminal run the following command to start a PostgreSQL container:</p> <pre><code>docker run \\\n  --name postgres \\\n  --env POSTGRES_PASSWORD=mysecretpassword \\\n  --publish 5432:5432 \\\n  --detach \\\n  postgres:latest\n</code></pre> <p>Explanation of flags:</p> <ul> <li><code>--name postgres</code> \u2192 Names the container postgres.</li> <li><code>--env POSTGRES_PASSWORD=mysecretpassword</code> \u2192 Sets the default password for the PostgreSQL instance.</li> <li><code>--publish 5432:5432</code> \u2192 Maps port 5432 from the container to the host, allowing connections.</li> <li><code>--detach</code> \u2192 Runs the container in detached mode, meaning it runs in the background.</li> <li><code>postgres:latest</code> \u2192 Specifies the PostgreSQL image to use, with <code>latest</code> pulling the most recent stable version.</li> </ul>"},{"location":"resources/database/1-sql/#where-is-the-data-stored","title":"Where is the Data Stored?","text":"<p>When using a Docker container without explicit volume mapping, PostgreSQL stores its data inside the container\u2019s filesystem. This means that even if you stop and restart the container, your data will still be available. However, if the container is removed, the data will be lost.</p> <p>For production environments, best practice is to mount a volume to ensure that database files persist independently of the container lifecycle. However, for simplicity in this tutorial, we are relying on the container's internal storage.</p>"},{"location":"resources/database/1-sql/#step-2-connect-to-postgresql","title":"Step 2: Connect to PostgreSQL","text":"<p>To interact with the running PostgreSQL container, use the following command which runs the CLI <code>psql</code> Postgres client:</p> <pre><code>docker exec \\\n  --interactive \\\n  --tty \\\n  --user postgres \\\n  postgres \\\n  psql\n</code></pre> <p>This command does not start a new container; it connects to the existing detached container named <code>postgres</code> and runs the <code>psql</code> command inside it.</p>"},{"location":"resources/database/1-sql/#exiting-postgresql","title":"Exiting PostgreSQL","text":"<p>To quit <code>psql</code>, type:</p> <pre><code>\\q\n</code></pre>"},{"location":"resources/database/1-sql/#stopping-restarting-and-removing-the-postgresql-container","title":"Stopping, Restarting, and Removing the PostgreSQL Container","text":"<p>If you need to stop the PostgreSQL container but keep the data intact, run:</p> <pre><code>docker stop postgres\n</code></pre> <p>To restart the container and restore access to the database, use:</p> <pre><code>docker start postgres\n</code></pre> <p>This ensures that all changes made to the database remain intact between restarts.</p> <p>If you no longer need the container and want to permanently delete it, along with all stored data, run:</p> <pre><code>docker rm postgres\n</code></pre> <p>Important: Removing the container deletes all stored data since we are not using a volume in this tutorial. In real applications, using a Docker volume ensures data persistence beyond container removal. Now that you have PostgreSQL running inside Docker, you can start writing SQL commands.</p>"},{"location":"resources/database/1-sql/#basics-of-sql-creating-tables-inserting-data-and-querying-with-select","title":"Basics of SQL: Creating Tables, Inserting Data, and Querying with SELECT","text":"<p>Now that we have PostgreSQL running inside Docker, let's explore some basic SQL commands. We'll cover:</p> <ul> <li>Creating tables</li> <li>Inserting data</li> <li>Querying data with SELECT</li> <li>Filtering results with WHERE</li> <li>Sorting and limiting query results</li> <li>Joining related tables</li> </ul>"},{"location":"resources/database/1-sql/#creating-a-table","title":"Creating a Table","text":"<p>A table in SQL is defined using the <code>CREATE TABLE</code> statement. Each table consists of columns, where each column has a specific data type.</p> <p>Let's create a <code>users</code> table:</p> <pre><code>CREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    email TEXT UNIQUE NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre>"},{"location":"resources/database/1-sql/#explanation","title":"Explanation:","text":"<ul> <li><code>id SERIAL PRIMARY KEY</code> \u2192 A unique, auto-incrementing identifier.</li> <li><code>name TEXT NOT NULL</code> \u2192 A required text field.</li> <li><code>email TEXT UNIQUE NOT NULL</code> \u2192 Ensures emails are unique and required.</li> <li><code>created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</code> \u2192 Automatically stores the time the record was created.</li> </ul>"},{"location":"resources/database/1-sql/#inserting-data","title":"Inserting Data","text":"<p>To add records to a table, use <code>INSERT INTO</code>:</p> <pre><code>INSERT INTO users (name, email) VALUES ('Alice', 'alice@email.com');\nINSERT INTO users (name, email) VALUES ('Bob', 'bob@email.com');\nINSERT INTO users (name, email) VALUES ('Charlie', 'charlie@email.com');\n</code></pre>"},{"location":"resources/database/1-sql/#retrieving-data-with-select","title":"Retrieving Data with <code>SELECT</code>","text":"<p>To retrieve all records from a table:</p> <pre><code>SELECT * FROM users;\n</code></pre> <p>This returns:</p> id name email created_at 1 Alice alice@email.com 2025-03-02 12:00:00 2 Bob bob@email.com 2025-03-02 12:01:00 3 Charlie charlie@email.com 2025-03-02 12:02:00"},{"location":"resources/database/1-sql/#filtering-results-with-where","title":"Filtering Results with <code>WHERE</code>","text":"<p>Use <code>WHERE</code> to filter results based on conditions.</p>"},{"location":"resources/database/1-sql/#find-a-specific-user-by-email","title":"Find a specific user by email:","text":"<pre><code>SELECT * FROM users WHERE email = 'alice@email.com';\n</code></pre>"},{"location":"resources/database/1-sql/#find-all-users-whose-names-start-with-b","title":"Find all users whose names start with \"B\":","text":"<pre><code>SELECT * FROM users WHERE name LIKE 'B%';\n</code></pre>"},{"location":"resources/database/1-sql/#find-users-created-after-a-specific-date","title":"Find users created after a specific date:","text":"<pre><code>SELECT * FROM users WHERE created_at &gt; '2025-03-01';\n</code></pre>"},{"location":"resources/database/1-sql/#sorting-and-limiting-query-results","title":"Sorting and Limiting Query Results","text":""},{"location":"resources/database/1-sql/#sort-users-by-name-ascending","title":"Sort users by name (ascending):","text":"<pre><code>SELECT * FROM users ORDER BY name ASC;\n</code></pre>"},{"location":"resources/database/1-sql/#get-the-most-recently-created-user","title":"Get the most recently created user:","text":"<pre><code>SELECT * FROM users ORDER BY created_at DESC LIMIT 1;\n</code></pre>"},{"location":"resources/database/1-sql/#conclusion","title":"Conclusion","text":"<p>In this chapter, we explored the fundamentals of relational databases and SQL. We then covered key relational database concepts, including tables, columns, rows, and primary keys. Additionally, we demonstrated SQL queries for creating tables, inserting data, retrieving data using SELECT, filtering with WHERE, sorting, and limiting results.</p> <p>Understanding relational databases and SQL is useful for managing and persisting structured data efficiently. In the next reading, we will look at creating relationships between tables with foreign keys and introducing the concept of transactions. Each of these concepts is an important feature of a relational database system.</p>"},{"location":"resources/exercises/ex01-api-design/","title":"EX01. API Design and Implementation","text":""},{"location":"resources/exercises/ex01-api-design/#breakdown-of-parts","title":"Breakdown of Parts","text":"<p>Part 1. You only need to implement the route decorators and function signatures, NOT the actual implementation of the API.</p> <p>Part 2. You wll implement the API and deploy it to a production environment.</p>"},{"location":"resources/exercises/ex01-api-design/#app-overview-the-pastebin-url-shortener","title":"App Overview: The Pastebin + URL Shortener","text":"<p>In this lab, you and a partner will collaborate to design and implement a service that combines:</p> <ul> <li>A Pastebin-style API (store text snippets and retrieve them by a unique URL).</li> <li>A URL-shortening API (submit a long URL and receive a short, redirectable URL).</li> </ul> <p>These two functionalities must share a common opaque namespace for links, presenting unique design challenges regarding how to store and retrieve resources come implementation time.</p> <p>This application will be able to generate shortened URLs for sharing content such as:</p> <ol> <li>Pastbin Example: https://pastebin.com/uYCCWaxy</li> <li>URL Shortener Example: https://go.unc.edu/Xj9b6</li> </ol> <p>This application design will feature three personas:</p> <ol> <li> <p>Sue Sharer is someone who wants to distribute content easily. She might be a student sharing notes, a blogger sharing a quote, or a prankster hiding a Rickroll. Sue values convenience, control, and customization, which is why she wants options like vanity URLs and expiration times.</p> </li> <li> <p>Cai Clicker is the person who receives and opens links. They might be a friend reading a shared snippet, a recruiter reviewing a resume, or an unsuspecting victim of a disguised meme. Cai values seamlessness and reliability\u2014when they click a link, they expect to either see content immediately or be redirected without confusion.</p> </li> <li> <p>Amy Admin is responsible for monitoring and managing all active resources. She is a community manager. Amy values visibility, control, and order, ensuring that shared content remains appropriate and awareness of high-traffic links.</p> </li> </ol>"},{"location":"resources/exercises/ex01-api-design/#user-journey-examples","title":"User Journey Examples","text":"<p>An journey may combine a few user stories in order to give a complete start-to-finish example of a feature's use. Since you may not be familiar with the point of a pastebin-like service, or URL shortener, consider these journeys before reading the user stories in a more standalone presentation.</p>"},{"location":"resources/exercises/ex01-api-design/#sue-sharer-creates-a-text-snippet","title":"Sue Sharer Creates a Text Snippet","text":"<ol> <li>Sue wants to share a quote from a book with a friend.  </li> <li>She submits the text: <pre><code>\"Not all those who wander are lost.\"\n</code></pre></li> <li>The system generates a random URL path and responds with the information Sue needs to share the URL.</li> <li>Sue shares this link with her friend.  </li> <li>Cai Clicker clicks the link, which looks something like <code>https://&lt;your-apps-hostname&gt;/xYzA12</code> and sees the text snippet: <pre><code>\"Not all those who wander are lost.\"\n</code></pre></li> </ol>"},{"location":"resources/exercises/ex01-api-design/#sue-sharer-creates-a-shortened-url","title":"Sue Sharer Creates a Shortened URL","text":"<ol> <li>Sue wants to prank a friend by disguising a Rick Astley video link.  </li> <li>She submits the long URL below and a vanity path of <code>exam-solutions</code>: <pre><code>https://www.youtube.com/watch?v=dQw4w9WgXcQ\n</code></pre></li> <li>The system generates a link with the vanity path and responds with the information Sue needs to share the URL. </li> <li>Sue shares this link with her friend: <code>https://&lt;your-apps-hostname&gt;/exam-solutions</code> </li> <li>Cai Clicker clicks the link and is redirected to: <pre><code>https://www.youtube.com/watch?v=dQw4w9WgXcQ\n</code></pre></li> </ol>"},{"location":"resources/exercises/ex01-api-design/#required-user-stories","title":"Required User Stories","text":"<ol> <li>Sue Sharer<ol> <li>As Sue Sharer, I want to create a new text snippet with an optional expiration time and the ability to request a custom vanity URL, so that I can control how long it is available and share a more meaningful link.</li> <li>As Sue Sharer, I want to create a shortened URL with an optional expiration time and the ability to request a custom vanity URL, so that I can control how long it is available and share a more meaningful link.</li> </ol> </li> <li>Cai Clicker<ol> <li>As Cai Clicker, I want to open a shared text snippet by clicking its unique link, so that I can read the content that was provided to me.</li> <li>As Cai Clicker, I want to open a shortened URL by clicking its unique link, so that I am automatically redirected to the original long URL.</li> </ol> </li> <li>Amy Admin<ol> <li>As Amy Admin, I want to see a list of all active resources (text snippets and shortened URLs) and filter by type or view counts greater than some low threshold, so that I can oversee what content is currently being shared.  </li> <li>As Amy Admin, I want to see how many times each resource has been accessed, so that I can monitor usage and identify high-traffic resources.</li> <li>As Amy Admin, I want to update the content of an active text snippet or change the target of a shortened URL, so that I can correct or modify existing resources when necessary.  </li> <li>As Amy Admin, I want to delete any active resource from the system, so that I can remove content that should no longer be available.  </li> </ol> </li> </ol>"},{"location":"resources/exercises/ex01-api-design/#path-requirement-specifications","title":"Path Requirement Specifications","text":"<p>User stories 2.a. and 2.b. above are the only stories which we will very specificaly share an API requirement, as follows:</p> <p>These two stories should both share the same opaque route, including method and path. The method is <code>GET</code> and the <code>path</code> in FastAPI route path syntax is <code>/{resource_identifier}</code>. This means that there is a single shared path pattern for retrieving both types of resources.</p> <ul> <li>When a user accesses a generated or vanity URL, the system must determine whether it corresponds to a text snippet or a shortened URL.</li> <li>The user should not be able to infer whether a given URL points to a text snippet or a redirection just by looking at it.</li> </ul>"},{"location":"resources/exercises/ex01-api-design/#no-authentication-enforced","title":"No Authentication Enforced","text":"<p>The concerns of how to authenticate a user, like Amy Admin, and authorize various actions, is beyond your concern in this initial API design. You should proceed with all routes publicly available, unprotected. Later, we'll learn strategies for authenticating and authorizing various actions at the HTTP API level.</p>"},{"location":"resources/exercises/ex01-api-design/#phase-1-api-design","title":"Phase 1: API Design","text":""},{"location":"resources/exercises/ex01-api-design/#getting-started","title":"Getting Started","text":"<p>To begin work on EX01, you and your partner will need to accept a GitHub classroom with your assigned Team Name (in the form of <code>team_0_NN</code>) found in the pairings sheet. First look up your team name by your PID and copy it. Then go accept the GitHub classroom assignment by following this link. Search for your team name and if it already exists, join it. If you are the first of your pair to begin, create the team with the assigned team name.</p>"},{"location":"resources/exercises/ex01-api-design/#create-a-branch-for-individual-api-design","title":"Create a Branch for Individual API Design","text":"<p>Clone the project, open your project in a dev container, and create a branch for your individual API design. Name your branch something that includes your onyen or github username.</p>"},{"location":"resources/exercises/ex01-api-design/#individual-api-design","title":"Individual API Design","text":"<p>In your branch created above, go ahead and stub out an HTTP API, making use of FastAPI routes and Pydantic models as necessary, that satisfy the user stories. Use the <code>/docs</code> user interface to review your routes. Your objectives are:</p> <ul> <li>Define Endpoints: Specify all the HTTP routes required for the required user stories above.</li> <li>Design Data Models: Create Pydantic models that define the structure of request bodies and responses.</li> <li>Clear OpenAPI Documentation: Fully document important your API using the OpenAPI standards discussed below.</li> <li>Establish Conventions: Ensure consistent naming and documentation throughout your API design.</li> </ul>"},{"location":"resources/exercises/ex01-api-design/#openapi-specification-requirements","title":"OpenAPI Specification Requirements","text":"<p>FastAPI and Pydantic have special constructs which allow you to more fully specify your API and its documentation to produce the standards-based <code>OpenAPI.json</code> spec powering the <code>/docs</code> user interface.</p> <p>You are required to add specification and documentation to your API design along each of the following dimensions. You can find examples of how each is done following this overview list:</p> <ul> <li>FastAPI Application: Ensure your app is properly instantiated with required metadata.</li> <li>Route-level: Always include a summary and description; document response bodies thoroughly.</li> <li>Route Parameters:<ul> <li>Path parameters: Must have descriptions and optional validations.</li> <li>Query parameters: Must have descriptions and can include validations.</li> <li>Body parameters: Must have descriptions and <code>openapi_examples</code> for clear request body documentation.</li> </ul> </li> <li>Pydantic Fields: Every field should include a description and an example (or examples) to aid API consumers.</li> </ul>"},{"location":"resources/exercises/ex01-api-design/#fastapi-application-level-documentation","title":"FastAPI Application-level Documentation","text":"<p>Instantiate your FastAPI app using the <code>FastAPI</code> constructor. You must provide a <code>title</code>, <code>contact</code>, <code>description</code>, and <code>openapi_tags</code> (for organizing routes), as shown below. Notice that the description is markdown and you can use a docstring to give your API documentation </p> <p>Example:</p> <pre><code>app = FastAPI(\n    title=\"EX01 API Design\",\n    contact={\n        \"name\": \"Parter A, Partner B\",\n        \"url\": \"https://github.com/comp423-26s/&lt;your-team-repo&gt;\",\n    },\n    description=\"\"\"\n## Introduction\n\nYour introduction text to your API goes here, in **markdown**.\nWrite your own brief intro to what his API is about.\n\"\"\",\n    openapi_tags=[\n        {\"name\": \"Sue\", \"description\": \"Sue Sharer's API Endpoints\"},\n        {\"name\": \"Cai\", \"description\": \"Cai Clicker's API Endpoints\"},\n        {\"name\": \"Amy\", \"description\": \"Amy Admin's API Endpoints\"},\n    ],\n)\n</code></pre> <p>After you've more fully configured your <code>app</code>, as shown above, try reloading your OpenAPI UI by navigating to <code>/docs</code> in your dev server. You should see the information above being used to improve the documentation generated. The tags added will allow you to organize your routes based on the intended user. In real APIs, tags are generally used to cluster endpoints for a specific feature together; here we're using them to organize by persona served.</p>"},{"location":"resources/exercises/ex01-api-design/#route-level-decorator-specification","title":"Route-level Decorator Specification","text":"<p>Define endpoints using FastAPI\u2019s route decorators (e.g., <code>@app.get</code>, <code>@app.post</code>). Each route must include a summary, description, and tag. The tag corresponds to the <code>openapi_tags</code> you specified above and will be a persona name. If your route returns response codes besides <code>200</code>, such as <code>404</code>, you need to specify the responses field as shown below. For a given status code, the description is required and the model (Pydantic subclass) is only necessary if the response returns a body.</p> <p>Example:</p> <pre><code>from typing import Annotated\n\nclass MessageResponse(BaseModel):\n    message: Annotated[str, Field(\n        description=\"Information conveyed ot user\", examples=[\"Hi!\"]\n    )]\n\n# ...\n\n@app.get(\n    \"/items/{item_id}\",\n    summary=\"Retrieve an Item\",\n    description=\"Get details of an item by its ID.\",\n    responses={\n        404: {\n            \"description\": \"Item not found\",\n        }\n    },\n    tags=[\"Shopping\"]\n)\ndef get_item(item_id: int) -&gt; MessageResponse:\n    if item_id &gt; 0:\n        return MessageResponse(message=\"Item found!\")\n    else:\n        raise HTTPException(status_code=404, detail=\"Item not found!\")\n</code></pre>"},{"location":"resources/exercises/ex01-api-design/#dynamic-path-parameters","title":"Dynamic Path Parameters","text":"<p>For dynamic segments in the URL (path parameters), use <code>Path</code>. Include a description and any additional keyword parameters found in the official documentation you believe would be helpful in specifying and documenting your path (useful ideas: 1. <code>examples</code> list of example values you might expect for the parameter, 2. validation such as <code>min_length</code> or <code>gt</code> (greater than) as shown below).</p> <p>Example:</p> <pre><code>from fastapi import FastAPI, Path\nfrom typing import Annotated\n\n# ...\n\n@app.get(\"/users/{user_id}\")\ndef get_user(\n    user_id: Annotated[int, Path(\n        description=\"The unique ID of the user\",\n        gt=0,\n        examples=[1, 423]\n    )]\n) -&gt; User:\n    ...\n</code></pre>"},{"location":"resources/exercises/ex01-api-design/#query-parameters","title":"Query Parameters","text":"<p>For query parameters (appended to the URL), use <code>Query</code>. Each query parameter must include a description, should probably include a default value, and can optionally include additional examples and validation rules, if needed. See the official documentation on supported keyword parameters when specifying and documenting query parameters.</p> <p>Example:</p> <pre><code>from fastapi import FastAPI, Query\nfrom typing import Annotated\n\n# ...\n\n@app.get(\"/search\")\ndef search_items(\n    q: Annotated[str, Query(\n        description=\"The product search query\",\n        examples=[\"jordans\"]\n    )] = \"\" # Default value is empty string\n) -&gt; SearchResults:\n    ...\n</code></pre>"},{"location":"resources/exercises/ex01-api-design/#documenting-pydantic-model-fields","title":"Documenting Pydantic Model Fields","text":"<p>Within your Pydantic models, use the <code>Field</code> function to document each field. Every field must have a description and examples list to aid API consumers.</p> <p>Example:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Annotated\n\nclass Item(BaseModel):\n    name: Annotated[str, Field(\n        description=\"Name of Product\",\n        examples=[\"UNC Jersey\", \"UNC Socks\"]\n    )]\n    price: Annotated[float, Field(\n        description=\"Sales Price\",\n        examples=[75.0, 20.0]\n    )]\n</code></pre>"},{"location":"resources/exercises/ex01-api-design/#request-body-parameters","title":"Request Body Parameters","text":"<p>For request body parameters (used in <code>POST</code>/<code>PUT</code>/<code>PATCH</code> requests), define a Pydantic model and use <code>Body</code> to add metadata. The body parameter must include a description and openapi_examples. These examples help make testing out the API in <code>/docs</code> easier, as you will see when you try it out. You can also add validation rules if needed.</p> <p>Example:</p> <pre><code>from fastapi import FastAPI, Body\nfrom typing import Annotated\n\n# ... Same Item model as above ...\n\n@app.post(\"/items\")\ndef create_item(\n    item: Annotated[Item, Body(\n        description=\"The product to create\",\n        openapi_examples={\n            \"Air Jordans\": {\n                \"summary\": \"Air Jordan 1 Mid SE\",\n                \"description\": \"Sample product to create\",\n                \"value\": {\n                    \"name\": \"Air Jordan 1 Mid SE\",\n                    \"price\": 134.99\n                },\n            }\n        }\n    )]\n) -&gt; Item:\n    ...\n</code></pre>"},{"location":"resources/exercises/ex01-api-design/#a-note-on-model-design-and-typing","title":"A Note on Model Design and Typing","text":"<p>In your design space for implementing your models, there are likely a few paths worth considering. The design challenge you are confronted with is your API involves two different kinds of resources (text versus links) and they have differences in behaviors, validations, and so on. However, they also share some things in common (such as the namespace for their shortened paths once created, the expiration, and the access counter).</p> <ol> <li>(Do not do this!) Unimodel - Single model shared by both types. This solution is taking on technical debt and becomes gnarly to maintain and extend.</li> <li>(Discouraged) Traditional Inheritance Hierarchy - a single resource subclass of <code>BaseModel</code> which is then subclassed for your specific resources.</li> <li>(Recommended) Discriminated Type Unions - models use a common string field to communicate their type. This is useful because once a Python or JavaScript object is serialized into JSON for transfer over an API, only the field names and values of an object are transferred. By encoding type into a field, it's easier to work with. This strategy has emerged in many dynamic programming languages and is recommended in Pydantic and our front-end language TypeScript.</li> </ol> <p>To learn more about discriminated type unions in Pydantic, see the [official Pydantic documentation]. Additionally, feel free to search or have an interactive learning conversation with ChatGPT to gain a better understanding. Here's an example prompt I tried in ChatGPT that gave a solid introduction. As always, with LLM generated content, read carefully and vigilantly: it doesn't always tell the complete story, the correct story, or have a complete understanding of what exactly you are doing.</p>"},{"location":"resources/exercises/ex01-api-design/#collaboration-for-phase-1","title":"Collaboration for Phase 1","text":"<p>Each of you should individually draft a design of your API in FastAPI on your own branches (branch naming specified after the Getting Started section above). You should both push your branches to GitHub.</p> <p>Once you are ready to merge your branches to form a unified API for your team, we do not recommend actually attempting a merge in <code>git</code>. You are welcomed to, but at your own peril. Since you both worked in <code>main.py</code>, and made design decisions independently, the merge conflict resolution will be gnarly.</p> <p>Instead of attempting a <code>git</code> merge, we strongly suggest pair programming, and starting over by going back to your <code>main</code> branch on one of your machines. Start a new branch based on <code>main</code> that is <code>pair-api-design</code>. On the other of your machines, have open both of your branches in GitHub to easily view how each of you approached the design and try to form a consensus on how to approach. You will be well served by each reading each other's design and then attempting to whiteboard your final approach before diving into code. Once you are complete, push your final <code>pair-api-design</code> to GitHub and submit your teams' reflection for Phase 1 on Gradescope.</p>"},{"location":"resources/exercises/ex01-api-design/#sanity-checks","title":"Sanity Checks","text":"<p>Questions to consider in the context of your API:</p> <ul> <li>Have we ensured that our design addresses every required user story for Sue, Cai, and Amy?</li> <li>Are our naming conventions for endpoints, models, and fields consistent and descriptive enough for all personas?</li> <li>How does our design distinguish between a text snippet and a URL shortener resource when using the same <code>/{resource_identifier}</code> endpoint?</li> <li>Are we including required metadata (e.g., summaries, descriptions, examples) for every endpoint and model field so that a developer can easily understand our API?</li> <li>How have we documented error responses (like 404 for missing resources) in our endpoints?</li> <li>Is the route design intuitive for both API users and maintainers?</li> <li>How easily can our design be extended in the future if new requirements are added?</li> </ul>"},{"location":"resources/exercises/ex01-api-design/#phase-1-submission-and-reflection-questions","title":"Phase 1 Submission and Reflection Questions","text":"<ul> <li>Gradescope submission will include:<ul> <li>Permalink to branches of both partners</li> <li>Permalink to the final <code>pair-api-design</code> branch</li> </ul> </li> <li>Brief reflection question:<ul> <li>What challenges did we encounter when comparing our individual designs, developing a single design, and pair programming our joint, final design?</li> </ul> </li> </ul>"},{"location":"resources/exercises/ex01-api-design/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Designing for Users and Use Cases</p> <p>Through this assignment, you\u2019ve gained experience designing an API that serves multiple types of users with distinct needs. You\u2019ve seen how clear, user-focused API design is essential\u2014not just for making the system functional but also for ensuring a smooth experience for different personas. This mirrors real-world software development, where balancing the needs of end users, system administrators, and stakeholders is key to building successful products.</p> </li> <li> <p>Writing Clear, Professional API Documentation</p> <p>By leveraging FastAPI\u2019s OpenAPI documentation, you\u2019ve practiced writing API specs that go beyond just making things work\u2014you\u2019ve created an API that is easy for others to understand, test, and use. In industry, well-documented APIs are what enable teams to scale, integrate with other systems, and onboard new developers quickly. This attention to detail will serve you well in any software engineering or product development role.</p> </li> <li> <p>Navigating Design Constraints</p> <p>You\u2019ve tackled a unique design challenge: storing and retrieving two different resource types (text snippets and URL redirects) while keeping a shared, opaque namespace. This required careful thinking about data modeling and routing logic. Real-world software design often involves trade-offs like these, where multiple features must coexist within a unified system without exposing unnecessary complexity to users.</p> </li> <li> <p>Collaborating on Software Design in a Team Environment</p> <p>By independently designing an API and then merging your ideas into a unified implementation, you\u2019ve practiced an essential part of professional software development: balancing individual contributions with collaborative decision-making. You\u2019ve navigated trade-offs, discussed design choices, and worked toward a shared vision\u2014skills that are essential in any software engineering role.</p> </li> <li> <p>Designing the Interface First for Human-Centered Development</p> <p>By focusing on the API interface before implementation, you\u2019ve embraced a human-centered approach\u2014prioritizing how users interact with the system rather than getting lost in internal details. This ensures the design is intuitive, valuable, and easy to integrate. A well-defined interface also enables parallel development: frontend teams can build against the spec while backend teams implement functionality, making collaboration more efficient. In real-world projects, this approach reduces wasted effort, improves usability, and accelerates development, ultimately leading to better software.</p> </li> </ol>"},{"location":"resources/exercises/ex01-api-design/#phase-2-implementation","title":"Phase 2: Implementation","text":"<p>In this phase of the exercise, you will implement a service layer in order to have a functional API. All of your Phase 2 business logic should be in the service layer. Your routes should only address HTTP concerns and otherwise delegate control to your service(s).</p> <p>Before beginning on Phase 2, you should complete the following readings and submit them on Gradescope:</p> <ul> <li>Layered Architecture</li> <li>Dependency Injection in FastAPI</li> </ul> <p>To get started on Phase 2, create a new branch named <code>phase2-services</code> and collaborate on it. If you and your partner work together with pair programming, working on this branch together is fine. If you are working async, start your own separate branches and be sure both of you attempt to complete this phase independently.</p> <p>By the end of Phase 2, you should be able to use the <code>/docs</code> UI to complete the stories of this exercise from each user's perspective. Of importance, you should also be able to follow Cai's stories directly in the web browser and be presented with plaintext or redirected to another URL by visiting the shortened URL. Finally, visit tracking and link expiration implementation is left as a challenge for extra credit.</p> <p>Phase 2 should have <code>pytest</code> integration tests cover Sue Sharer and Cai Clicker's stories. You should also write unit tests that cover Sue Sharer and Cai Clicker's stories. See the introduction to testing reading for more guidance on testing. Testing Amy's stories is left as an extra credit opportunity.</p>"},{"location":"resources/exercises/ex01-api-design/#implementation-extra-credit","title":"Implementation Extra Credit","text":"<ul> <li>1 point of extra credit for integration testing and unit testing Amy's stories</li> <li>1 point of extra credit for implementing click tracking in a way that's demonstrable and unit tested</li> <li>1 points of extra credit for successfully implementing resource experiation in a way that's demonstrable and unit tested (hint: you'll need to find a way to cleverly simulate the passage of time by patching or mocking...)</li> </ul>"},{"location":"resources/exercises/ex01-api-design/#phase-2-cicd-in-production","title":"Phase 2: CI/CD in Production","text":"<p>Let's deploy your backend API to the UNC Kubernetes/OKD cluster! Then you can share snippets and redirects with your friends.</p> <p>The steps we follow will be very similar to the tutorial you worked through in class on Monday, February 17th. The general overview is:</p> <ol> <li>Setup Continuous Integration with a GitHub Action</li> <li>Setup a Cloud Deployment on OKD</li> <li>Setup Continuous Deployment from GitHub Action to OKD</li> </ol>"},{"location":"resources/exercises/ex01-api-design/#setting-up-continuous-integration-with-a-github-action","title":"Setting up Continuous Integration with a Github Action","text":"<p>You will want your GitHub Action established on your <code>main</code> branch as that is where the CI/CD pipeline will run. Only one team member should complete this sequence of steps, so coordinate as to who that will be to avoid conflicts. We recommend doing this together, if possible! This is valuable infrastructure to understand how to stand-up and what the implications are. If you do not do it together, and your team mate completes these steps, be sure to read through and check your understanding along the way. Switch to your <code>main</code> branch.</p> <p>GitHub actions run in containers and will need to have your project's dependencies installed, including <code>pytest</code>. Those dependencies are in <code>requirements.txt</code>, but you may not have added <code>pytest</code> to it yet. Be sure <code>pytest</code> is in your <code>requirements.txt</code> pinned to the current version (as of this writing is <code>8.3.4</code>).</p> <p>Add a file named <code>.github/workflows/cicd.yml</code> to your project with the following contents:</p> <pre><code>name: CI/CD Pipeline\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  ci:\n    name: \"Continuous Integration\"\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python 3.13\n        uses: actions/setup-python@v3\n        with:\n          python-version: \"3.13\"\n\n      - name: Install Dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n\n      - name: Validation - pytest\n        env:\n          PYTHONPATH: .\n        run: pytest\n</code></pre> <p>At this point, you are only defining the CI job. This file will be extended to perform the CD job soon.</p> <p>Go ahead and create a commit on <code>main</code> and push this to your repository. Open your repository on GitHub and go to your Actions tab to see that the job runs. If you have no tests merged into your <code>main</code> branch yet then the <code>pytest</code> job will fail, which is expected. You can continue on.</p>"},{"location":"resources/exercises/ex01-api-design/#creating-a-ruleset-for-continuous-integration","title":"Creating a Ruleset for Continuous Integration","text":"<p>Go to your repository's Settings tab &gt; Rules &gt; Rulesets &gt; New ruleset. Create a Ruleset with the following settings:</p> <ul> <li>Name: <code>main</code></li> <li>Enforcement status: Active</li> <li>Targets &gt; Add Target &gt; Include the Default Branch</li> <li>Checked Rules:<ul> <li>Restrict creations</li> <li>Restrict deletions</li> <li>Require a pull request before merging<ul> <li>Required approvals: 0</li> </ul> </li> <li>Require status checks to pass This is CI!<ul> <li>Require branches to be up to date before merging (check)</li> <li>Status checks that are required: Add checks<ul> <li>Search for \"Continuous Integration\" and check it. This is the job in the GitHub action you set up above! This is where your <code>pytest</code>s run. </li> </ul> </li> </ul> </li> <li>Block force pushes</li> </ul> </li> </ul> <p>Save changes. This is a slightly more sophisticated branch ruleset that you have seen prior. Namely, it will require us to use Pull Requests to merge into main. Additionally, before merging, your branches will need to successfully pass your automated tests. This is fundamentally important and characteristic of what continuous integration is.</p>"},{"location":"resources/exercises/ex01-api-design/#creating-a-pull-request","title":"Creating a Pull Request","text":"<p>Let's test this Ruleset by creating a Pull Request. In your GitHub Repository, navigate to the Pull Request tab and click New Pull Request. For the base, be sure your team's repository is targetted and select the <code>main</code> branch. For the compare branch, select your Phase II branch. You should see the commit history and changes between <code>main</code> and your Phase II branch here. Click Create Pull Request.</p> <p>For the title, add a descriptive title along the lines of \"Phase II Milestone: Implementation\" and a meaningful description that describes what you've done in Phase II in a few sentences. Additionally, mention the \"verification steps\" of additional tests added with <code>pytest</code>. Then click Create pull request. </p> <p>After creating the PR, you will see your history of commits and you should see a message indicating \"Some checks haven't completed yet\" with \"Continuous Integration\". Assuming your tests all pass <code>pytest</code>, you will now see a button to merge your branch into <code>main</code>. Wait to do so, for now. If you accidentally do merge, you will want to follow the steps below for what to do after successfully merging and let your team mate know they'll need to do the same.</p>"},{"location":"resources/exercises/ex01-api-design/#adding-okdkubernetes-oc-tool-to-the-dev-container","title":"Adding OKD/Kubernetes' <code>oc</code> Tool to the Dev Container","text":"<p>To setup and manage your Kubernetes/OKD cloud project from your dev container, we need to download and install the <code>oc</code> program onto your image. For the CI/CD Tutorial, this was automatically setup for you. In your project, one of you will need to add this additional configuration and push to your Phase 2 branch (or a new branch if you accidentally already merged).</p> <p>If only one of you completed the prior steps alone, our suggestion is to have the other complete these steps. Like before, both of you should understand what is happening in this sequence. Here we need some additional steps taken after our dev container is created. To add these steps, we'll create a bash shell script in the <code>.devcontainer</code> directory named <code>post-create.sh</code>. A shell script is just a sequence of commands like those you could type into a terminal.</p> .devcontainer/post-create.sh<pre><code># Install `oc` CLI tool\narch=\"$(arch)\"\ncase \"$arch\" in \n    x86_64) export TARGET='' ;; \n    aarch64) export TARGET='arm64-' ;; \nesac\nwget -O /tmp/oc.tgz \"https://github.com/okd-project/okd/releases/download/4.15.0-0.okd-2024-03-10-010116/openshift-client-linux-${TARGET}4.15.0-0.okd-2024-03-10-010116.tar.gz\"\npushd /tmp\ntar -xvzf oc.tgz\nsudo mv oc /usr/bin/oc\nrm kubectl oc.tgz README.md\npopd\n\n# Install Python Packages\npip install --upgrade pip\npip install -r requirements.txt\n</code></pre> <p>The first set of commands downloads the correct <code>oc</code> package (<code>x86_64</code> is for Intel/AMD-based CPUs and <code>aarch64</code> is for Mac M-family chips). The second set of commands installs your <code>python</code> packages.</p> <p>After creating this file, we need to replace the current <code>postCreateCommand</code> in <code>devcontainer.json</code> to run this script instead. Open up <code>.devcontainer/devcontainer.json</code> and change the <code>postCreateCommand</code>'s assigned string to be <code>\"bash .devcontainer/post-create.sh\"</code> (instead of the <code>pip</code> command). Save this file and if you are prompted to rebuild the dev container, accept. If you are not prompted, be sure you saved and then use the Code Command Palatte to run \"Rebuild Container.\"</p> <p>After rebuilding, you should be able to run <code>oc version</code> and see client version <code>4.15.0...</code>. Congrats, you have successfully added the <code>oc</code> tool to your dev container setup!</p>"},{"location":"resources/exercises/ex01-api-design/#adding-a-production-dockerfile","title":"Adding a production <code>Dockerfile</code>","text":"<p>When your project builds in production on OKD it will produce a Docker image. We will control the steps to produce this image using a <code>Dockerfile</code>, as explored earlier in the course. Add a new file to the root directory of your project named <code>Dockerfile</code> with the following contents:</p> Dockerfile<pre><code># Dockerfile for Production Build\n# Use the official Python 3.13 image as the base image.\nFROM python:3.13\n\n# Set the working directory in the container.\nWORKDIR /app\n\n# Copy requirements file to the container.\n# This file should list all Python dependencies.\nCOPY ./requirements.txt /app/requirements.txt\n\n# Install the Python dependencies.\nRUN pip install --upgrade pip &amp;&amp; \\\n    pip install -r requirements.txt\n\n# Copy the rest of the application code.\nCOPY . /app\n\n# Expose port 8080 which uvicorn will run on.\nEXPOSE 8080\n\n# Command to run FastAPI in production mode.\nCMD [\"fastapi\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n</code></pre> <p>This is a good point to add the configuration changes you just made to a new commit, push it to your Phase 2 branch, or another branch if you already merged.</p>"},{"location":"resources/exercises/ex01-api-design/#merging-your-prs-and-continuing-work","title":"Merging your PRs and Continuing Work","text":"<p>We are going to recommend using the Squash and Merge strategy of merging your PRs into <code>main</code> moving forward in this project. Pull up the PR for your Phase 2 changes and look for the green button. If it does not say \"Squash and Merge\", click the down triangle on the button and select Squash and merge. Go ahead and merge. Give the commit message line and extended description a meaningful message. Don't delete the branch after merging, leave it for posterity's sake in this project.</p> <p>Back in your dev container, and your team mates, you will want to switch to <code>main</code> and then pull the changes from your repo. If you use <code>git log -1</code> you should see the squashed and merged commit. Additionally, you should be able to run <code>pytest</code> and see your passing tests. Woo!</p> <p>Generally, when working with Pull Requests (PRs), this is your workflow:</p> <ol> <li>Create a branch locally, push the branch to GitHub</li> <li>Create a Pull Request (PR) on GitHub (select your team's repository's <code>main</code> branch as target)</li> <li>Once ready to merge, merge via Squash and Merge and Fill in Commit message/Message</li> <li>Click Confirm</li> <li>In you and your team mates' dev container: switch to <code>main</code> and pull.</li> </ol> <p>If you haven't followed these steps for your Phase 2 branch, now is a good time to go ahead and do so such that your tests and latest routes are on the <code>main</code> branch before we go to deploy.</p> <p>Additionally, if you make additional changes or fixes to your implementation following deployment, just create new branches, give them meaningful names, be sure your tests cover your changes, and continue on following this process.</p> <p>Soon we will see how Code Review plays into this process in engineering teams, but for now we can forego the code review process.</p>"},{"location":"resources/exercises/ex01-api-design/#manually-deploying-your-project-to-okdkubernetes","title":"Manually Deploying your Project to OKD/Kubernetes","text":"<p>Both members of the team will deploy the exercise to their respective OKD namespaces. This ensures both of you have additional experience setting up the pieces and seeing how they come together. You will want to be sure you and your partner have completed the steps above. Your local dev container should be on <code>main</code>, now with all of your Phase 2 changes merged, and your dev container rebuilt such that <code>oc version</code> succeeds.</p> <p>To work with UNC's OKD/Kubernetes cluster, you need to be on Eduroam or VPN'ed in.</p> <p>Login to OKD here: https://console.apps.unc.edu</p> <p>Get your <code>oc</code> login command by clicking your name in the top right and navigating to copy login command. Click display token and then copy the <code>oc login --token=...</code> line and paste it into your dev container's Terminal. Try running the <code>oc project</code> command to see your OKD/Kubernetes project selected (<code>comp590-140-26sp-&lt;your-onyen&gt;</code>).</p>"},{"location":"resources/exercises/ex01-api-design/#setup-a-fine-grained-personal-access-token","title":"Setup a Fine-grained Personal Access Token","text":"<p>In the CI/CD Tutorial we used a classic personal access token with wide ranging access to read your repositories on GitHub. For this project, let's use the newer style Fine-grained token that gives the token holder permission to read only your EX01 repository. The production setup will be given this token so it can access your code to build your project. To create a new one, on GitHub click your Profile &gt; Settings &gt; Developer settings &gt; Personal access tokens &gt; Fine-grained tokens &gt; Generate new token.</p> <ul> <li>Token Name: EX01 - OKD Access -  <li>Resource owner: comp423-26s</li> <li>Expiration: 30 Days is Fine</li> <li>Description: Giving access to OKD to clone/access the EX01 repo.</li> <li>Repository Access:<ul> <li>Only select repositories: Select your team's repository for ex01.</li> </ul> </li> <li>Permission:<ul> <li>Repository permissions:<ul> <li>Contents: Read-only</li> </ul> </li> </ul> </li> <p>After clicking Generate Token you will be brought to a screen where it shows you, very lightly, the access token created. Copy this token to your clipboard (click the copy icon button), you will need it in the next step!</p> <p>Register your EX01 GitHub Access token as an OKD secret. This secret will be used by your OKD BuildConfig to clone your repository into the build process. Run the following command and substitute your GitHub Username and the Access Token in the placeholders (replace the &lt; and &gt;'s!):</p> <pre><code>oc create secret generic ex01-pat \\\n    --from-literal=username=&lt;your-github-username&gt; \\\n    --from-literal=password=&lt;your-github-pat&gt;\n</code></pre> <p>Let's also label this secret as belonging to <code>app</code> named <code>ex01</code> so that we can easily manage it with other <code>ex01</code> related resources in the future (such as deleting everything when we move on to another project):</p> <pre><code>oc label secret ex01-pat app=ex01\n</code></pre>"},{"location":"resources/exercises/ex01-api-design/#set-up-the-app","title":"Set up the App","text":"<p>OKD's <code>oc</code>'s <code>new-app</code> subcommand is an all-in-one command to establish a <code>Deployment</code>, <code>BuildConfig</code>, <code>ImageStream</code>, and <code>Service</code> for an application based on some common conventions. The option flags we provide below tell it our app is <code>Docker</code>-based, gives it access to the personal access token secret setup above, and because we are running this command in our current working directory <code>.</code>, <code>oc</code> is clever and looks at your <code>git</code> repository's <code>remote</code> servers to know which repository it is hooked up to.</p> <pre><code>oc new-app . \\\n    --name=ex01 \\\n    --source-secret=ex01-pat \\\n    --strategy=docker \\\n    --labels=app=ex01\n</code></pre> <p>Once this command succeeds you can try the following command to follow along with the build of your app in production:</p> <pre><code>oc logs -f buildconfig/ex01\n</code></pre> <p>This command will \"follow\" (thanks to <code>-f</code>), also commonly called tail, the output of your build in production on OKD/Kubernetes. Once the build completes you will be returned back to the command prompt, but you can also stop tailing the log with <code>Ctrl+C</code>. You can observe it following the steps of the <code>Dockerfile</code> when building your image.</p> <p>Next you'll expose a secure \"edge\" route to your service:</p> <pre><code>oc create route \\\n    edge \\\n    --service=ex01 \\\n    --insecure-policy=Redirect\n</code></pre> <p>Finally, find your app's public URL on cloud apps with the following subcommand:</p> <pre><code>oc get route ex01\n</code></pre> <p>Copy the host and paste it into your browser. Try navigating to <code>/docs</code> on this host, as well. You should see your app running in production!</p> <p>If you wanted to manually initiate a new build for your app based on your <code>main</code> branch, you can now do so with the following command: <code>oc start-build ex01</code>. However, we'd really like to automate deployment following a successful CI verification run and merge into the <code>main</code> branch. So let's setup continuous deployment!</p>"},{"location":"resources/exercises/ex01-api-design/#setting-up-continuous-deployment","title":"Setting up Continuous Deployment","text":"<p>Now that your app is running in production, let's automate deployment on successful PR merges to <code>main</code>. You will update your GitHub Action to include a CD step.</p>"},{"location":"resources/exercises/ex01-api-design/#adding-a-repository-secret-with-the-url-of-your-okd-build-webhook","title":"Adding a Repository Secret with the URL of your OKD Build WebHook","text":"<p>What is a WebHook URL? It's just an API endpoint that one service (OKD/Kubernetes in our case) can expose to allow other services (GitHub) to notify them of something important. For us, we will find a secret URL OKD/Kubernetes exposes which, if we make an HTTP POST request to it, it will kick off a new build for our project in OKD/Kubernetes. We can try this from the terminal of your dev container:</p> <p>First, find the secret URL:</p> <pre><code>oc describe bc/ex01 | grep -C 1 generic\n</code></pre> <p>You should see a URL that looks something like: <code>https://api.apps.unc.edu:6443/apis/build.openshift.io/v1/namespaces/comp590-140-26sp-ONYEN/buildconfigs/ex01/webhooks/&lt;secret&gt;/generic</code></p> <p>Next, we need to find the secret to plug into the <code>&lt;secret&gt;</code> part of the path. This is found in the YAML configuration for the BuildConfig (<code>bc</code>). We can filter down to it using <code>grep</code> to search for <code>generic</code> with one line of context around the matching line:</p> <pre><code>oc get bc ex01 -o yaml | grep -C 1 generic\n</code></pre> <p>You should copy the secret and paste it in place of the <code>&lt;secret&gt;</code> place holder in your WebHook URL. Copy this whole URL to your clipboard, we'll use it in two places.</p> <p>First, let's test making a POST request to the WebHook from your dev container using the <code>curl</code> command-line utility. The <code>curl</code> program allows you to make HTTP requests from the command-line and is highly configurable:</p> <pre><code>curl -X POST &lt;paste your URL with secret here&gt;\n</code></pre> <p>This command will result in a message that says an invalid content-type was provided (we didn't provide any request body!) but that it is \"ignoring payload and continuing with build.\" Woo!</p> <p>You can once again follow the build you just kicked off with the command:</p> <pre><code>oc logs -f buildconfig/ex01\n</code></pre> <p>We didn't really want to start this build, we were just testing it, so you can cancel the build with the following:</p> <pre><code>oc get builds\noc cancel-build ex01-X\noc get builds\n</code></pre> <p>Replace the <code>X</code> with the number of the build you are attempting to cancel.</p> <p>Let's add this secret WebHook URL to your EX01 repository as a secret your GitHub Action will be able to use. Repository &gt; Settings &gt; Secrets and variables &gt; Actions &gt; New Repository Secret:</p> <ul> <li>Name: <code>CD_BUILD_WEBHOOK_&lt;ONYEN&gt;</code> (substitute your ONYEN)</li> <li>Secret: Paste your secret Webhook URL found above</li> </ul> <p>Save your repository secret. This will serve as a variable name in the next step.</p>"},{"location":"resources/exercises/ex01-api-design/#updating-github-action","title":"Updating GitHub Action","text":"<p>Since both you and your team mate will both kick-off the CD step from the same GitHub Action definition, you will want to coordinate who makes the initial changes and who adds to it second. This will help avoid merge conflicts.</p> <p>If you are the first to establish continuous deployment for your production environment, add the following to the end of your YAML file. If you are the second of your pair, just read this step to understand it, pull from <code>main</code> to get this going, and then continue to your instructions following.</p> <pre><code>  cd:\n    name: \"Continuous Deployment\"\n    needs: ci\n    if: ${{ github.event_name == 'push' }}\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Notify OKD to Build and Deploy\n        run: |\n          curl -X POST ${{ secrets.CD_BUILD_WEBHOOK_&lt;ONYEN&gt; }}\n</code></pre> <p>Be sure to substitute the <code>&lt;ONYEN&gt;</code> with yours so that it matches the name of the secret you established above.</p> <p>The <code>cd</code> line should be at the same level of indentation as the <code>ci</code> entry which came in the section above. Both <code>ci</code> and <code>cd</code> are direct descendents of <code>jobs</code> based on indentation. Notice a few features of this <code>cd</code> definition:</p> <ul> <li><code>needs: ci</code> Indicates that this job is taking a dependency on the <code>ci</code> job being successful above.</li> <li><code>if: ${{ github.event_name == 'push' }}</code> this conditional differentiates a push (which a merge is treated as) from a pull request build. Thus, continuous deployment is skipped on Pull Requests until a PR is merged to <code>main</code> (and \"pushed\" to <code>main</code>). This makes sense because you do not want to deploy to production until you merge to <code>main</code>!</li> <li>Notice in the <code>steps</code> that the command <code>run</code> is the same one you ran from the terminal with <code>curl</code>. This will trigger the WebHook.</li> </ul> <p>Switch to a new branch, perhaps <code>cd-setup</code>, add these changes, make a commit, and push. Following the push, go create a Pull Request into your repository's <code>main</code> branch from the <code>cd-setup</code> branch you just pushed.</p> <p>After creating the PR, you should see that your tests will run as part of the CI step and that your CD steps will be skipped, correctly, because of the <code>if</code> condition added above. Go ahead and merge this change into <code>main</code> with Squash and Merge.</p> <p>After merging, go checkout your Actions tab. Take a look at how you now have a pipeline that runs: Continuous Integration followed by Continuous Deployment. These names were configured by the <code>name</code> fields in your <code>cicd.yaml</code> file in your project. You can click on either to see the process each moves through. Once the Continuous Deployment step succeeds, you should be able to check your builds to see a new build was initiated:</p> <pre><code>oc get builds\n</code></pre> <p>Once that build completes, it will deploy and you have setup a complete CI/CD workflow to a kubernetes cluster! This is something to be proud of! Now, as you make changes to your project, you will need to push to a branch, create a PR, pass all your tests, and then upon merging to <code>main</code> your work will be pushed to production automagically. This is a very real industry-grade software engineering workflow and pipeline.</p> <p>Back in your dev container, switch back to <code>main</code> and pull to incorporate your squashed and merged commit.</p> <p>For the second member of the team to setup continuous deployment. After adding your OKD project's webhook URL as a secret to your repository, your steps are as follows:</p> <p>Switch to <code>main</code> in your dev container and pull your partner's merged work. Open your <code>.github/workflows/cicd.yaml</code> file and you should see their <code>cd</code> step added above. Go ahead and create a new branch, perhaps named <code>cd-extension</code>. For your part, you're just going to add one more line to the \"Notify OKD to build and deploy\" step:</p> <pre><code>    steps:\n      - name: Notify OKD to Build and Deploy\n        run: |\n          curl -X POST ${{ secrets.CD_BUILD_WEBHOOK_&lt;FIRST_ONYEN&gt; }}\n          curl -X POST ${{ secrets.CD_BUILD_WEBHOOK_&lt;SECOND_ONYEN&gt; }}\n</code></pre> <p>You will add the second <code>curl</code> command with your ONYEN replacing the <code>&lt;SECOND_ONYEN&gt;</code> placeholder. This will cause <code>curl</code> to first trigger your partner's build and then yours immediately after.</p> <p>Go ahead and add this to a <code>git</code> commit on your <code>cd-extension</code> branch. Push it. Make a new PR to your repository's <code>main</code> branch. Your tests should all still pass in CI and then you should be able to squash and merge.</p> <p>Upon merge, your GitHub Actions page should reflect that another worflow has begun and you can see the pipeline progress. After it completes the Continuous Deployment step, you can check your OKD production builds:</p> <pre><code>oc get builds\n</code></pre> <p>You can also tail your build:</p> <pre><code>oc logs -f bc/ex01\n</code></pre> <p>Now your Continuous Deployment step initiates builds on both teammates' cloud projects! Congratulations, you have a working CI/CD pipeline.</p>"},{"location":"resources/exercises/ex01-api-design/#finishing-up-ex02","title":"Finishing up EX02","text":""},{"location":"resources/exercises/ex01-api-design/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"resources/exercises/ex01-api-design/#my-github-action-is-not-running-or-failing-why","title":"My Github Action is Not Running or Failing, Why?","text":"<p>If you do not see any attempts to build your Actions in the GitHub Actions tab of your repository, it is likely one of three reasons:</p> <ol> <li>You have not pushed the original action definition to <code>main</code></li> <li>You have not initiated a Pull Request with the action definition and targetted <code>main</code></li> <li>Your action file is improperly named. Be sure the file extension is <code>.yml</code></li> </ol> <p>If your build is failing, there are likely one of two reasons. Open up the Action and dig into its details to see exactly where it fails by drilling in.</p> <ol> <li>If the validation/pytest step fails because <code>pytest: command not found</code>, it is because you are missing the <code>pytest</code> dependency in <code>requirements.txt</code>. Be sure not to skip that step (search for it in the steps above!)</li> <li>Your tests are failing in GitHub Actions for some reason. Drill in in to investigate and keep pushing additional commits to your PR until your tests are passing.</li> </ol>"},{"location":"resources/exercises/ex01-api-design/#should-shortened-urls-redirect-to-the-url-or-just-display-the-url","title":"Should shortened URLs redirect to the URL or just display the URL?","text":"<p>For Cai's stories, you should be able to access the shortened URL directly in the web browser, not using <code>/docs</code> and be redirected to the URL that was shortened. You can search for FastAPI's <code>RedirectResponse</code> (and see the FAQ entry below about making sure this works in production). Note that the OpenAPI <code>/docs</code> UI will look like there is an error occuring on most redirects because it follows the redirect rather than shows you the redirect response. If you see a CORS error, you're probably doing it right, but to be sure you can try accessing your URL directly (e.g. <code>127.0.0.1:8000/short-url</code>) or looking in the network tab of your browser when using the <code>/docs</code> UI. Additionally, this is a place where you should have an integration test that can confirm a redirect response is correctly being returned.</p>"},{"location":"resources/exercises/ex01-api-design/#how-should-you-handle-hostname-differences-between-development-and-production","title":"How should you handle hostname differences between development and production?","text":"<p>In development your host name is likely your localhost IP address followed by a port: <code>localhost:8000</code>. In production, your hostname will be something like <code>ex01-comp590-140-26sp-ONYEN.apps.unc.edu</code>. If your Sue routes need to produce URLs for Cai to click on, you should not hard code <code>localhost</code>. Instead, you can use a dependency injection for FastAPI's <code>Request</code> object and inspect its host. Or, you can add the following helpful service to a new <code>url_service.py</code> file and inject it into a route instead. Here's the implementation:</p> url_service.py<pre><code>\"\"\"Service for creating URLs based on the current request's hostname.\"\"\"\n\nfrom fastapi import Request\n\n__author__ = \"Kris Jordan &lt;kris@cs.unc.edu&gt;\"\n\n\nclass URLService:\n    \"\"\"Service for creating URLs.\"\"\"\n\n    def __init__(self, request: Request):\n        \"\"\"Request is dependency injected by FastAPI.\"\"\"\n        self._request = request\n\n    def url_to_path(self, path: str) -&gt; str:\n        \"\"\"Create a URL with the same scheme and host as the request for a given path.\n        This is useful for avoiding hardcoding a host name or http/https prefix so that\n        the service can be used in both production and development environments.\n\n        In OKD, `x-forwarded-port` will be \"443\" for HTTPs and \"80\" for HTTP.\n\n        Args:\n            path: The path to create a URL for based on the current request.\n\n        Returns:\n            The created URL.\n        \"\"\"\n        port = self._request.headers.get(\"x-forwarded-port\") or self._request.url.port\n        scheme = \"https\" if port == \"443\" else \"http\"\n        host = self._request.headers.get(\n            \"x-forwarded-host\"\n        ) or self._request.headers.get(\"host\")\n        return f\"{scheme}://{host}/{path}\"\n</code></pre> <p>Then, from your <code>main.py</code>, you can import <code>URLService</code> and inject it into a route and use it as such:</p> <pre><code>@app.get(\"/demo\", ...)\ndef demo(url_svc: Annotated[URLService, Depends()]) -&gt; str:\n    return url_svc.url_to_path(\"abc123\")\n</code></pre> <p>The result of using this service's <code>url_to_path</code> as shown above is it would return <code>\"http://127.0.0.1:8000/abc123\"</code> if you are running in development. In production, it will return <code>\"https://ex01-comp590-140-26sp-ONYEN.apps.unc.edu/abc123\"</code>, instead. If you hardcoded a hostname anywhere, you will want to either come up with your own solution or use the service above.</p>"},{"location":"resources/exercises/ex02-ng-frontend/","title":"Angular Front-end for Link Sharing App","text":"<p>This exercise is a SOLO exercise. Everyone will establish their own repository and work to complete a simple front-end for their own API backend.</p> <p>In this exercise you will build a web front-end for your Link Sharing API of EX01. In the process you will gain experience with:</p> <ol> <li>High-level CORS (Cross-Origin Resource Sharing) Concerns</li> <li>TypeScript:<ul> <li>Interfaces</li> </ul> </li> <li>Angular:<ul> <li>Components</li> <li>Signals</li> <li>Services</li> <li>HttpClient</li> <li>Dependency Injection</li> <li>Routing</li> </ul> </li> </ol>"},{"location":"resources/exercises/ex02-ng-frontend/#enabling-cross-origin-resource-sharing-in-ex01","title":"Enabling Cross-Origin Resource Sharing in EX01","text":""},{"location":"resources/exercises/ex02-ng-frontend/#understanding-cors","title":"Understanding CORS","text":"<p>CORS (Cross-Origin Resource Sharing) is a security feature implemented by web browsers that restricts web pages from making requests to a different domain than the one that served the original page. This security measure exists to prevent malicious websites from making unauthorized requests to other domains using your credentials.</p> <p>When you build a web application with a separate frontend and backend (like we're doing with Angular and FastAPI), they typically run on different domains or ports during development. Without proper CORS configuration, your Angular application running on one port (e.g., localhost:4200) won't be able to make API requests to your FastAPI backend running on another port (e.g., localhost:8000).</p> <p>Additionally, for this exercise, we will deploy your front-end as a static web page on GitHub pages to further emphasize the separation between frontend and backend separation. Your EX02 frontend will run on GitHub Pages and (origin host: comp423-26s.github.io) and backend will run on your personal OKD project hostname. This also requires CORS for requests to succeed across origins.</p>"},{"location":"resources/exercises/ex02-ng-frontend/#using-middleware-in-fastapi","title":"Using Middleware in FastAPI","text":"<p>In web frameworks like FastAPI, middleware functions as a bridge between the server and your application code. Middleware intercepts requests and responses, allowing you to modify or process them before they reach your route handlers or before they're sent back to the client.</p> <p>The <code>CORSMiddleware</code> specifically manages HTTP headers related to CORS. When added to your application, it automatically handles setting the appropriate headers that tell browsers to permit cross-origin requests from your frontend application.</p> <p>In EX02, for simplicity's sake, we will enable very permissive CORS settings that are generally far more permissive than you would typically enable in a production application. In essence, we are disabling CORS protection of your API to make integration with our client in this application more straightforward. In true, industrial applications you will specify very specific hosts which you accept CORS API requests from.</p>"},{"location":"resources/exercises/ex02-ng-frontend/#enable-cors","title":"Enable CORS","text":"<p>You will need to add the following code to update your EX01 production deployment to enable CORS for working on EX02. Collaborate with your partner on EX01 to decide who will make this update, push the branch, and make the PR. </p> <p>First, import FastAPI's CORSMiddleware module in <code>main.py</code>'s imports section:</p> <pre><code>from fastapi.middleware.cors import CORSMiddleware\n</code></pre> <p>Then, register the middleware by adding the following snippet after you define your <code>app</code> in <code>main.py</code>:</p> <pre><code># Add CORS middleware to allow requests from any origin\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Allows all origins\n    allow_credentials=True, \n    allow_methods=[\"*\"],  # Allows all methods\n    allow_headers=[\"*\"],  # Allows all headers\n)\n</code></pre> <p>Push this to a branch, make a PR, and ask your teammate to merge the PR so they can confirm the work you completed.</p>"},{"location":"resources/exercises/ex02-ng-frontend/#setting-up-the-ex02-dev-container","title":"Setting up the EX02 Dev Container","text":"<p>To get started on this project, you will need to accept and clone the EX02 starter repository here: https://classroom.github.com/a/ZInataQQ</p> <p>Once cloned, open the project in a dev container and wait for the post create build steps to complete. </p> <p>Start the development server with the following command:</p> <pre><code>ng serve --host=\"0.0.0.0\"\n</code></pre> <p>You may need to press enter to a prompted question about autocompletion rules. Just accept the default suggestion.</p> <p>The <code>host</code> flag instructs the Angular development server to listen on all IP addresses (0.0.0.0) for requests, which ensures Docker is able to route you through from your host to the dev container.</p> <p>Open http://localhost:4200 and you should be greeted with a simple user interface with some code to help get you started with this project.</p>"},{"location":"resources/exercises/ex02-ng-frontend/#starter-code-orientation","title":"Starter Code Orientation","text":""},{"location":"resources/exercises/ex02-ng-frontend/#core-files","title":"Core Files","text":"<ul> <li><code>src/main.ts</code> - Application entry point that bootstraps the Angular app</li> <li><code>src/app/app.component.ts</code> - Root component that serves as the application shell</li> <li><code>src/app/app.routes.ts</code> - Defines the application's routing configuration</li> </ul>"},{"location":"resources/exercises/ex02-ng-frontend/#components","title":"Components","text":"<ul> <li> <p>Share Component (<code>src/app/share/</code>):</p> <ul> <li><code>share.component.ts</code> - Form implementation using Angular Reactive Forms and Signals</li> <li><code>share.component.html</code> - Template with resource type selection and content input</li> </ul> </li> <li> <p>Navigation Component (<code>src/app/navigation/</code>):</p> <ul> <li><code>navigation.component.ts</code> - Simple navigation logic</li> <li><code>navigation.component.html</code> - Navigation links using Angular's router directives</li> </ul> </li> </ul>"},{"location":"resources/exercises/ex02-ng-frontend/#1-implement-sues-stories","title":"1. Implement Sue's Stories","text":"<p>Your task is to introduce an angular Service object that integrates with your API in order to complete the implementation of the <code>ShareComponent</code>, which will make use of the Service.</p> <p>The general strategy for making progess here is:</p> <ol> <li>Read and understand what is going on in the Share Component's TypeScript controller and HTML view template.</li> <li>Generate a Service (hint: use <code>ng generate service</code> subcommand).</li> <li>Define a method on the service for creating a link and/or snippet. It should take the appropriate parameters (which will be provided by the component) for creating a resource for your API (e.g. the type of resource and the content of the resource).<ul> <li>Assuming your API end point for Sue just returns a string, this method should return an <code>Observable&lt;string&gt;</code>, which will represent a shortened URL. If your API returns an object with properties, you will need to define an <code>interface</code> that has thoes properties and give it a name. Your method would return <code>Observable&lt;YourInterfaceName&gt;</code> instead.</li> <li>For now, you can implement a fake skeleton of the method by having it return <code>of(\"https://foo.bar\")</code> by importing the <code>of</code> function from <code>rxjs</code> (read more about <code>of</code> here: https://rxjs.dev/api/index/function/of). If your API returns an object, you can return an anonymous object with the expected fields, such as <code>of({\"url\":\"https://foo.bar\"})</code>;</li> </ul> </li> <li>Inject your service into the <code>ShareComponent</code>'s constructor as a private variable.</li> <li>Update the submission logic to call out to your service object's method(s) and subscribe to the result as you read about in the HttpClient reading: https://angular.dev/guide/http/making-requests#fetching-json-data. For more information on how to subscribe, see the official RxJS documentation: https://rxjs.dev/guide/observer.</li> <li>Ultimately, you will need to update the <code>ShareComponent</code>'s controller and HTML template upon success (or failure...) of the service method such that the template displays the resulting URL generated by your API. You should be able to click the URL and be taken to a new tab (make it an <code>a</code> tag and set the <code>target</code> attribute to <code>\"_blank\"</code>).</li> <li>Now it's time to actually implement integrating with your production API! You will need to inject <code>HttpClient</code> into your service, as is discussed in the <code>HttpClient</code> setup documentation https://angular.dev/guide/http/setup. Then, in your methods, replace the call to <code>of</code> with a call to your injected <code>httpClient</code>'s <code>post</code> method, like you read about in the \"Mutating server state\" of the Angular HttpClient Reading. You will need to provide a request body object which contains all necessary information for your API.</li> <li>Try it! One of two things will happen:<ul> <li>Nothing or an error. Open your Chrome Developer Tools and look in the console and the network tab for more diagnostics on what is going on behind the scenes. The network tab will be your best tool here. Investigate the tab. Not seeing your request? Be sure <code>Fetc/XHR</code> is shown. Still not seeing it? Did you save everything and <code>subscribe</code> to the result of your service method call from your component? Remember, the returned value from your service method is an <code>Observable</code> not an actual result! Seeing an error? Look at the HTTP response code to diagnose. If it's a CORS issue, return back to the CORS steps above and be sure you successfully deployed. If it's a 422, it means your request body did not fulfill the expectations of your API from EX01. Be sure you are sending all the data you need!</li> <li>Success: Woo! Sue will be so happy to use your new UI rather than the <code>/docs</code> UI!</li> </ul> </li> </ol>"},{"location":"resources/exercises/ex02-ng-frontend/#2-implement-resource-list-link","title":"2. Implement Resource List Link","text":"<p>For the second required portion of this exercise, you will introduce a new routable component for listing resources in your backend API for Amy. No filtering user interface is required. We'll make this functionality visible to all users, though, since this is just a demo application.</p> <p>Your goal here is to add a new item to the navigation for a resource list, add a component that you get routed to when viewing the resource list, and add additional service functionality and control templating for displaying all resources stored in your backend API.</p> <p>The general strategy here is to:</p> <ol> <li>Generate a new component (<code>ng g component</code>)</li> <li>Make the component routable via <code>app.routes.ts</code></li> <li>Add a link to <code>navigation/navigation.component.html</code> so that you can navigate to the component</li> <li>Inject your API service into your component (like you did in <code>ShareComponent</code>)</li> <li>Implement a skeleton of a method in your service for listing resources from your API. Is should return type <code>Observable&lt;YourAPIResponseType&gt;</code> where <code>YourAPIResponseType</code> is an interface you define to match your API's expectations. It may also be <code>YourAPIResponseType[]</code> if your API returns a list of the response and not an object.</li> <li>Implement the lifecycle interface <code>OnInit</code> that gets called when a component is loaded (in this case: routed to). This will subscribe to the service class' method. It should update one or more signal(s) you define in the class.</li> <li>The values of those signals should be read by the HTML template so that the resulting list of resources in your API are produced to the view layer of the system and you can see them in a table format.</li> </ol> <p>When deciding what and how to show the information on this screen the question you should ask yourself is, \"what would I want to see if I were Amy Admin?\"</p>"},{"location":"resources/exercises/ex02-ng-frontend/#publishing","title":"Publishing","text":"<p>The project comes predefined with a GitHub Action for publishing your web site. You will need to enable GitHub Pages for your project, though. To do so, go to your project's settings, go to Pages, and turn pages on from the <code>gh-pages</code> branch. It will take a couple minutes for this to take effect. You should be able to follow with the publishing process in the Actions tab. Once publishing is fully ready, you should see a link to your website if everything built correctly.</p>"},{"location":"resources/exercises/ex02-ng-frontend/#branch-expectations","title":"Branch Expectations","text":"<p>You are encouraged to experiment in branches while making progress on this exercise. However, your <code>git</code> workflow in this exercise is up to you. Ultimately, we expect your final product to be on <code>main</code> since that is what will be deployed live.</p>"},{"location":"resources/exercises/ex02-ng-frontend/#required-angular-conventions","title":"Required Angular Conventions","text":"<p>No credit will be given to projects which use old Angular style <code>ngIf</code> or <code>ngFor</code> tags, you should use the <code>@if</code> and <code>@for</code> functionality we read about. Additionally, all data that is shared between a <code>Component</code> typescript file and its HTML template should be done so via modern angular <code>signal</code>s, like you read about.</p>"},{"location":"resources/exercises/ex02-ng-frontend/#extra-credit","title":"Extra Credit","text":"<p>The following opportunities are available for extra credit, arranged by least to most challenging:</p> <ol> <li>1pt - Implement Vanity URLs in the Frontend</li> <li>1pt - Implement Expiration Dates when sharing a URL in the Frontend</li> <li>1pt - Implement Amy's Filtering Stories in the List Resources View</li> <li>2pt - Implement all of Amy's Stories (update, delete, view access counts)</li> </ol>"},{"location":"resources/exercises/sp00-api-spec/","title":"Sprint 1 - Week 1 - Interactive Prototypes &amp; API Scaffolding","text":"<p>Now that your initial feature design document and mid-fi wireframes are in place, your team will extend your work by developing interactive, clickable prototypes. These prototypes will enable your team to clearly demonstrate user interactions, providing peers with tangible insights into your feature\u2019s workflow. These clickable prototypes will be high fidelity and utilize the Material 3 design system of the CSXL. Additionally, you'll start scaffolding out the backend API routes and models necessary for your feature using FastAPI as the foundation.</p>"},{"location":"resources/exercises/sp00-api-spec/#assignment-details","title":"Assignment Details","text":""},{"location":"resources/exercises/sp00-api-spec/#1-clickable-prototypes","title":"1. Clickable Prototypes","text":"<ul> <li>Choose your primary user stories and their wireframes from last week to convert them a high fidelity wireframes using the CSXL Figma template.<ul> <li>Duplicate the CSXL Figma template into one of your team members' Figma spaces and add your team members to it.</li> <li>Add your story to the <code>Getting Started + YOUR DESIGN</code> page. Copy starter pages from CSXL Design page and material components from Components.</li> <li>We expect to see Material components and design principles followed throughout, unlike in the lo/mid-fi wireframes produced last week.</li> </ul> </li> <li>Turn your high-fidelity wireframes into a clickable Figma prototype as shown in class.</li> <li>Clearly illustrate the interactions from your top 3 critical user stories.</li> <li>Ensure each clickable prototype demonstrates:<ul> <li>The sequence of interactions.</li> <li>Where your feature will use the OpenAI LLM API.</li> <li>Key interactions or critical decision points clearly presented.</li> </ul> </li> <li>Link each clickable prototype clearly within your stories in your design document (Google Docs, Notion, etc.).</li> <li>Be prepared to present your top two story prototypes in a concise, 5-minute demonstration during class on Monday, March 31st. For each presented user story, include:<ul> <li>The associated REST API route(s) (next part).</li> <li>The model(s) it utilizes.</li> </ul> </li> </ul> <p>Prototype Presentation Advice</p> <ul> <li>Clearly narrate your user's journey through your prototype.</li> <li>Highlight the value your feature provides through specific interactions.</li> <li>Keep explanations clear and to-the-point to manage your time effectively.</li> </ul>"},{"location":"resources/exercises/sp00-api-spec/#2-rest-api-model-scaffolding","title":"2. REST API &amp; Model Scaffolding","text":"<p>Begin detailing your API in your design document by:</p> <ul> <li>Defining REST API routes that clearly support your critical user stories.</li> <li>Describing new models your feature's REST API will require. Refer to existing models located at <code>backend/models</code> in the CSXL repository to guide your designs.</li> <li>Clearly indicate if existing routes or models will need augmentation or modification.</li> </ul> <p>Your document should clearly communicate:</p> <ul> <li>Route HTTP methods (<code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>) and paths.</li> <li>The purpose of each route.</li> <li>Basic descriptions of the data each route will handle.</li> </ul>"},{"location":"resources/exercises/sp00-api-spec/#3-integration-analysis","title":"3. Integration Analysis","text":"<p>Answer the following critical integration questions in your design document:</p> <ul> <li>Existing Dependencies:<ul> <li>Identify specific files, classes, and methods your feature's starting point will directly depend upon, extend, or integrate with. Provide permalinks (including line numbers) from the CSXL codebase.</li> </ul> </li> <li>API &amp; Models:<ul> <li>Clearly cite where you plan to add routes (new files are acceptable, but provide their complete paths).</li> <li>Clearly cite where you plan to add or modify models.</li> </ul> </li> <li>Frontend Components:<ul> <li>Identify how you will organize your frontend component(s).</li> </ul> </li> <li>AI Prompts:<ul> <li>Behind the scenes your backend will need to make requests to the OpenAI API (think: ChatGPT). Start to brainstorm the prompts your backend routes will use with the AI, in other words: what would you put into the ChatGPT chat box and expect back. A common strategy is taking some user text and converting it into a structured JSON output you specify.</li> <li>Identify at least one prompt and the JSON model format you expect as a response from the OpenAI API. Give some sample input text or JSON data from user inputs, representative of what your backend would send to the OpenAI API, and provide concrete examples of expected responses.</li> </ul> </li> </ul> <p>Clearly identifying these dependencies and frontend needs early will streamline your future development and avoid surprises.</p>"},{"location":"resources/exercises/sp00-api-spec/#project-management-best-practices","title":"Project Management Best Practices","text":"<p>On Wednesday, we'll provide instructions for setting up your team's project board on GitHub. We'll dedicate class time on Friday to discussing GitHub issues and project board best practices. Keep the following in mind:</p> <ul> <li>Tasks should be clearly described, assigned to team members, and updated frequently.</li> <li>Link each project board card to relevant issues in your GitHub repository.</li> </ul> <p>Why Project Management Matters</p> <p>Using structured project management practices from the beginning will improve:</p> <ul> <li>Team coordination and productivity.</li> <li>Code quality through continuous peer review.</li> <li>Your shared understanding of the codebase and collaborative skills.</li> </ul>"},{"location":"resources/exercises/sp00-api-spec/#team-project-setup","title":"Team Project Setup","text":"<p>Your team will share a GitHub repository for collaboration. This repository is where you all will create pull requests, perform code review, and establish a continuous deployment pipeline (next sprint).</p> <ol> <li> <p>To get started, designate one member of your team to establish the repository. This member will create a new team named after your assigned team table, e.g. Team A1 if you are assigned table A1. See the teams sheet to verify your team table. Then, follow this link to establish your team and create a blank, starter repository. Once this is completed, other members of team should join the team and the repository.</p> </li> <li> <p>Another member of the team should be designated for the initial repository push. This team member should have already joined the team and should be able to see the blank repository accepting the assignment resulted in on GitHub (named after your team table). As part of RD26, you setup a local development environment for the CSXL. You should open that dev container and go ahead and pull from origin one more time to get the latest updates from the upstream CSXL repository. Go ahead and remove the remote repository named <code>origin</code> from your repository. Then, add a new remote repository named <code>origin</code> that is directed at the <code>https://github.com</code> URL of your final project. Go ahead and push <code>main</code> to <code>origin</code> and confirm that your team's repository now has the complete history of the CSXL repo in it. You will see a latest commit from your TA Andrew (<code>ItIsAndrewL</code>) as the most recent commit with ID prefix <code>a46bf64</code>.</p> </li> <li> <p>After completing step 2, all other members of the team should update their local CSXL development environment to remove the git <code>origin</code> remote and establish a new remote, named <code>origin</code>, that points to your team's GitHub repository (use the <code>https://github.com</code> URL). After doing so, you should be able to perform <code>git pull origin main</code> and it succeed. Additionally, you can verify correctness by running <code>git remote show origin</code> to see that it is pointed to your team's repository and not the official CSXL repository.</p> </li> <li> <p>Only after everyone has successfully completed step 3, a third member of your team should establish a project board for your team.</p> <ol> <li>Begin by opening you your team repo (<code>comp423-26s/csxl-team-XN</code> where <code>XN</code> is your table number, like <code>a1</code>). </li> <li>Be sure you do this from your team's repository page! From this page, click the <code>Projects</code> tab. Click <code>New Project</code>. </li> <li>From the modal with templates, select <code>Featured</code> and then select the <code>Kanban</code> template. </li> <li>Name the project \"Team XN Project Board\", where XN is your table. </li> <li>Finally, press the ellipses <code>...</code> in the top right corner, beneat your profile photo, and select Settings. </li> <li>Click Manage Access and under \"Invite Collaborators\", search for your team, select it, make the role Admin, and click Invite.</li> </ol> </li> <li> <p>After completing step 4, other members of the team should verify they are able to access the team project board by going to the repository on GitHub, clicking the Projects tab, seeing the project show up there and able to navigate to it.</p> </li> </ol> <p>We will discuss setting up your project board in class on Friday 3/28.</p>"},{"location":"resources/exercises/sp00-api-spec/#submission-demonstration","title":"Submission &amp; Demonstration","text":"<ul> <li>Continue updating your original design document.</li> <li>Ensure all clickable prototypes are clearly linked to from within your user stories. Test these links in an incognito window. You should be taken directly to the start of a story clickthrough.</li> <li>Be ready to demonstrate your clickable prototypes clearly and succinctly on March 31st, including clear references to routes and models associated with each story.</li> </ul> <p>This structured, design-forward approach will enhance both the quality and manageability of your feature, setting a strong foundation for the development cycles ahead.</p>"},{"location":"resources/exercises/sp00-cfp/","title":"Call for Proposals: Integrating AI into the CSXL Web Application","text":"<p>This semester in our software engineering course, you'll have the opportunity to enhance the CSXL web application by incorporating AI functionality using OpenAI's powerful language models (LLMs). Our goal is to make the XL app smarter, more user-friendly, and more engaging through thoughtful integration of AI.</p>"},{"location":"resources/exercises/sp00-cfp/#what-is-the-csxl-web-application","title":"What is the CSXL Web Application?","text":"<p>The CSXL web app, , currently helps you and other UNC CS students by: <ul> <li>Reserving coworking spaces (rooms and desks)</li> <li>Managing tickets for course office hours</li> <li>Providing a directory of student organizations</li> <li>Allowing applications to become an undergraduate teaching assistant (UTA)</li> </ul> <p>We are seeking proposals that creatively use AI to expand and enhance these existing functionalities\u2014or propose entirely new features!</p>"},{"location":"resources/exercises/sp00-cfp/#project-ideas","title":"Project Ideas","text":"<p>The following suggestions illustrate possible areas for AI integration, but feel free to expand upon them or propose something completely new! The three provided ideas are arranged in relative order of difficulty and scope. We believe the first is the most straightforward idea, followed by the second, and finally the third (study buddy) is the broadest in scope.</p>"},{"location":"resources/exercises/sp00-cfp/#1-ai-enhanced-room-and-desk-reservations","title":"1. AI-Enhanced Room and Desk Reservations","text":"<p>Imagine a conversational assistant integrated into the CSXL app that allows you to:</p> <ul> <li>Check room or desk availability using natural language (\"Are there pair programming rooms available tomorrow at noon?\")</li> <li>Manage or cancel reservations easily (\"Cancel my reservation for today at 3 PM.\")</li> <li>Query real-time usage information (\"Is my friend Joan at the XL right now?\" or \"How busy is it right now?\")</li> <li>Tracking of requests asked, along with success or failure in addressing the user's prompt, so that we can improve the feature over time.</li> </ul> <p>What other intelligent reservation features can you envision?</p>"},{"location":"resources/exercises/sp00-cfp/#2-smarter-office-hours-ticket-support","title":"2. Smarter Office Hours Ticket Support","text":"<p>AI could help students submit better, more detailed office-hour tickets by:</p> <ul> <li>Checking ticket submissions to ensure clarity and detail (\"Did the student provide enough information about their issue?\") and giving instructors the ability to set guidelines the AI enforces</li> <li>Automatically tagging or categorizing tickets for TAs to understand what is going on in the queue and improved data analysis</li> <li>Generating reports for instructors / TAs that summarizing common issues over a specified timespan to better anticipate and address frequent problems</li> </ul> <p>What other AI-driven improvements could help streamline office hours?</p>"},{"location":"resources/exercises/sp00-cfp/#3-ai-generated-practice-questions-and-study-buddies","title":"3. AI-Generated Practice Questions and Study Buddies","text":"<p>Create an AI-powered study helper that:</p> <ul> <li>Generates practice questions tailored to specific course topics and learning objectives defined by instructors or TAs as a part of their CSXL course site</li> <li>Allows students to rate or TAs to pre-screen questions, ensuring quality and relevance</li> <li>Tracks which questions students have seen or struggled with, ensuring variety and personalized learning experiences</li> </ul> <p>How else could AI facilitate collaborative learning or individual study?</p>"},{"location":"resources/exercises/sp00-cfp/#4-your-own-creative-ai-driven-feature","title":"4. Your Own Creative AI-Driven Feature","text":"<p>Don't feel limited! If you have an innovative idea on how AI could make the CSXL web app more useful, engaging, or intelligent, we encourage you to propose it. Feel free to think big and be creative, this is your chance to shape the future of the CSXL web application!</p>"},{"location":"resources/exercises/sp00-epic-stories/","title":"Sprint 0 - Week 0 - An Epic Tale of Short Stories","text":""},{"location":"resources/exercises/sp00-epic-stories/#agile-team-kickoff-feature-design-document","title":"Agile Team Kickoff: Feature Design Document","text":"<p>Your team will collaborate to create an initial design document outlining the epic of your feature. This task will guide your team through the planning of first Agile sprint and help you clearly define your project's scope, purpose, and value. Begin by creating a shared Google Doc for initial brainstorming and drafting. Ultimately, this document will be converted into a markdown file (<code>feature-design.md</code>) in your team's repository under the <code>docs</code> directory.</p> <p>For features, please see our Call for Projects this Spring 2026. We are integrating AI functionality into the CSXL web application! All features must utilize the OpenAI LLM API in some way that adds intelligence to . <p>Important Note on AI: Generative AI tools (e.g., ChatGPT, Bard) are strictly prohibited in crafting your design document. This exercise focuses on teamwork, critical thinking, communication, and a thorough understanding of the problem you aim to solve. Detection of generative AI writing in submissions risks a team score of 0% for this sprint.</p>"},{"location":"resources/exercises/sp00-epic-stories/#required-sections-of-your-document","title":"Required Sections of Your Document","text":""},{"location":"resources/exercises/sp00-epic-stories/#1-title-team","title":"1. Title &amp; Team","text":"<ul> <li>Create a descriptive, engaging title for your feature. You are encouraged to go beyond the original RFP title.</li> <li>Clearly list all team members with their full names and links to their GitHub repositories.</li> </ul>"},{"location":"resources/exercises/sp00-epic-stories/#2-overview","title":"2. Overview","text":"<ul> <li>Clearly restate the purpose, value, and context of your chosen feature.</li> <li>Include your own interpretation of the problem and explain the value your solution will provide to users.</li> <li>Keep this brief and impactful (1-2 concise paragraphs).</li> </ul>"},{"location":"resources/exercises/sp00-epic-stories/#3-key-personas","title":"3. Key Personas","text":"<ul> <li>Identify and briefly describe the 2-4 primary personas your feature targets.</li> <li>For each persona, clearly articulate:<ul> <li>Role: Who are they, generally?</li> <li>Goals: What do they aim to achieve using your feature?</li> <li>Pain Points: What specific problems does your feature help resolve for them?</li> </ul> </li> </ul>"},{"location":"resources/exercises/sp00-epic-stories/#4-user-stories","title":"4. User Stories","text":"<ul> <li>Write user stories following this template:</li> </ul> <pre><code>As a [persona], I want [to perform some action] \nso that [I can achieve some goal or benefit].\n\nAcceptance Criteria:\n- Clear, testable criteria describing what completing this story looks like.\n- Criteria 2\n- Criteria 3 (as necessary)\n</code></pre> <ul> <li>Organize user stories by persona and prioritize them by:<ul> <li>Necessity (Essential for minimum viable product? Nice-to-have?)</li> <li>Frequency of use (Daily, weekly, occasional?)</li> <li>Importance to user value</li> </ul> </li> <li>Clearly label and separate your stories by persona for clarity.</li> </ul>"},{"location":"resources/exercises/sp00-epic-stories/#5-wireframes-mockups-sample-interactions","title":"5. Wireframes / Mockups / Sample Interactions","text":"<ul> <li>For your top 3 critical user stories, create clear wireframes illustrating each step of the story.<ul> <li>If your feature involves content managed by instructors, CSXL staff, career services, etc., then one of your top stories should also include what the process looks like for this persona to add and edit this content.</li> </ul> </li> <li>Use Figma to produce mid-fidelity wireframes/prototypes to ensure consistency and a professional appearance. Export your Figma frames and add them to your epic design document.</li> <li>Include a brief but clear explanation beneath each wireframe:<ul> <li>What interaction is happening?</li> <li>What action(s) are users performing?</li> <li>What is the outcome or expected result?</li> </ul> </li> <li>For interactions where there is natural language or integration with the large-language model, provide two to threee different samples of what the user might prompt with and a reasonable response to expect (either language or UI) from the system.</li> </ul> <p>Collaboration Advice</p> <ul> <li>Discuss regularly as a team. Consensus early on saves rework later.</li> <li>Use simple language to clearly express ideas.</li> <li>Get frequent feedback from each other\u2014be direct, constructive, and supportive.</li> </ul> <p>Submission Requirements</p> <p>Your final document must:</p> <ul> <li>Be written in your own words\u2014no AI-generated content.</li> <li>Clearly demonstrate a thoughtful understanding of user needs, product vision, and realistic implementation.</li> <li>Be neatly organized, professionally formatted, and submitted as a PDF.</li> </ul>"},{"location":"resources/exercises/sp01-one-story-db/","title":"Sprint 1 - Week 1 - One Story End-to-End with DB","text":"<p>The goal of this sprint, which began last week, is to have the most valuable story of your project demonstratable end-to-end from front-end down to the database and OpenAI AI integration. On Monday, April 14th, your team will demo the story running on CloudApps.</p> <p>Requirements:</p> <ul> <li>Integration with OpenAI API</li> <li>Persistence in DB</li> <li>Project Management Standards</li> <li>Running on CloudApps</li> </ul>"},{"location":"resources/exercises/sp01-one-story-db/#integration-with-the-openai-api","title":"Integration with the OpenAI API","text":"<p>On Friday, April 4th, API keys for all teams were handed out. Each member has an individual API key for use in their own development environment which is tracked for appropriate, class-related purposes. Additionally, we showed the strategy and some helper functions for integrating with the API in your projects. See the recording for a more fullsome description, but in general here are some useful links:</p> <ul> <li>Backend OpenAI Service Helper</li> <li>Backend Service Using OpenAI Service</li> <li>Backend Model for OpenAI Response Example</li> <li>Backend API Route using the Service</li> </ul> <p>The path toward adding this sample code to your project shown in class involved adding the primary CSXL repository as a remote named <code>upstream</code> and using the <code>cherry-pick</code> subcommand to pull in the commit which added the above files. If you'd prefer to add these files manually to a branch, that's OK, too. The strategy shown in class was:</p> <pre><code>git remote add upstream https://github.com/unc-csxl/csxl.unc.edu.git\ngit remote fetch upstream\ngit switch stage\ngit switch -c add-openai-code\ngit cherry-pick 3c0822e\ngit push origin add-openai-code\n</code></pre> <p>Additionally, in your <code>backend/.env</code> file, you need to add an environment variable named <code>UNC_OPENAI_API_KEY</code> which is set to your personal key (no spaces) as handed out in class. This is a secret and should not be committed to your team's repository!</p> <p>Then, create a PR which targets your <code>stage</code> branch for integration and have a team member review and merge.</p> <p>Notably: You need to rebuild your devcontainer via the command palette in VSCode so that you get the updated <code>backend/requirements.txt</code> dependencies which includes <code>openai</code></p> <p>Requirement: If your Sprint 1 end-to-end story integrates with the AI; great! You should be able to show your own backend service that injects the <code>OpenAIService</code> and makes use of its <code>prompt</code> method with a Pydantic <code>response_model</code> parameter type of your team's design. If your Sprint 1 story does not have the AI integration in the user experience path, you should go ahead and write an additional backend API route and respective service that will integrate with the AI API and can be demonstrated via the <code>/docs</code> interface.</p>"},{"location":"resources/exercises/sp01-one-story-db/#persistence-in-db","title":"Persistence in DB","text":"<p>Your story should involve persisting new information to the database in some way; specifically, and technically, this means either augmenting an existing <code>Entity</code> or introducing one or more of your own <code>sqlalchemy</code> <code>Entity</code> classes.</p> <p>If your story (or feature!) does not necessarily directly involve new data in the database, then for this requirement, your team should introduce some sort of \"AI Audit Log\" entity that persists the user prompt and JSON-encoded text response from the AI. You do not need to surface this to an API/UI for this Sprint, but the successful recording of data should be visible via the Postgres Explorer plugin shown in class on Monday, April, 7th.</p> <p>Database persistence integration should only occur in the backend services layer, not directly from the HTTP API layer.</p> <p>If your demo / feature depends on data being prepopulated in your database, then you should add it such that running <code>python3 -m backend.script.reset_demo</code> repopulates your database correctly for demo purposes. When TAs are grading, they will run this script on your</p>"},{"location":"resources/exercises/sp01-one-story-db/#project-management-standards","title":"Project Management Standards","text":"<p>Continue utilizing the expected tools and workflow of the course:</p> <ol> <li>Maintain your Project Board with cards linked to issues, assigned to team member(s), with descriptive titles for all cards/issues</li> <li>Perform work on branches off of <code>stage</code></li> <li>Perform pull requests with well written titles and messages and request code reviews from team members</li> <li>Make effortful and helpful code reviews for your team mates, helpfully maintaining high standards of code</li> <li>Squash and merge approved PRs into <code>stage</code></li> </ol>"},{"location":"resources/exercises/sp01-one-story-db/#running-on-cloudapps","title":"Running on CloudApps","text":"<ul> <li>Instructions for CloudApps deployment will post on Wednesday, April 9th.</li> </ul>"},{"location":"resources/exercises/sp01-one-story/","title":"Sprint 1 - Week 1 - One Story End-to-End","text":"<p>The goal of this sprint is to have your primary story working end-to-end in the CSXL code base. Additionally, your team will get into the flow of performing Pull Requests and Code Reviews for one another and keeping your project board updated to reflect the status of ongoing work.</p> <p>In the first week of this Sprint, your feature's backend services layer is permitted, and generally expected, to fake responses from the database and AI integration.</p>"},{"location":"resources/exercises/sp01-one-story/#expectations","title":"Expectations","text":"<ul> <li>One story end-to-end</li> <li>Project Management Standards</li> <li>PR/CR Workflow Enforced from Stage</li> </ul>"},{"location":"resources/exercises/sp01-one-story/#your-primary-story","title":"Your Primary Story","text":"<p>Focus on your team's most valuable user story that incorporates an AI integration. Your goal is to have this working and interactive from the front-end all the way to the back-end. In this first week, your focus is orienting yourself with existing code and finding where your team's work will fit into it. For now, your aim is to implement from the front-end down to the back-end services, but those back-end services will be faked. Next week, we can worry about the AI integration and data persistence concerns. Since this is a large code base, you can likely find examples of most everything you are trying to accomplish by looking around at other areas of the application and seeing how they were achieved there.</p>"},{"location":"resources/exercises/sp01-one-story/#back-end-api-and-data-models","title":"Back-end API and Data Models","text":"<p>Your team should start with establishing your back-end Pydantic models and routes. The main entrypoint of the backend API is <code>backend/main.py</code>. You will see it imports its routes from FastAPI router files in the <code>backend/api</code> directory.</p> <p>For now, we recommend establishing your team's own separate router file in an appropriate <code>backend/api</code> subdirectory (e.g. room/desk reservation logic is in <code>coworking</code>, courses is in <code>academics</code>, office hours queue is in <code>office_hours</code>). Start with a single test route to be sure you can get it working and showing up in the <code>localhost:1560/docs</code> OpenAPI interface. After defining a router and adding a route to it, be sure to register it in <code>backend/main.py</code> as a <code>feature_api</code>. Before continuing further, be sure you see your route appear in <code>/docs</code>.</p> <p>Helpful hints:</p> <ul> <li>To run the development CSXL server, from the DevContainer, use <code>honcho start</code> and navigate to <code>localhost:1560</code></li> <li>When in doubt, reset your database following the steps in <code>docs/database.md</code></li> <li>Be sure to name your <code>APIRouter</code> instance in your backend router module <code>api</code></li> </ul> <p>Once you have a \"hello world\" route that you can successfully use from <code>/docs</code>, you are ready to start fully defining the routes you will need for this story. For now, we recommend focusing only on the routes you need for your initial story and no more. These routes will also need Pydantic models, or changes to existing models. You should make those changes in the <code>backend/models</code> directory. New models should be added to new files whose file structure is informed by where you chose to define your routes. If you need to \"modify\" an existing model, we recommend the approach you take for now is to use inheritance to define a new model just for your feature which extends the existing model and adds any additional fields needed. For an example of inheritance, see <code>backend/models/user_details.py</code> where <code>UserDetails</code> extends <code>User</code> and adds some additional fields to <code>User</code>.</p> <p>The FastAPI routes you define and need for this story should follow the conventions we learned about annotating route parameters this semester. The conventions we have learned are newer (and better!) than the more dated style you are seeing in the CSXL code base. (We hope to update them this summer!) You should also include documentation for your route definitions like we expected during the FastAPI exercise earlier this semester.</p>"},{"location":"resources/exercises/sp01-one-story/#back-end-services","title":"Back-end Services","text":"<p>Your routes should handle HTTP-level concerns, but ultimately should delegate the business logic to a service with necessary inputs coming from the request. You should define a new service similar to how other services are implemented in <code>backend/services</code>, also appropriately organized in the file structure, with the service methods your routes will depend on. For now, you can fake return values from these service methods in order to make progress on this project with your team. In doing so, the front-end work will be able to progress independently of the back-end and the API contract will be the shared agreement between layers of the stack.</p>"},{"location":"resources/exercises/sp01-one-story/#front-end","title":"Front-end","text":"<p>How your team's front-end is organized will be highly dependent upon your feature and story. You should find your way to the components and widgets you are likely to integrate with. If you need an entirely new front-end route, take a look at how other features work in the codebase and find relevant examples to work off of. You should utilize Material UI widgets in your feature. If you're looking for how to utilize a specific widget, look for relevant examples.</p> <p>While there may be a tendency to reach for GPT or Co-pilot on the front-end, please note it's likely to create a bigger headache and mess than you think if you lack confidence in the front-end. You are much better off trying to make slow, steady progress pair programming and having a sense of everything you are changing.</p> <p>If you arrive in office hours with code for your feature which you cannot explain, the TAs are instructed to help you revert back to <code>stage</code> and ask you to go work on it more intently, to try again.</p>"},{"location":"resources/exercises/sp01-one-story/#team-project-management","title":"Team Project Management","text":"<p>Continue utilizing the expected tools and workflow of the course:</p> <ol> <li>Maintain your Project Board with cards linked to issues, assigned to team member(s), with descriptive titles for all cards/issues</li> <li>Perform work on branches off of <code>stage</code></li> <li>Perform pull requests with well written titles and messages and request code reviews from team members</li> <li>Make effortful and helpful code reviews for your team mates, helpfully maintaining high standards of code</li> <li>Squash and merge approved PRs into <code>stage</code></li> </ol>"},{"location":"resources/exercises/sp01-one-story/#prcr-settings","title":"PR/CR Settings","text":"<p>Let's setup your GitHub repository so that your team is able to work from a branch named <code>stage</code> as your primary branch. We will reserve the <code>main</code> brach to reflect production's <code>main</code> branch.</p> <p>One member of your team should create a branch in the project named <code>stage</code> and push it to your team's repository. Other members of the team should <code>fetch</code> and switch to <code>stage</code>.</p> <p>A member of the team should setup the branch protection rules for <code>main</code> and <code>stage</code> in your team repository. In your team's final project repository, navigate to:</p> <ol> <li>Settings</li> <li>General &gt; Default Branch &gt; Change Branch to <code>stage</code><ul> <li>Press the Swap Button (not the Pencil!) and select <code>stage</code></li> <li>If you do not see <code>stage</code>, be sure you completed all the steps in \"Initialize Team Repository\", then refresh this page and try again.</li> <li>Press Update and accept the change</li> </ul> </li> <li>Change to the Branches tab in the sidebar<ol> <li>Add Branch Ruleset</li> <li>Ruleset Name: <code>main</code></li> <li>Enforcement status: <code>Active</code></li> <li>Targets, Add Target, Include by Pattern: <code>main</code></li> <li>(Check) Restrict creations</li> <li>(Check) Restrict updates</li> <li>(Check) Block force pushes</li> <li>Save Changes with <code>Create</code> Button</li> </ol> </li> <li>Add another Ruleset (Go back to Rulesets tab)<ol> <li>New Branch Ruleset</li> <li>Ruleset name: <code>stage</code></li> <li>Enforcement status: <code>Active</code></li> <li>Targets, Add Target, select Include default branch</li> <li>(Check) Restrict deletions</li> <li>(Check) Require linear history</li> <li>(Check) Require a pull request before merging<ol> <li>(Check) Require approvals: 1 required</li> <li>(Check) Dismiss stale pull request approvals when new commits are pushed</li> <li>(Check) Require approval of the most recent reviewable push</li> <li>(Check) Require conversation resolution before merging</li> </ol> </li> <li>Save Changes with <code>Create</code></li> </ol> </li> </ol> <p>Your team repository now protects <code>main</code> from modifications and requires Pull Requests and Code Reviews on <code>stage</code>. This workflow is representative of many industrial workflow settings.</p>"},{"location":"resources/exercises/sp02-deployment/","title":"Staging Server Environment (DevOps)","text":"<p>Your team should setup the staging environment on one team member's namespace. Choose one members' OKD CloudApps course namespace to setup the production staging environment. The other team member(s) will be added to this namespace as collaborator(s) so everyone has access.</p> <p>In establishing the staging environment on our cloud infrastructure, we will work from the bottom up. We will start with the database server, then add secrets needed to build and run the application, then add the application, and finally add the route to expose the application to the internet.</p>"},{"location":"resources/exercises/sp02-deployment/#accessing-cloud-infrastructure","title":"Accessing Cloud Infrastructure","text":"<p>Warning: If you are not on-campus when working with the cloud infrastructure, it will have the appearance of being \"down\". This is due to a firewall preventing off-campus access without a virtual private network VPN connection. If you try to load Cloud Apps at any point this semester and see a non-responsive or blank web page, it is likely because you are not connected via Eduroam nor the VPN.</p> <p>If you are off-campus, you will need to establish a VPN connection in to make use of Carolina CloudApps: https://ccinfo.unc.edu/start-here/secure-access-on-and-off-campus/</p> <p>Access the OKD CloudApps console here: https://console.apps.unc.edu/</p> <p>Under the okd logo, you should see \"Developer\" in a drop down and to the right \"Project: \" followed by <code>comp590-140-26sp-ONYEN</code> where ONYEN is your UNC ONYEN.</p>"},{"location":"resources/exercises/sp02-deployment/#using-the-oc-command-line-tool-to-administer-okd","title":"Using the <code>oc</code> Command-line Tool to Administer OKD","text":"<p>The <code>oc</code> tool is included in the <code>csxl</code> developer container. As long as you are successfully running Docker locally and connected to a campus network or VPN, you will be able to use <code>oc</code> from your container. However, if you are making use of CodeSpaces, you will need to install <code>oc</code> onto your host machine so that you can access the OKD cluster from VPN/campus rather than the cloud container.</p>"},{"location":"resources/exercises/sp02-deployment/#logging-into-the-okd-container","title":"Logging into the OKD Container","text":"<p>Our course projects will continue to be hosted on the OKD cluster of CloudApps, found here: https://console.apps.unc.edu/</p> <p>Go ahead and log-in. Remember: off-campus access requires VPN as described in the above section!</p> <p>Next, you will need to log-in to OKD from the Command-Line Utility in your DevContainer. The login command is found in the OKD Console. Look in the top right for your name, click the drop down, and select \"Copy Login Command\". From here select \"Display Token\". Here, look for the line \"Log in with this token\" and copy the complete command beginning with <code>oc login</code> to your clipboard.</p> <p>Paste this command into a terminal in your DevContainer (or, if you are on CloudSpaces, your host machine's terminal). You should be correctly logged in and see a message of success.</p>"},{"location":"resources/exercises/sp02-deployment/#giving-team-members-access-to-your-project","title":"Giving Team Members Access to Your Project","text":"<p>One member of each team should be designated the project host. From this member's OKD CloudApps account, you will need to add other team members as follows:</p> <ol> <li>Add team members to your course workspace</li> <li>Navigate to the Administrator sidebar (Default is Developer)</li> <li>Select: User Management &gt; Role Bindings</li> <li>For each team member, with their <code>onyen</code>:<ol> <li>Create binding</li> <li>Name: <code>admin-ONYEN</code> (replace <code>ONYEN</code> with teammate's ONYEN)</li> <li>Be sure the project you are in in CloudApps is <code>comp590-140-26sp-ONYEN</code></li> <li><code>ONYEN</code> should be your UNC ONYEN</li> <li>Role name: <code>admin</code></li> <li>Subject:</li> <li>User</li> <li>Name: your teammate's ONYEN</li> </ol> </li> </ol> <p>Add all of your team members and have them confirm that they have access.</p>"},{"location":"resources/exercises/sp02-deployment/#creating-the-database-server","title":"Creating the Database Server","text":"<p>The first step in establishing the cloud deployment is to establish the backend database pod.</p> <ol> <li>Add a PostgreSQL database to your project:<ol> <li>Developer View Sidebar</li> <li>Add (from the Sidebar)</li> <li>Database</li> <li>PostgreSQL Provided by Red Hat</li> <li>Instantiate Template</li> <li>Change only the following settings:<ol> <li>Database Service Name: <code>db</code></li> <li>PostgreSQL Database Name: <code>csxl</code></li> <li>Version of PostgreSQL Image: <code>latest</code></li> </ol> </li> <li>Create</li> <li>Navigate to the secrets page as described in the next paragraph</li> </ol> </li> </ol> <p>Once the database is created, go to the Secrets page found in the left-hand sidebar and view the generated credentials for the database under <code>db</code> (this is the name we gave it above). If you select \"Reveal Values\" you can see the name, username, and password for the database. These secrets will be used as environment variables in your application in the next step.</p>"},{"location":"resources/exercises/sp02-deployment/#creating-secrets-for-your-application","title":"Creating Secrets for your Application","text":"<p>Let's create a secret for your application to use. This will be used to store the database credentials, and will ultimately be mounted as environment variables in your application.</p> <p>Leave open the tab with these secrets, which you navigated to in Step 8 above. Additionally, you will need to generate a random string for the <code>JWT_SECRET</code> environment variable. This will be used to sign the JWT tokens that your application will use to authenticate users. You can generate a random string using the following command in your Dev Container:</p> <pre><code>openssl rand -hex 32\n</code></pre> <p>From your DevContainer's terminal, in a shell prompt run the following command to create the secret on OpenShift BE CAREFUL TO AVOID TYPOS.</p> <p>You can copy this command and edit it, replacing the placeholders, in an empty text file before running the command in your DevContainer terminal.</p> <pre><code>oc create secret generic final-project-environment \\\n    --from-literal=POSTGRES_HOST=db \\\n    --from-literal=POSTGRES_PORT=5432 \\\n    --from-literal=POSTGRES_DATABASE=csxl \\\n    --from-literal=POSTGRES_USER=&lt;from-secret-above&gt; \\\n    --from-literal=POSTGRES_PASSWORD=&lt;from-secret-above&gt; \\\n    --from-literal=JWT_SECRET=&lt;generate-random-string&gt; \\\n    --from-literal=UNC_OPENAI_API_KEY=&lt;your-teams-deployment-api-key&gt;\n</code></pre> <p>For the final secret, be sure to use the team's UNC OpenAI API key you were provided on the print out. It's the last key on the sheet. If this is not accessible, you can also use your own deployment key.</p> <p>Changing Secrets Later</p> <p>If you realize you need to change a secret, you can do so via the OKD web console by navigating to</p> <p>Developer &gt; Secrets &gt; <code>final-project-environment</code> &gt; Actions &gt; Edit Secret.</p> <p>From the OpenShift web console, you can verify that the secret was created by navigating to the Secrets page and selecting the <code>final-project-environment</code> secret.</p>"},{"location":"resources/exercises/sp02-deployment/#deploy-key-secrets","title":"Deploy Key Secrets","text":"<p>Before OpenShift's builder process is able to clone your repository from GitHub, we need to establish a means for OpenShift to authenticate itself to gain access to your team's private repository. This will closely resemble how you authenticate yourself with GitHub, but with the key difference it's the builder process running on Carolina Cloud app's machines-- not yours!-- that needs to gain access to your GitHub repository.</p> <p>It is worth pausing to reflect on how sensitive this step is in real world applications: you are setting up a means to directly access your code in a private repository. At organizations you may find yourself employed by, or founding, the code in your private repositories are some of it's most valuable assets and their secure handling is very important to maintaining their secrecy and protecting customers from hacks.</p> <p>It is for these reasons that you want to be very careful to never commit secrets to a <code>git</code> repository. The keys we are about to setup are considered secrets! (If you find yourself at an organization that breaks this rule: run.)</p> <p>To avoid accidentally commiting secrets to a project, one strategy is to be sure the filenames containing the secrets are added to the project's <code>.gitignore</code>. This file lists patterns that will not be included by default. Go ahead confirm the following rule is in the project's <code>.gitignore</code> file:</p> <p><code>deploy_key*</code></p> <p>Go ahead and make a commit with this change to <code>.gitignore</code> included in the commit.</p> <p>DEVCONTAINER: In your DevContainer's terminal, not your host's, generate an SSH key:</p> <pre><code>ssh-keygen -t ed25519 -C \"GitHub Deploy Key\" -f ./deploy_key\n</code></pre> <p>Note: Do NOT set a passphrase for the ssh key, just press enter at the prompt without typing anything when asked.</p> <p>You should now see the files <code>deploy_key</code>, which is the private/secret key, and <code>deploy_key.pub</code> which is the public key.</p> <p>Add the public key to your project repository's settings on GitHub:</p> <ol> <li>Navigate to your repository's settings</li> <li>Select Deploy Keys</li> <li>Add Deploy Key</li> <li>Title: <code>CloudApps Deploy Key</code></li> <li>Key: Copy the contents of <code>deploy_key.pub</code> into the key field</li> <li>Check the box to allow write access</li> <li>Click Add Key</li> </ol> <p>Now run the following command to add the private key as a secret to your Cloud Apps account:</p> <pre><code>$ oc create secret generic comp590-final-project-deploykey \\\n    --from-file=ssh-privatekey=./deploy_key \\\n    --type=kubernetes.io/ssh-auth\n</code></pre> <p>To verify the secret was correctly created, run the following command:</p> <pre><code>oc get secret comp590-final-project-deploykey\n</code></pre> <p>Finally, you need to link the secret to the \"builder\" process of OpenShift. This will allow OpenShift to use the secret when it pulls your code from GitHub and builds your project.</p> <pre><code>oc secrets link builder comp590-final-project-deploykey\n</code></pre> <p>This command will succeed silently.</p>"},{"location":"resources/exercises/sp02-deployment/#create-the-openshift-application","title":"Create the OpenShift Application","text":"<p>IMPORTANT: Be sure you substitute your team's information in TWO places below! First: the repository URL, second the HOST variable should not be <code>csxl-team-XX</code>, but should instead be your team zone + number. This is your team's table. Using lowercase is encouraged.</p> <p>IMPORTANT: Be sure you are currently on your stage branch. If you are not, go ahead and stash and/or commit changes on your current branch, and switch to stage.</p> <pre><code>oc new-app python:3.11~git@github.com:comp423-26s/&lt;your-final_repo_name&gt;.git#stage \\\n  --source-secret=comp590-final-project-deploykey \\\n  --name=final-project \\\n  --strategy=docker \\\n  --env=MODE=development \\\n  --env=HOST=csxl-team-&lt;TEAM NUMBER&gt;-comp423-26s.apps.unc.edu\n</code></pre> <p>Notice the <code>#stage</code> at the end of the repository URL. This is the branch name that OpenShift will pull from. When setting up the final project, you created a branch named <code>stage</code> and established it as the primary branch for your repository. This notion of a staging branch is a common practice in DevOps, and is a good way to keep your production code separate (live at csxl.unc.edu) from your development code (which you are establishing right now).</p> <p>While the project is building, link the secrets you created as the environment variables of the deployment and verify their existence with <code>list</code>:</p> <pre><code>oc set env deployment/final-project --from=secret/final-project-environment\noc set env deployment/final-project --list\n</code></pre>"},{"location":"resources/exercises/sp02-deployment/#exposing-the-application","title":"Exposing the Application","text":"<p>Once your application builds, it will be running on a pod that is not exposed to the internet. To establish a public route, first we need to expose it as a service, run the following command:</p> <pre><code>oc expose deployment final-project \\\n  --port=80 \\\n  --target-port=8080\n</code></pre> <p>Next, we can create a route to the service with a specifically chosen hostname. Please replace <code>XX</code> with your team's number  and table number.</p> <pre><code>oc create route edge \\\n  --service=final-project \\\n  --hostname=csxl-team-&lt;TEAM NUMBER&gt;-comp423-26s.apps.unc.edu\n</code></pre> <p>You can now visit the hostname for your team and access it in the browser. If you see a message from OpenShift that says \"Application is not available\", it means that the application is still building. Once your build completes, you should see the application running, but there is still one more important step: resetting the database.</p> <p>Crash-loop Back-off and 'OOM Killed' (Out of Memory)</p> <p>If your pod keeps crashing, with a message like \"Killed by OOM Manager\", it's because the Python/FastAPI server process requires more memory than your deployment is configured to provide by default. Our deployment platform, Kubernetes/OKD, monitors resource usage so that its resources are shared fairly among us. It takes a very conservative default, which can lead to your process being crashed when it needs more memory than the default. To ask for more memory, but still a modest amount for 2025 standards, take the following steps in the <code>oc</code> tool: </p> <pre><code>oc set resources deployment/final-project --requests=memory=256Mi --limits=memory=1Gi\noc rollout restart deployment/final-project\n</code></pre> <p>The first command requests a higher memory limit for the deployment and the second restarts the pod in the deployment so that it uses the new settings.</p>"},{"location":"resources/exercises/sp02-deployment/#resetting-the-database","title":"Resetting the Database","text":"<p>The database that you created in the previous step is empty. You will need to reset the database to the state that it was in when you submitted your final project. To do this, you will need to run the <code>reset_demo.py</code> script that is included in your final project repository.</p> <p>This script needs to be run from within your pod, so in this section you will learn how to connect to your pod and run commands from within it.</p> <p>First, you will need to find the name of your pod. Run the following command to get a list of pods running in your project:</p> <pre><code>oc get pods --selector deployment=final-project\n</code></pre> <p>You should see a single pod with a name like <code>final-project-648fdff8d5-rr4fs</code>. The letters and numbers at the end of the name are a unique identifier for the pod. This identifier changes every time a new build of your pod is deployed, environment variables change, the pod gets restarted, and in other instances. Copy the name of your running pod and run the following command to connect to it:</p> <pre><code>oc rsh final-project-YOUR-POD-IDENTIFIER\n</code></pre> <p>The <code>rsh</code> stands for \"remote shell\".  You are now connected to your pod running in the cloud via a secure shell (ssh)! The commands you run are not running on your host machine, but on the CloudApps infrastructure. If you <code>ls</code> you will see you are in your project's built directory. Not everything is there, importantly not the frontend because it was compiled into the <code>static</code> directory as part of the build process.</p> <p>To confirm you are logged into your pod, you can assure yourself with the following command:</p> <pre><code>hostname\n</code></pre> <p>You can now run the <code>reset_demo</code> script to reset the database. Run the following command to do so:</p> <pre><code>python3 -m backend.script.reset_demo\n</code></pre> <p>You should see the SQLAlchemy log messages creating tables, inserting dev data, etc. Your staging database is now reset!</p> <p>Important: As you deploy new versions, add new entities, add new dev data, etc., this process of resetting the database in staging is one you and your team members will both need to be comfortable doing and remember to do.</p>"},{"location":"resources/exercises/sp02-deployment/#setting-up-push-to-deploy-webhooks","title":"Setting up Push-to-Deploy Webhooks","text":"<p>GitHub repositories can be configured with webhooks, which are URLs that get called when events occur in order to notify another service of the event. In our case, we want to set up a webhook so that when we merge pull requests to the <code>stage</code> branch, OpenShift's build configuration for our project will receive a webhook notification and kick off a new build and deploy pipeline.</p> <p>To find the URL for the web hook, open up your project in OpenShift and navigate to the Admin sidebar, followed by Builds &gt; BuildConfigs. Select <code>final-project</code> and look for the webhooks section at the bottom. Click the Copy URL with Secrets button for the GitHub webhook. This copies the URL to your clipboard.</p> <p>Next, open your project's settings in GitHub and navigate to Webhooks. Click Add Webhook and paste the URL into the Payload URL field. Be sure to set the content type to <code>application/json</code> and leave the secret field empty. Click Add Webhook.</p> <p>From the \"Webhooks\" page you're brought back to, click your webhook. Then go to the Recent Deliveries tab. You should see a successful delivery. Congratulations, your project is now set up to automatically build and deploy every time your team merges PRs into the <code>stage</code> branch! As a reminder, if your data entities change, you will need to reset the database in staging after the build and deploy completes.</p>"},{"location":"resources/exercises/sp02-deployment/#youre-in-stagingproduction","title":"You're in Staging/Production!","text":"<p>This setup mirrors our production setup of <code>csxl.unc.edu</code> and is running the same code base. Congratulations on setting up what is, in essence, a production cloud environment for a small, modern web application!</p> <p>We call this \"Staging\" in a nod to how many organizations think of \"Staging\" environments. It's an area where your team can work on a feature, see it deployed publicly on the internet, and test it without having any impact on our production deployment.</p>"},{"location":"resources/exercises/sp02-second-story/","title":"Sprint 2","text":""},{"location":"resources/exercises/sp02-second-story/#sprint-2-expectations","title":"Sprint 2 Expectations","text":"<p>This sprint is about arriving at a well implemented, tested, and thoughtful production-quality feature.</p>"},{"location":"resources/exercises/sp02-second-story/#expectation-0-complete-2nd-end-user-story-end-to-end","title":"Expectation 0: Complete 2nd End-user Story End-to-End","text":"<p>In addition to the first story from SP01, we expect a 2nd end-user story working end-to-end. This story may not depend upon the AI integration (and likely will not!). If your feature significantly involves two different personas, try choosing the most important story from your second persona.</p>"},{"location":"resources/exercises/sp02-second-story/#expectation-1-polish-for-end-user-stories","title":"Expectation 1: Polish for End-user Stories","text":"<p>The interactions designed for your primary persona should be smooth and polished. This includes user-friendly interactions and form design, friendly error messages (or, even better, designing away the ability for there to even be errors!), and thoughtful user experience considerations such as clear user instructions and being sure everything visible works. If there are features you added user interface elements for that are not yet implemented, you should remove them by the end of this sprint. Everything visible should be functional.</p> <p>Additionally, polish should be added to the implementation wherever possible. You are encouraged to move through each story of your feature from end-to-end, including docs and tests, and improve your implementation wherever possible.</p>"},{"location":"resources/exercises/sp02-second-story/#expectation-2-document-your-implementation-for-future-developers","title":"Expectation 2: Document Your Implementation for Future Developers","text":"<p>Your initial design document from SP00 set you out in a direction to head with respect to design and routes. It is highly likely that while your team moved in that direction, the hopes and dreams of the design document were met with the realities of time constraints and technical challenges that required some iteration and deviation from the original plan. This is both typical and why overplanning without any implementation experimentation is rarely wise.</p> <p>For this expectation, draft a markdown document in the <code>docs/</code> directory of your project repository that is based on the realities of the feature work your team is doing and has implemented. It should be written for another developer to read and to understand how your feature is implemented. Structure it in such a way as to document how your feature works at each layer of the stack, from frontend to backend, database implications, and AI integration. Especially for the API, document the key routes and data models that exist in your code base. These should be visible and readable, fully formatted, on GitHub after you push your branches.</p> <p>You should avoid language such as, \"we would direct new developers to X and give them...\". Your documentation is written directly addressing a developer, not the course staff. If it helps, imagine you are writing for a future COMP423 student working to understand and extend your feature. Include screen grabs of the primary components and/or widgets of the frontend and look for other docs files on how to organize and include screenshots in your markdown.</p> <p>Choose plain language where possible.</p> <p>Ensure the formatting of your document is easy to read and understand. Make appropriate use of paragraph text, versus merely bulleted lists, where needed. Choose heading text that is appropriate for your feature and the audience. If information would better be represented with a table, use a table instead of a list. This list is not exhaustive. Use your best judgement to make your documentation a great artifact you can be proud of and share with future employers.</p> <p>Finally, include screenshots of your feature from the end user persona's perspective with narrative of what is being shown. To add an image to your project, place it in the <code>docs/images</code> directory.</p> <p>Include an authors section toward the top of the document listing the names of everyone in the group with links to their GitHub profiles.</p> <p>This document should be written by your four group members as first authors. LLMs usage is only appropriate for copyediting and feedback on how your documentation could be improved.</p>"},{"location":"resources/exercises/sp02-second-story/#expectation-3-project-management-standards","title":"Expectation 3: Project Management &amp; Standards","text":"<p>These remain the same as in the previous sprint.</p> <p>Issues are kept up-to-date on project boards and closed out when completed. Changes are merged into stage exclusively via pull requests with meaningful code reviews. Commits merged into stage are descriptive following best practices of commit messages.</p> <p>Angular Material components are used anywhere there are inputs, tables, tabs, etc. If there is an Angular Material component that achieves what your user interfaces need, you should use it in the frontend rather than standard, or bespoke, native HTML controls.</p> <p>Backend service classes should be tested using Pytest with mock data. Backend service classes and methods should be documented using docstrings following the Google Python Style Guide. Your final backend service code files must maintain 100% passing test coverage on stage. Your feature can and should mock the OpenAIService with a fixture similar to how similar services are mocked (See <code>backend/test/services/fixtures.py</code> for an example of how <code>permission_svc</code> is mocked, for example. You will see just after how <code>user_svc</code> is mocked to depend upon this. Then see <code>backend/test/services/user_service.py</code>'s <code>test_list_enforces_permission</code> for an example of how the mock is used.)</p> <p>Careful attention to permissions and access control should be paid to adhere to the principle of least privilege. Users should only be able to perform the necessary actions on the permitted resources for their legitimate purpose.</p> <p>Stories merged in to <code>stage</code> should be of usable, production quality.</p>"},{"location":"resources/exercises/sp02-second-story/#expectation-4-running-in-production-on-cloudapps","title":"Expectation 4. Running in Production on CloudApps","text":"<p>Find deployment instructions here.</p>"},{"location":"resources/exercises/sp02-second-story/#extra-credit-1-point-catch-your-stage-branch-up-with-csxluncedu-production","title":"Extra-Credit (1 point) - Catch Your <code>stage</code> Branch up with csxl.unc.edu Production","text":"<p>The TAs are not permitted to assist with this extra credit opportunity in or outside of Office Hours.</p> <p>Some PRs have landed in production at <code>csxl.unc.edu</code> since you began Sprint 2: https://github.com/unc-csxl/csxl.unc.edu/commits/main</p> <p>To earn this point of extra credit, you should catch your stage branch up to <code>upstream/main</code> such that you have commit <code>cef9639</code>, or later, in your <code>stage</code> branch. You may need to resolve conflicts. When creating a PR for this catch-up branch, rather than squashing and merging into your <code>stage</code> in this instance you should use the \"Create a Merge Commit\" strategy. This strategy will retain the history from production's main branch.</p> <p>This is good practice, and easier to achieve, if it's done semi-regularly. Additionally, this serves as a prerequisite to being considered for merging your team's features into production after the semester ends.</p>"},{"location":"resources/frontend/1-tools/","title":"JavaScript and the the Rise of the Web Client Platform","text":""},{"location":"resources/frontend/1-tools/#introduction-and-motivation","title":"Introduction and Motivation","text":"<p>Welcome to the front-end side of software engineering! Until now, you've gained experience with Python-based backends\u2014building REST APIs with FastAPI, configuring containerized environments, and even touching on production considerations like Kubernetes. You've seen how static type annotations can improve clarity in Python, and you've embraced unit testing to maintain software quality.</p> <p>Now, we\u2019re shifting toward web client applications. These are dynamic, interactive applications that run efficiently in the browser, providing smooth user experiences comparable to native apps. To build these, we need a strong foundation in modern JavaScript development tools and practices.</p>"},{"location":"resources/frontend/1-tools/#a-brief-history-of-javascript-and-the-web","title":"A Brief History of JavaScript and the Web","text":""},{"location":"resources/frontend/1-tools/#from-static-pages-to-dynamic-experiences","title":"From Static Pages to Dynamic Experiences","text":"<p>In the early days of the web (1990s), pages were built solely with hypertext markup language (HTML) for structure and cascading style sheets (CSS) for styling. Interactivity was minimal\u2014typically limited to simple tricks like swapping images on hover.</p> <p>Enter JavaScript in 1995. Created by Netscape, JavaScript allowed developers to write program scripts to add dynamic behavior to web pages, such as validating forms without requiring a full-page reload. It quickly became the standard client-side programming language since it was supported by all major browsers.</p>"},{"location":"resources/frontend/1-tools/#the-evolution-of-javascript-and-ecmascript","title":"The Evolution of JavaScript and ECMAScript","text":"<p>As web applications became more complex, JavaScript had to adapt. However, due to its rapid adoption and browser inconsistencies, the language needed standardization. This led to ECMAScript (ES), the official specification that defines JavaScript\u2019s behavior. JavaScript and ECMAScript are colloquially used interchangeably.</p> <p>Key milestones in JavaScript\u2019s evolution:</p> <ul> <li>ES5 (2009): Introduced JSON support, <code>strict mode</code>, and array methods like <code>map</code> and <code>forEach</code>.</li> <li>ES6 (2015): Major upgrade with modern syntax including <code>let</code> and <code>const</code>, arrow functions, template literals, classes, and modules. This release was a very big deal and significantly changed the experience of writing web client applications!</li> <li>ES7 and Beyond: Continuous yearly updates adding features like async/await, optional chaining, and improved performance optimizations.</li> </ul>"},{"location":"resources/frontend/1-tools/#key-characteristics-of-javascript","title":"Key Characteristics of JavaScript","text":"<p>JavaScript has several defining characteristics that influence how it operates:</p> <ul> <li>Interpreted: Unlike compiled languages like C++, JavaScript is executed line-by-line at runtime. This is akin to Python, which is also an interpreted language.</li> <li>Dynamically Typed: Variables do not have fixed types, allowing flexibility but also potential runtime errors. Again, like Python, this is more common in interpreted languages.</li> <li>Single-Threaded &amp; Event-Driven: JavaScript runs on a single execution thread but uses an event loop to handle asynchronous operations efficiently. You will learn more about this soon as it is an important runtime model to understand the nuances of for modern web development.</li> <li>Prototype-Based: Unlike traditional class-based languages, JavaScript is prototype-based, allowing objects to inherit directly from other objects. We will not dig into this and write TypeScript that feels more like the traditional OOP that you have learned thus far, but it's worth a mention that if you dig deeper there are differences here.</li> <li>Runs in the Browser &amp; Beyond: Originally designed for web browsers, JavaScript can now run on servers and desktops via environments like Node.js. We will use Node.js to write JavaScript/TypeScript programs that run at the commandline.</li> </ul> <p>As JavaScript grew, so did its complexity. Large applications needed better structure, maintainability, and developer tooling\u2014which led to the creation of TypeScript.</p>"},{"location":"resources/frontend/1-tools/#what-is-nodejs","title":"What is Node.js?","text":"<p>At its core, Node.js is a runtime environment that allows JavaScript to run outside of a web browser. When JavaScript was originally designed, it was intended to be executed only within browsers to make web pages interactive. However, as web applications became more complex, developers needed a way to run JavaScript on servers and in development tools. This is where Node.js comes in.</p> <p>Node.js is built on Chrome\u2019s V8 engine, which is the same high-performance JavaScript engine used in Google Chrome to process JavaScript code efficiently. By using this engine outside of the browser, Node.js enables JavaScript to be used for a variety of applications beyond just web pages. With Node.js, developers can build server-side applications, command-line tools, and development scripts, effectively making JavaScript a full-stack programming language.</p> <p>One of Node.js\u2019s major advantages is its non-blocking, event-driven architecture, which makes it well-suited for handling real-time applications and high-performance web services. Unlike traditional languages like Python and Java, which use a thread-based model where each task or request waits for the previous one to complete, Node.js operates differently. Instead of waiting for one operation to finish before moving on to the next, Node.js continues executing other tasks while waiting for asynchronous operations to complete.</p>"},{"location":"resources/frontend/1-tools/#understanding-non-blocking-event-driven-architecture","title":"Understanding Non-Blocking, Event-Driven Architecture","text":"<p>Consider a scenario in Python where you need to make an HTTP request to an external API and process its response. A typical synchronous implementation might look like this:</p> <pre><code>import requests\n\nresponse = requests.get('https://api.example.com/data')\ndata = response.json()\nprint(\"Data fetched successfully\")\nprint(data)\n</code></pre> <p>In this example, Python blocks execution while waiting for the HTTP request to complete. The program cannot continue to the next task until the request finishes.</p> <p>Now, let's compare this to a Node.js equivalent using an asynchronous approach:</p> <pre><code>const https = require('https');\n\nhttps.get('https://api.example.com/data', (res) =&gt; {\n    let data = '';\n    res.on('data', chunk =&gt; data += chunk);\n    res.on('end', () =&gt; {\n        console.log(\"Data fetched successfully\");\n        console.log(data);\n    });\n});\n\nconsole.log(\"Fetching data...\");\n</code></pre> <p>Here, instead of blocking execution, Node.js continues running the next line (<code>console.log(\"Fetching data...\");</code>) while the HTTP request happens in the background. Once the request completes and data is received, the provided callback function executes. This is what makes Node.js non-blocking\u2014it does not pause the execution of other tasks while waiting for an I/O operation to complete.</p> <p>We will spend significantly more time and effort digging into the implications and approaches to working with asynchronous, event-driven programming.</p>"},{"location":"resources/frontend/1-tools/#the-challenges-of-writing-large-scale-javascript-applications","title":"The Challenges of Writing Large-Scale JavaScript Applications","text":"<p>While JavaScript is a powerful and flexible language, it comes with several challenges when developing and maintaining large applications, particularly in team settings. Here are some key limitations:</p> <ol> <li>Lack of Static Typing: JavaScript is dynamically typed, meaning type errors can go undetected until runtime, leading to unpredictable behavior and increased debugging time.</li> <li>Poor Readability and Maintainability: Without enforced types and clear function signatures, large codebases become harder to read and maintain, especially for new team members.</li> <li>Inconsistent Code Quality: JavaScript allows multiple ways to achieve the same outcome, making it difficult to enforce consistent coding standards across a team. Just because you can do something in JavaScript in a shortcut way, doesn't mean an engineering team should and the lack of enforcement against these concerns leads to unruly codebases.</li> <li>Limited IDE Support and Refactoring Tools: JavaScript\u2019s flexibility limits the effectiveness of modern development tools like autocompletion and refactoring assistance.</li> </ol> <p>These limitations have led to the growing adoption of TypeScript, a superset of JavaScript that adds static typing and improved tooling. TypeScript is an open source programming language created by Microsoft's Developer Division, the same group that makes VSCode! In fact, VSCode is implemented in TypeScript! In the next section, we\u2019ll explore how TypeScript helps address these challenges and why it has become a preferred choice for scalable web development.</p>"},{"location":"resources/frontend/2-typescript/","title":"TypeScript For the COMP 301 Java Developer","text":"<p>Written by Ajay Gandecha and Kris Jordan for the CSXL Web Application and for COMP 423: Foundations of Software Engineering.</p>"},{"location":"resources/frontend/2-typescript/#preface","title":"Preface","text":"<p>During your career at UNC, you likely started out in COMP 110 learning the Python programming language. In COMP 210, you transitioned to Java and learned how to organize your code using classes and common data structures. In COMP 301, you built off of this idea and learned more about useful language features to help you gain a toolbox of design patterns and strategies for approaching software architecture challenges.</p> <p>Since this course's semester project is currently focused on web applications, you need to gain familiarity with the programming languages and tools needed to build high quality web apps. As you just read, JavaScript is the leading programming language that powers the web and web applications. According to the 2023 StackOverflow Developer Survey, developers ranked JavaScript as their most commonly-used programming language - an eleven-year long streak and counting!</p> <p>In this course, we will use TypeScript to build out the frontend of our web applications. TypeScript is a superset of JavaScript - it adds static typing with optional type annotations to JavaScript. It transpiles to JavaScript.</p> <p>Static typing is practiced in COMP110 with Python's modern type annotations, which were actually inpsired by TypeScript's success, and has its roots in industrial languages like Java and C. The word \"static\", in this context, refers to at development time which is, importantly, not at runtime. You can think of static as \"code at rest\" when it is in your editor or being analyzed by a compiler, not code that is actually running on a machine. Specifying static types allows the TypeScript IDE (e.g., VSCode) and compiler to verify that your code's expressions and statements are type safe. An example of a type safety check would be ensuring that if your code contains a call to a method named <code>bar</code> on an object of type <code>Foo</code>, that in the <code>Foo</code> class definition there actually exists a method named <code>bar</code> with the correct parameters corresponding to the arguments provided. Without type safety, or static type annotations, you increase your risk of writing code that breaks when your users are using it, after release, rather than at development time, before release. Static type specification also serves as a form of built-in documentation for other developers on your team to know how to properly use each other's code more reliably and confidently. Static type checking represents an important theme of Software Engineering as a discipline, and this course: time invested in detailed specification and documentation allows teams to collaborate more successfully and unlocks opportunities to verify correctness with tools during development.</p> <p>TypeScript shares some similarities to Java - both are high-level languages, both support object-oriented programming. However, there are many key differences (including their purpose, how they are compiled, etc). You will learn more about these features throughout the course. </p> <p>This document is designed to help you become familiar with the syntax and features of TypeScript from the context of the Java experience you all have had in COMP 301. It compares the syntax between Java and TypeScript in various situations and should serve as a guide going into the first few weeks of the semester.</p>"},{"location":"resources/frontend/2-typescript/#syntax","title":"Syntax","text":"<p>The syntax for TypeScript is pretty succint and less verbose than Java! In this section, you will learn the syntax of TypeScript code with the context of the Java syntax you have worked in throughout COMP 210 and COMP 301.</p>"},{"location":"resources/frontend/2-typescript/#typing","title":"Typing","text":"<p>The first major distinction between Java and TypeScript exists with its typing system. Recall the following:</p> <ul> <li>Primitive types are a set of basic data types in a programming language. All other data types and classes can be constructed from these primitive types. Using standard programming conventions, primitive types are often denoted with an all-lowercase name and usually do not need to be imported.</li> <li>Reference types, on the other hand, are all of the other types in a language. Reference types are defined as structures that contain or build upon the basic primitive types. Reference types are often defined by interfaces, classes, and enumerations. Reference types, like the name of all clases, often start with a capital letter (For example, <code>Dog</code> or <code>Cat</code>).</li> </ul> <p>In Java, we have the following primitive types: * <code>int</code>: Represents a number with no decimal places. * <code>double</code>: Represents a number that can store fractions (decimal places). * <code>boolean</code>: Represents a state that can either be <code>true</code> or <code>false</code>.</p> <p>In TypeScript, on the otherhand, we have different primitive types. TypeScript defines the following: * <code>number</code>: Represents a number that can store fractions (decimal places). * <code>boolean</code>: Represents a state that can either be <code>true</code> or <code>false</code>. * <code>string</code>: Represents a sequence of characters.</p> <p>Notice there is not a type distinction between integers and floats / doubles. We use <code>number</code> in TypeScript for both. This is helpful because it effectively allows us to work with double-floating point, 64-bit, values for all numerical computations.</p> <p>Second, notice that <code>string</code> is not capitalized in TypeScript. Technically, the string values we use wind up being references and realizations of the immutable <code>String</code> class in TypeScript/JavaScript, with its expected methods, but its type is specified with lowercase letters as a built-in.</p>"},{"location":"resources/frontend/2-typescript/#variable-and-constant-declarations","title":"Variable and Constant Declarations","text":"<p>Now that you know a bit about the basic data types in TypeScript, let's take a look at how to define variables.</p> <p>Let's compare a number declaration in Java and TypeScript, then compare more generally.</p> JavaTypeScript <pre><code>// Declaring a Number\nint myNumber = 88;\n\n// General Formula\ntype name = value;\n</code></pre> <pre><code>// Declaring a Number\nlet myNumber: number = 88;\n\n// General Formula\nlet name: type = value;\n</code></pre> <p>As you can see, there are a few differences. First, in Java, we specify the data type first. In TypeScript, we provide a type annotation after the name of the variable. We also provide the <code>let</code> keyword before variable name.</p> <p>You can also notice the difference in types. In Java, we use the <code>int</code> primitive type. In TypeScript, we use <code>number</code> instead. Lastly, you can note that both Java and TypeScript use semicolons at the end of their lines.</p> <p>What if we wanted to make these values constants instead of variables (so that we cannot change their value later)?</p> JavaTypeScript <pre><code>// Declaring a Constant\nfinal int myNumber = 88;\n\n// General Formula\nfinal type name = value;\n</code></pre> <pre><code>// Declaring a Constant\nconst myNumber: number = 88;\n\n// General Formula\nconst name: type = value;\n</code></pre> <p>As you can see, in Java, we use the <code>final</code> keyword to turn a variable into a constant. The keyword is appended to the front. In TypeScript however, we just use the <code>const</code> keyword instead of the <code>let</code> keyword!</p>"},{"location":"resources/frontend/2-typescript/#arrays","title":"Arrays","text":"<p>The way that arrays work in Java and TypeScript are a bit different, and so is the syntax to create them.</p> <p>In Java, you probably remember that the length of an array cannot be changed once it is set - and that using <code>ArrayList&lt;&gt;</code> or any other subtype of <code>List</code> (imported from <code>java.utils.*</code>) provides this functionality!</p> <p>TypeScript arrays are more similar to the Java <code>List</code> than to the Java array. Creating arrays in TypeScript is also very similar to creating lists in Python. To declare an array in TypeScript, we can simply add <code>[]</code> to the end of a variable's type annotation, and use brackets to add initial values. Compare the following:</p> TypeScriptPythonJava <pre><code>// Initialize\nlet names: string[] = [\"Aziz\", \"Andrew\"]\n// Add values\nnames.push(\"Jordan\");\n// Replace a value\nnames[2] = \"Kris\";\n// Remove a value\nnames.splice(names.indexOf(\"Kris\"), 1); // Removes by value\nnames.splice(1, 1); // Removes by index\n// Access a value \nlet aziz: string = names[0];\n</code></pre> <pre><code># Initialize\nnames: list[str] = [\"Aziz\", \"Andrew\"]\n# Add values\nnames.append(\"Jordan\")\n# Replace a value\nnames[2] = \"Kris\"\n# Remove a value\nnames.remove(\"Kris\") # Removes by value\nnames.remove(1) # Removes by index\n# Access a value\naziz = names[0]\n</code></pre> <pre><code>// Initialize\nList&lt;String&gt; names = new ArrayList&lt;&gt;();\nnames.add(\"Aziz\");\nnames.add(\"Andrew\");\n// Add values\nnames.add(\"Jordan\");\n// Replace a value\nnames.set(\"Kris\", 2);\n// Remove a value\nnames.remove(\"Kris\"); // Removes by value\nnames.remove(1); // Removes by index\n// Access a value\nString aziz = names.get(0);\n</code></pre> <p>Just like in Python lists and traditional Java arrays (but unlike Java's <code>List</code>), we can index values of TypeScript arrays using the subscription <code>[]</code> syntax.</p> <p>As shown in the code above, TypeScript does not have a built-in delete method - but, it does have <code>.splice(i, n)</code>, which removes <code>n</code> number of elements starting at index <code>i</code>. So, we can combine this with <code>.indexOf()</code> to delete our value.</p> <p>TypeScript's arrays also have a <code>.pop()</code> method that removes the last item of an array. </p> <p>To access the length of a TypeScript array, you can use the array's <code>length</code> field. For example, given an array <code>a</code>, the length of this array would be accessed using <code>a.length</code>.</p>"},{"location":"resources/frontend/2-typescript/#conditionals","title":"Conditionals","text":"<p>Java and TypeScript have similar syntax for creating conditional statements and if-statements. </p> <p>TypeScript uses the same boolean operators that Java does. This means that <code>&amp;&amp;</code> represents AND, <code>||</code> represents OR, and <code>!</code> represents NOT. TypeScript and Java both use the lowercased <code>true</code> and <code>false</code> for boolean values.</p> <p>Additionally, following in the C-family heritage, the <code>&amp;&amp;</code> and <code>||</code> operators are short-circuiting. If the left-hand expression of an <code>&amp;&amp;</code> operator is <code>false</code>, the right-hand expression will not be evaluated. Conversely, if the left-hand expression of an <code>||</code> operator is <code>true</code>, then the right-hand expression will not be evaluated. This matters when the right-hand expression contains a function or method call that mutates state.</p> <p>Coming from Java, one surprising feature of JavaScript and TypeScript is the notion of truthiness. You can learn more about truthy values on MDN, a great documentation resource for front-end web concerns. Many values besides the boolean value <code>true</code> are treated as <code>true</code>/\"truthy\" in boolean contexts in JavaScript. For example, any non-empty strings and any non-zero numbers are considered \"truthy\" in boolean contexts. This means you can write a valid, type safe expression like <code>\"foo\" || \"\"</code>. Surprisingly, the <code>||</code> operator evaluates to its first truthy value, so <code>\"foo\" || \"\"</code>, <code>\"\" || \"foo\"</code>, and <code>\"foo\" || \"bar\"</code> all evaluate to <code>\"foo\"</code>, not <code>true</code>. This is handy and commonly used in variable initialization statements, such as <code>let initialValue: string = userInput || \"Default Value\";</code></p> <p>If-statements have the same syntax and usage as they do in Java!</p> JavaTypeScript <pre><code>if (conditionA || conditionB) {\n // Some code here!\n}\nelse {\n // Some code here.\n}\n</code></pre> <pre><code>if (conditionA || conditionB) {\n // Some code here!\n}\nelse {\n // Some code here.\n}\n</code></pre> <p>NOTE: Both Java and TypeScript require the use of parenthesis <code>( )</code> around the conditional statements in if-statements.</p>"},{"location":"resources/frontend/2-typescript/#loops","title":"Loops","text":""},{"location":"resources/frontend/2-typescript/#the-while-loop","title":"The <code>while</code> Loop","text":"<p>Just like with if-statements, both Java and TypeScript use the same syntax for while loops! We use parenthesis around the conditional in both languages. </p> JavaTypeScript <pre><code>while (conditionA) {\n // Some code here!\n}\n</code></pre> <pre><code>while (conditionA) {\n // Some code here!\n}\n</code></pre>"},{"location":"resources/frontend/2-typescript/#the-for-loop","title":"The <code>for</code> Loop","text":"<p>In both Java and TypeScript, there are two types of loops that both serve distinct purposes.</p> <p>The first type of loop contains a counter variable that is modified each time the the loop iterates - and, iteration stops when some provided condition evaluates to false. This type of loop exists in both Java and TypeScript. The code is nearly identical, but notice that in the TypeScript version, we need to use our new method of creating variables. We do not say <code>int i = 0;</code>, instead we say <code>let i = 0;</code>. We can see this here:</p> JavaTypeScript <pre><code>for(int i = 0; i &lt; 10; i++) {\n // Loop body here...\n}\n</code></pre> <pre><code>for(let i = 0; i &lt; 10; i++) {\n // Loop body here...\n}\n</code></pre> <p>NOTE: In this example, notice how we do not include the type annotation on the conditional variable. In general, type annotations on variables in TypeScript are not necessary by default. TypeScript infers types of variables when there is no explicit type annotation provided. However, including them is strongly encouraged. In this case, for conciseness in the for loop header body and the the fact that the variable's type is guaranteed to be <code>number</code>, it can be omitted here.</p> <p>The second type of loop in Java allows you to iterate over a collection, where a variable is updated with a value corresponding to the current iteration. This is often the most widely-used loop. There are syntactical differences here between Java and TypeScript, both in the keywords used and the variable creation convention.</p> JavaTypeScript <pre><code>for(String name : names) {\n // Loop body here...\n}\n</code></pre> <pre><code>for(let name of names) {\n // Loop body here...\n}\n</code></pre> <p>As you can see, like in previous examples, TypeScript uses the <code>let</code> keyword. In addition, Java uses <code>:</code>, while TypeScript uses <code>of</code>.</p>"},{"location":"resources/frontend/2-typescript/#defining-functions","title":"Defining Functions","text":"<p>Functions are the most fundamental abstraction technique we use in software engineering. It is important to note that in Java, we create methods, which are functions that are members of a class. In TypeScript, we also mainly work in the context of classes, but we are not necessarily required to. So, if you are hearing the term \"functions\" and \"methods\" passed around, it is useful to remember this distinction: methods are called on an object (e.g. <code>object.method()</code>) whereas functions are generally called standalone <code>function()</code>. This distinction has some nuance in more advanced uses of TypeScript/JavaScript, but is generally how you should approach it.</p> <p>There are many fundamental differences in the syntax for creating functions in Java and TypeScript. Let's take a look at an example of a function that takes in a user's name and returns a string that greets the user.</p> JavaTypeScript <pre><code>String greet(String name) {\n return \"Welcome, \" + name + \"!\";\n}\n</code></pre> <pre><code>function greet(name: string): string {\n return \"Welcome, \" + name + \"!\";\n}\n</code></pre> <p>There are a few noticeable differences. First, TypeScript uses the <code>function</code> keyword at the front rather than specifying a return type first. Also, the type annotation is at the end of the function header (and before the body). The placement of type annotations for the function parameters also changes here.</p> <p>In the case that a function returns nothing, note that in Java, we specify the return type to be <code>void</code>. We can do this in TypeScript too, however it is optional. Both including <code>: void</code> or not is valid. For example:</p> JavaTypeScript <pre><code>void doSomething() {\n // Implementation Not Shown\n}\n</code></pre> <pre><code>function doSomething() {\n // Implementation Not Shown\n}\n\n// OR\n\nfunction doSomething(): void {\n // Implementation Not Shown\n}\n</code></pre>"},{"location":"resources/frontend/2-typescript/#arrow-functions","title":"Arrow Functions","text":"<p>TypeScript also has a tremendously useful feature called arrow functions. Arrow functions are a more compact and concise method of defining traditional functions. Let's take a look at a function from above as a traditional function and one as an arrow function.</p> TypeScript - Traditional FunctionTypeScript - Arrow Function <pre><code>function greet(name: string): string {\n return \"Welcome, \" + name + \"!\";\n}\n</code></pre> <pre><code>let greet = (name: string): string =&gt; {\n return \"Welcome, \" + name + \"!\";\n}\n</code></pre> <p>There are a few things to unpack here. First, it looks like we are ultimately assigning \"something\" to a variable. We use the <code>let</code> keyword and we provide a variable name! On the right, we have a weird structure that would go in the value spot of our variable formula.</p> <p>In fact, this is exactly what we are doing! We are saving a function to a variable and giving it a name that we can use to call it. In the <code>( )</code>, we provide the parameters to the function. We provide a return type in the type annotation as well. Then, we use <code>=&gt;</code> to connect these parameters to a function body.</p> <p>We can then call our function in the same way we would normally, like so: <pre><code>greet(\"Jade\");\n</code></pre></p> <p>While this seems like just a syntactic change, the implications of this are massive and opens the door to an entire new world of programming called functional programming, as we can pass around functions as values. This is something that we will be covering extensively throughout this course, however it is super important to become familiar with the arrow function syntax now so it is less suprising later!</p> <p>To conclude this section, provide two important caveats must be emphasized: * Arrow functions don't have their own <code>this</code> bindings and therefore should not be used when defining methods of a class. * Arrow functions cannot be used as constructors. Calling them with <code>new</code> throws a <code>TypeError</code>.</p> <p>These caveats are important to note because traditional functions and arrow functions are not exactly the same, and there are some semantic differences.</p>"},{"location":"resources/frontend/2-typescript/#class-and-interface-construction","title":"Class and Interface Construction","text":"<p>Classes define data types and are the foundation of object-oriented programming. It will be critical for you to be comfortable working within TypeScript classes throughout your time in COMP 423! While there are many syntax differences between classes in Java and TypeScript, the core idea and motivation for using them remains the same. Below is an example of a full class in both Java and TypeScript. I recommend that you read this in its entirely and try to compare line by line! From there, we will go through each section.</p> TypeScriptJava <pre><code>/** Represents a UNC Student. */\npublic class Student {\n\n    /* Fields\n    * NOTE: In COMP 301, you learned about using the\n    * `private` keyword on fields to control access\n    * via getter and setter methods. For this example,\n    * I am making some fields public and others private.\n    */\n\n    /** Represents the name of the student */\n    public name: string;\n    /** Represents the year of the student*/\n    public year: number;\n    /** Represents the address of the student */\n    private address: string;\n\n    /* Constructor */\n    constructor(name: string, year: number, adr: string) {\n        this.name = name;\n        this.year = year;\n        this.address = adr;\n        this.welcome();\n    }\n\n    /* Methods */\n\n    /** Prints a welcome message to the console. */\n    public welcome() {\n        console.log(\"Hello, \" + this.name + \"!\");\n    }\n\n    /** Converts a year number to a description. */\n    public static yearToString(year: number): string {\n        if(year == 1) {\n            return \"Freshman\";\n        } else if(year == 2) {\n            return \"Sophomore\";\n        } else if(year == 3) {\n            return \"Junior\";\n        } else if(year == 4) {\n            return \"Senior\";\n        }\n\n        return \"Oops...\";\n    }\n}\n</code></pre> <pre><code>/** Represents a UNC Student. */\npublic class Student {\n\n    /* Fields\n    * NOTE: In COMP 301, you learned about using the\n    * `private` keyword on fields to control access\n    * via getter and setter methods. For this example,\n    * I am making some fields public and others private.\n    */\n\n    /** Represents the name of the student */\n    public String name;\n    /** Represents the year of the student*/\n    public int year;\n    /** Represents the address of the student */\n    private String address;\n\n    /* Constructor */\n    public Student(String name, int year, String adr) {\n        this.name = name;\n        this.year = year;\n        this.address = adr;\n        this.welcome();\n    }\n\n    /* Methods */\n\n    /** Prints a welcome message to the console. */\n    public void welcome() {\n        System.out.println(\"Hello, \" + this.name + \"!\");\n    }\n\n    /** Converts a year number to a description. */\n    public static String yearToString(int year) {\n        if(year == 1) {\n            return \"Freshman\";\n        } else if(year == 2) {\n            return \"Sophomore\";\n        } else if(year == 3) {\n            return \"Junior\";\n        } else if(year == 4) {\n            return \"Senior\";\n        }\n\n        return \"Oops...\";\n    }\n}\n</code></pre> <p> </p> <p>As you can see, there are a few similarities between classes in Java and TypeScript! First, we can look at access modifiers. The <code>public</code>, <code>private</code>, and <code>protected</code> keywords are the same in both Java and TypeScript.</p> <p>Notice that the fields are the same conventions as well! Note however that the <code>let</code> keyword is not used when defining fields - it is only needed when defining regular variables.</p> <p>The constructor also differs a bit. In TypeScript, the <code>constructor</code> keyword replaces <code>public ClassName</code> from Java! The type annotations in the parameters also follow the normal conventions of TypeScript functions.</p> <p>Lastly, like fields, methods in TypeScript also do not use their respective keyword (<code>function</code>) to be defined. Instead, we can just provide an access modifier.</p> <p>We use the <code>static</code> keyword to denote class methods the same way in both Java and TypeScript. Learn more about class fields and methods here.</p> <p>Within a function, we also have access to the <code>this</code> keyword that references the current object.</p> <p>Now, how do we instantiate objects?</p> <p>We actually use the same syntax as in Java:</p> JavaTypeScript <pre><code>Student noah = new Student(\"Noah\", 3, \"Columbia St\");\n</code></pre> <pre><code>noah: Student = new Student(\"Noah\", 3, \"Columbia St\");\n</code></pre> <p>We can also define interfaces in TypeScript like we do in Java. Take a look at the following:</p> JavaTypeScript <pre><code>public interface Person {\n String name;\n}\n\npublic class Student implements Person { /* ... */ }\n</code></pre> <pre><code>public interface Person {\n name: string;\n}\n\npublic class Student implements Person { /* ... */ }\n</code></pre> <p>As you can see, the keywords remain the same between both languages with <code>interface</code> and <code>implements</code>! The only difference between the two languages are with variable creation and type annotation conventions.</p> <p>There is also another interesting feature of TypeScript worth mentioning here.</p> <p>TypeScript employs structural type checking. This means that TypeScript views objects as equivalent types if they share the same structure, NOT just the same name! On the otherhand, Java is a nominally type language, which means it views objects as equivalent types if they share the same name ONLY (or if there is an inheritence relationship).</p> <p>So, we can technically directly create a value of type <code>Person</code> in TypeScript! This is not something you can directly do in Java without creating a subclass. The syntax would look like so:</p> TypeScript <pre><code>let person: Person = {\n name: \"Charles\"\n};\n</code></pre> <p>In this example, we use JSON (JavaScript object notation) to create an object of data that contains the same properties that a <code>Person</code> objct should have. Surprisingly enough, due to TypeScript being a structural language, this is a valid way to instantiate an object, technically of type object, that can used anywhere an object of type <code>Person</code> is expected without explicitly implementing the <code>Person</code> interface!</p> <p>This feature is often called \"duck typing\", thanks to the addage \"if it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck.\" TypeScript and structural typing take it further: \"if it's a goose that looks like a duck, then it's a duck.\" In programming, structural type checking like TypeScript's, relaxes the strictness of nominal typing like Java's, by embracing the idea that if an object has all the same fields and methods needed as some other type, then it's probably OK to treat it as that other type.</p>"},{"location":"resources/frontend/2-typescript/#extra-typescript-features","title":"Extra TypeScript Features","text":""},{"location":"resources/frontend/2-typescript/#comments","title":"Comments","text":"<p>The examples throughout this document have already used many comments, however we create comments in Java and TypeScript in the exact same way! This is shown below:</p> JavaTypeScript <pre><code>// This is a single-line comment.\n\n/*\nThis is an example of a\nmulti-line comment!\n*/\n</code></pre> <pre><code>// This is a single-line comment.\n\n/*\nThis is an example of a\nmulti-line comment!\n*/\n</code></pre>"},{"location":"resources/frontend/2-typescript/#printing-values","title":"Printing Values","text":"<p>In Java and TypeScript, we have statements to print out values! In TypeScript, values are printed to the console. We use the following convention to print values:</p> JavaTypeScript <pre><code>String taName = \"Jean\";\nSystem.out.println(taName);\n// Output:\n// &gt;&gt; Jean\n</code></pre> <pre><code>let taName: string = \"Jean\";\nconsole.log(taName);\n// Console:\n// &gt;&gt; Jean\n</code></pre> <p>As you can see, we use <code>console.log()</code> to print out values to the console in TypeScript. To see values printed to <code>console.log()</code> in a browser, you will need to open your browser's developer tools and view its console tabl.</p>"},{"location":"resources/frontend/2-typescript/#enums","title":"Enums","text":"<p>Enums (enumerators) are an extremely useful language feature in many programming languages! Enums allow you to define custom, related values or states. Think of enums as implementing a multiple-choice question, where there are many options! Let's look at an example:</p> JavaTypeScript <pre><code>enum Direction {\n  UP,\n  DOWN,\n  LEFT,\n  RIGHT,\n}\n</code></pre> <pre><code>enum Direction {\n  Up,\n  Down,\n  Left,\n  Right,\n}\n</code></pre> <p>As you might notice, creating enums in TypeScript is nearly equivalent to its Java counterpart that you saw in COMP 301! The main difference is that it is convention for Java enum options to be entirely  capitalized, while TypeScript enum options only have their first letter capitalized.</p> <p>Let's look at the <code>Direction</code> enum applied in a TypeScript function:</p> TypeScript <pre><code>function directionToText(direction: Direction): string {\n if(direction == Direction.Up || direction == Direction.Down) {\n  return \"Let's go vertically!\"\n }\n else if (direction == Direction.Left || direction == Direction.Right) {\n  return \"Let's go horizontally!\"\n }\n}\n</code></pre> <p>Enumerations will be extremely useful in your final projects to model data.</p>"},{"location":"resources/frontend/2-typescript/#type-aliases","title":"Type Aliases","text":"<p>There is a nifty feature in TypeScript called the type alias, which essentially allows you to create another label by which you can refer to a type. This can be useful to make types more concise, or to make it more readable for your feature. Look at the following example:</p> TypeScript <pre><code>type Rating = number;\n\nlet csxlRating: Rating = 10;\n</code></pre> <p>Using the <code>type</code> keyword, we give <code>number</code> an alias as <code>Rating</code>. Now, we can use <code>number</code> and <code>Rating</code> interchangeably. Next, we create a variable called <code>csxlRating</code> of type <code>Rating</code> (which is really just type <code>number</code>), and then assign a number to it.</p>"},{"location":"resources/frontend/2-typescript/#ternary-operator","title":"Ternary Operator","text":"<p>The last super useful feature of TypeScript to feature in this document is the ternary operator. The ternary operator allows you to write a conditional expression. Unlike the <code>if</code>/<code>else</code> syntax in TypeScript and Java, which are statements, the ternary operator results in an expression. This means that if a condition is <code>true</code>, the expression can evaluate to one value and if it's <code>false</code>, another.</p> <p>The ternary operator uses the following syntax: <code>condition ? expr_if_true : expr_if_false</code></p> <p>Let's look at an example relating to the CSXL site:</p> TypeScript <pre><code>// Stores the hour which the CSXL opens.\n// For sake of example, say the CSXL opens at 10am on weekdays and 12pm on weekends:\nlet csxlOpeningHour: number = isWeekday ? 10 : 12;\n\nconsole.log(csxlOpeningHour);\n\n// Output IF isWeekday = true:\n// &gt;&gt; 10\n// Output IF isWeekday = false:\n// &gt;&gt; 12\n\n// Since the ternary operator produces an expression, it can\n// also be used like:\nconsole.log(isWeekday ? \"Weekday\" : \"Weekend\")\n</code></pre> <p>This is the same syntax that is used in Java! Ternary operators are extremely useful and are used numerous times throughout the CSXL application. I highly recommend checking out the codebase and searching for <code>?</code> / <code>:</code> to see more relevant examples!</p>"},{"location":"resources/frontend/2-typescript/#generic-types","title":"Generic Types","text":"<p>Generic types are a powerful convention in Java that allows you to pass types as a parameter into objects. This makes objects support multiple data types.</p> <p>For example, take a <code>LinkedList</code> implementation in Java. Linked lists are data structures that can store many different types of values. For example, look at the following in Java:</p> Java <pre><code>// Create a linked list that stores strings.\nLinkedList&lt;String&gt; myStringList = new LinkedList&lt;&gt;();\n// Create a linked list that stores students.\nLinkedList&lt;Student&gt; myRoster = new LinkedList&lt;&gt;(); \n</code></pre> <p>The above Java syntax likely looks vaguely familiar! Here, we are creating two linked lists - one of <code>String</code> objects and the other of <code>Student</code> objects. We pass the data type of the object we want to store into the <code>&lt; &gt;</code> part of the type annotation.</p> <p>TypeScript also supports generic types! This is a feature that will be used a lot throughout this course. First, let's compare the syntax for creating the hypothetical lists shown above:</p> JavaTypeScript <pre><code>// Create a linked list that stores strings.\nLinkedList&lt;String&gt; myStringList = new LinkedList&lt;&gt;();\n// Create a linked list that stores students.\nLinkedList&lt;Student&gt; myRoster = new LinkedList&lt;&gt;(); \n</code></pre> <pre><code>// Create a linked list that stores strings.\nlet myStringList: LinkedList&lt;string&gt; = new LinkedList&lt;&gt;();\n// Create a linked list that stores students.\nlet myRoster: LinkedList&lt;Student&gt; = new LinkedList&lt;&gt;(); \n</code></pre> <p>As you can see, the only thing different between both code snippets are how we declare the variable! The usage of <code>&lt; &gt;</code> remains the same.</p> <p>Now, how would we actually implement the <code>LinkedList&lt;T&gt;</code> class? Let's compare two rudimentary implmentations in both Java and TypeScript:</p> JavaTypeScript <pre><code>/** Represents a linked list node. */\npublic class LinkedList&lt;T&gt; {\n\n /** Value for the node. */\n private T value;\n /** Next node, if it exists. */\n private LinkedList&lt;T&gt; next;\n\n /** Constructor */\n public LinkedList(T value) {\n  this.value = value;\n }\n\n /** Returns the value of the node. */\n public T getValue() {\n  return this.value;\n }\n\n/* Modifies the value of the node. */\n public void setValue(T value) {\n  this.value = value;\n }\n\n /* Other methods not shown */\n}\n</code></pre> <pre><code>/** Represents a linked list node. */\npublic class LinkedList&lt;T&gt; {\n\n /** Value for the node. */\n private value: T;\n /** Next node, if it exists. */\n private next: LinkedList&lt;T&gt;;\n\n /** Constructor */\n constructor(value: T) {\n  this.value = value;\n }\n\n /** Returns the value of the node. */\n public getValue(): T {\n  return this.value;\n }\n\n/* Modifies the value of the node. */\n public setValue(value: T) {\n  this.value = value;\n }\n\n /* Other methods not shown */\n}\n</code></pre> <p>In the header of the class, we put <code>&lt;T&gt;</code>, which specifies that we are adding a type parameter! Whenever this is then provided, like in <code>LinkedList&lt;string&gt;</code> or <code>LinkedList&lt;Student&gt;</code>, the <code>T</code> used throughout the class is then replaced by the type that is provided! So in the <code>LinkedList&lt;string&gt;</code> example, the field <code>value</code> becomes of type <code>string</code>. In the <code>LinkedList&lt;Student&gt;</code> example, the field <code>value</code> becomes of type <code>Student</code>.</p> <p>If this concept is unfamiliar, we highly recommend to practice using generic types in classes, as well as experimenting on your own! Generic types are an invaluable tool to make code extendable to multiple use-cases, and is used in many of the packages we are going to use throughout the course.</p>"},{"location":"resources/frontend/2-typescript/#conclusion","title":"Conclusion","text":"<p>Congratulations! \ud83c\udf89 TypeScript is an extremely powerful and useful language, and we you will have the chance to work with TypeScript code this entire semester. If you are having trouble remembering TypeScript syntax, feel free to return to this document at any time. In addition, for practice, we highly recommend you go to the official TypeScript playground or open a new Repl.it ! The TypeScript playground, as well as opening a <code>.ts</code> TypeScript file in Visual Studio Code and playing around with it, are some of the best ways to get more familiar and accustomed to using the language. As you have seen throughout your computer science careers so far, sometimes the best way to learn is to dive right in!</p> <p>In addition, below are some additional resources that you may find useful as your work through learning TypeScript.</p>"},{"location":"resources/frontend/2-typescript/#extra-resources","title":"Extra Resources","text":"<ul> <li>Official TypeScript Cheat Sheets</li> <li>Official Docs - TypeScript for the Java Programmer</li> </ul>"},{"location":"resources/frontend/3-event-driven/","title":"Introduction to Event-Driven Programming in TypeScript","text":""},{"location":"resources/frontend/3-event-driven/#function-definitions-and-arrow-functions","title":"Function Definitions and Arrow Functions","text":"<p>As a refresher from the previous reading, in TypeScript, functions are first-class citizens, meaning they can be assigned to variables, passed as arguments, and returned from other functions. One of the most common ways to define functions concisely is using arrow functions.</p> <p>Here\u2019s a simple example:</p> <pre><code>const square = (x: number): number =&gt; {\n    return x * x;\n};\n</code></pre> <ul> <li>The function takes a parameter <code>x</code> of type <code>number</code>.</li> <li>It returns a <code>number</code>.</li> <li>The function itself is stored in a variable <code>square</code>, which holds a reference to the function definition.</li> </ul>"},{"location":"resources/frontend/3-event-driven/#short-hand-arrow-function-syntax","title":"Short-Hand Arrow Function Syntax","text":"<p>For functions whose bodies consist of a single return statement, like the <code>square</code> function, JavaScript and TypeScript provide a shorthand syntax:</p> <pre><code>const square = (x: number): number =&gt; x * x;\n</code></pre> <p>This is still a full function definition! It simply omits the curly braces and <code>return</code> keyword to make the code more concise. This form is often used in functional programming and for inline callbacks.</p>"},{"location":"resources/frontend/3-event-driven/#functional-interfaces","title":"Functional Interfaces","text":"<p>In TypeScript, we can define an interface that represents a function type. This makes our code more structured and provides strong type-checking. Unlike an object-oriented interface, a functional interface does not name the methods it supports; it simply defines a function signature that any matching function can satisfy.</p> <pre><code>interface UnaryFunction {\n    (x: number): number;\n}\n\nconst squareFunction: UnaryFunction = (x) =&gt; x * x;\n</code></pre>"},{"location":"resources/frontend/3-event-driven/#why-is-this-a-functional-interface","title":"Why is this a Functional Interface?","text":"<p>The key distinction between a functional interface and a traditional object-oriented interface is that a functional interface does not define named methods; instead, it only specifies a function signature. This means that any function conforming to the parameter and return types can be assigned to a variable of this type.</p>"},{"location":"resources/frontend/3-event-driven/#understanding-function-types","title":"Understanding Function Types","text":"<p>A function\u2019s type is determined by the combination of its parameter types and return type. This is intuitive because if two functions agree on these types, they can be used interchangeably in function call expressions without breaking type safety.</p> <p>For example, consider:</p> <pre><code>const double: UnaryFunction = (x) =&gt; x * 2;\nconst negate: UnaryFunction = (x) =&gt; -x;\n</code></pre> <p>Since <code>double</code> and <code>negate</code> both conform to the <code>UnaryFunction</code> signature, we can substitute one for the other in any context where a <code>UnaryFunction</code> is expected, and TypeScript will still type-check correctly.</p> <p>This principle of substitutability ensures that TypeScript maintains strong type safety while allowing flexibility in function-based programming paradigms.</p>"},{"location":"resources/frontend/3-event-driven/#type-inference","title":"Type Inference","text":"<p>A modern feature of TypeScript is type inference, which allows the compiler to automatically determine types based on context. Since the type of function variables like <code>double</code> and <code>negate</code> is explicitly defined, TypeScript can infer and enforce the correct parameter and return types without needing additional annotations.</p> <p>This provides the best of both worlds: full type safety while reducing the effort required to redundantly specify types that can be inferred.</p>"},{"location":"resources/frontend/3-event-driven/#higher-order-functions","title":"Higher-Order Functions","text":"<p>A higher-order function is a function that takes another function as an argument or returns a function. This allows us to write more flexible and reusable code by parameterizing behavior.</p>"},{"location":"resources/frontend/3-event-driven/#higher-order-function-example-map","title":"Higher-order Function Example: <code>map</code>","text":"<p>Before diving into the code, let\u2019s break down <code>map</code> in English. Imagine you have a list of numbers, and you want to apply a specific operation\u2014like squaring each number\u2014to every element. One way to do this is with a loop, applying the operation to each item manually. But what if you want to apply different operations, such as doubling or taking the absolute value? Instead of writing separate loops for each case, we can use <code>map</code> to generalize the process.</p> <p>The <code>map</code> function takes an array and a function as arguments. It applies the function to each element of the array and returns a new array with the transformed values. This lets us reuse <code>map</code> with any function we choose, making our code more modular and expressive.</p>"},{"location":"resources/frontend/3-event-driven/#implementing-a-custom-map-function","title":"Implementing a Custom <code>map</code> Function","text":"<p>Here's an example of how we might implement <code>map</code> from scratch in TypeScript:</p> <pre><code>const map = (arr: number[], func: (num: number) =&gt; number): number[] =&gt; {\n    const result: number[] = [];\n    for (const num of arr) {\n        result.push(func(num));\n    }\n    return result;\n};\n\nconst square = (x: number) =&gt; x * x;\n\nconst numbers = [1, 2, 3, 4];\nconst squaredNumbers = map(numbers, square);\nconsole.log(squaredNumbers); // [1, 4, 9, 16]\n</code></pre>"},{"location":"resources/frontend/3-event-driven/#why-pass-functions-around","title":"Why Pass Functions Around?","text":"<p>The ability to pass functions as arguments gives us powerful ways to structure our code. Instead of defining heavyweight classes or duplicating logic, we can simply pass different behaviors into a function like <code>map</code> to get the desired result. This keeps our code cleaner and more flexible.</p> <p>For example, we can easily swap out the function to apply different transformations:</p> <pre><code>const double = (x: number) =&gt; x * 2;\nconsole.log(map(numbers, double)); // [2, 4, 6, 8]\n\nconst absolute = (x: number) =&gt; Math.abs(x);\nconsole.log(map([-1, -2, 3, -4], absolute)); // [1, 2, 3, 4]\n</code></pre> <p>By using higher-order functions, we reduce redundancy and make our programs more expressive, letting us focus on the what rather than the how. This approach is at the core of functional programming and leads to more readable and maintainable code.</p>"},{"location":"resources/frontend/3-event-driven/#introduction-to-event-driven-asynchrony","title":"Introduction to Event-Driven Asynchrony","text":"<p>Most modern web applications rely on event-driven programming to handle user interactions and network requests efficiently. Imagine clicking a button in a web app that triggers a network request to fetch data. If JavaScript were to process this request in a blocking manner, the entire page would become unresponsive until the request finished. This would be a terrible experience\u2014no scrolling, no typing, no animations\u2014just a frozen screen until the network responded. Instead, JavaScript uses an event loop and an asynchronous model to ensure that while waiting for a task to complete, the rest of the application can continue running smoothly.</p>"},{"location":"resources/frontend/3-event-driven/#why-asynchrony","title":"Why Asynchrony?","text":"<p>Web applications frequently make requests to APIs across the internet. These requests can take time to complete due to network latency, slow servers, or large amounts of data being transferred. If the UI were to block while waiting for each request, users would experience frustrating lags. Instead, JavaScript offloads such tasks to the event loop, which allows the browser to continue processing other user interactions while waiting for the network request to finish.</p> <p>Here\u2019s how the event loop works:</p> <pre><code>sequenceDiagram\n    participant User\n    participant JavaScript\n    participant WebAPI\n    participant CallbackQueue\n\n    User-&gt;&gt;JavaScript: Clicks a button\n    JavaScript-&gt;&gt;WebAPI: Sends API request (non-blocking)\n    JavaScript-&gt;&gt;User: UI remains interactive\n    WebAPI--&gt;&gt;CallbackQueue: Response arrives\n    CallbackQueue-&gt;&gt;JavaScript: Executes callback (updates UI)</code></pre> <p>The event loop continuously checks the callback queue for pending tasks and runs them only when the main execution stack is empty. This ensures smooth execution without blocking.</p>"},{"location":"resources/frontend/3-event-driven/#promises-and-fetch-api","title":"Promises and Fetch API","text":"<p>A Promise represents a value that might be available now, or in the future, or never. JavaScript provides a built-in <code>Promise</code> class, which is a powerful tool for handling asynchronous operations. Instead of writing nested callbacks (which can become unreadable), Promises provide methods that allow us to register behavior for when an operation completes or fails.</p> <p>A <code>Promise</code> has two key methods:</p> <ul> <li><code>.then(callback)</code> \u2013 Runs the callback function when the Promise resolves successfully.</li> <li><code>.catch(callback)</code> \u2013 Runs the callback function when the Promise is rejected due to an error.</li> </ul> <p>The <code>fetch</code> API in JavaScript returns a Promise when making HTTP requests. Here\u2019s an example:</p> <pre><code>const fetchData = (): Promise&lt;void&gt; =&gt; {\n    return fetch(\"https://jsonplaceholder.typicode.com/todos/1\")\n        .then(response =&gt; response.json()) // First promise\n        .then(data =&gt; console.log(data))   // Second promise\n        .catch(error =&gt; console.error(\"Error fetching data:\", error));\n};\n\nfetchData();\n</code></pre>"},{"location":"resources/frontend/3-event-driven/#understanding-chained-methods-and-fluent-apis","title":"Understanding Chained Methods and Fluent APIs","text":"<p>At first glance, this code might feel overwhelming, especially if you haven\u2019t encountered method chaining or fluent APIs before. Let\u2019s break it down step by step.</p> <ol> <li>Fetching Data: The <code>fetch</code> function initiates an HTTP request to retrieve data from the given URL. It immediately returns a <code>Promise</code> that will resolve when the response arrives.</li> <li>Processing the Response: Since <code>fetch</code> doesn\u2019t directly return the data but a <code>Response</code> object, we need to call <code>.json()</code> on it. This method also returns a Promise, which resolves when the response body has been parsed.</li> <li>Handling the Data: The next <code>.then()</code> receives the parsed JSON and logs it to the console.</li> <li>Error Handling: If anything goes wrong (e.g., network failure or an invalid response), the <code>.catch()</code> method ensures we handle errors gracefully.</li> </ol> <p>Each <code>.then()</code> method processes the result of the previous step, allowing us to chain operations in a clean and readable way. This avoids deeply nested callbacks (a problem known as \"callback hell\").</p>"},{"location":"resources/frontend/3-event-driven/#flow-explanation","title":"Flow Explanation:","text":"<ol> <li><code>fetch</code> returns a Promise resolving to a <code>Response</code> object.</li> <li>We call <code>.json()</code> on the <code>Response</code>, which returns another Promise.</li> <li>The second <code>.then()</code> handles the parsed JSON data.</li> <li>If anything goes wrong, <code>.catch()</code> captures errors.</li> </ol>"},{"location":"resources/frontend/3-event-driven/#understanding-the-promise-state-machine","title":"Understanding the Promise State Machine","text":"<p>A Promise in JavaScript follows a specific lifecycle, transitioning through different states:</p> <pre><code>graph TD;\n    A[Pending] --&gt;|Operation succeeds| B[Fulfilled]\n    A --&gt;|Operation fails| C[Rejected]</code></pre> <ul> <li>Pending: The initial state, where the Promise is waiting for the asynchronous operation to complete.</li> <li>Fulfilled: The operation completed successfully, and the <code>then()</code> handler is called.</li> <li>Rejected: The operation encountered an error, and the <code>catch()</code> handler is called.</li> </ul> <p>Each state transition happens exactly once. Once a Promise is either fulfilled or rejected, it cannot change state again.</p> <p>By understanding how Promises work and integrating them into event-driven programming, we gain fine control over asynchronous workflows. This ensures that applications remain responsive and efficient, leading to a better user experience.</p>"},{"location":"resources/frontend/3-event-driven/#asyncawait","title":"Async/Await","text":"<p>The <code>async/await</code> syntax is a modern way to write asynchronous code in JavaScript that builds directly on top of Promises. It makes asynchronous operations easier to read and reason about by allowing developers to write code that looks synchronous while retaining all the benefits of non-blocking execution.</p> <p>Under the hood, an <code>async</code> function always returns a Promise. The <code>await</code> keyword pauses execution inside the function until the awaited Promise resolves. This approach avoids deeply nested <code>.then()</code> chains and allows for more natural error handling with <code>try/catch</code> blocks.</p> <p>Using <code>async/await</code>, we can rewrite the previous Promise-based example in a more readable way:</p> <pre><code>const fetchDataAsync = async (): Promise&lt;void&gt; =&gt; {\n    try {\n        const response = await fetch(\"https://jsonplaceholder.typicode.com/todos/1\");\n        const data = await response.json();\n        console.log(data);\n    } catch (error) {\n        console.error(\"Error fetching data:\", error);\n    }\n};\n\nfetchDataAsync();\n</code></pre>"},{"location":"resources/frontend/3-event-driven/#why-use-asyncawait","title":"Why Use Async/Await?","text":"<ul> <li>Improves readability: Makes asynchronous code look and behave more like synchronous code, reducing cognitive load.</li> <li>Avoids callback hell: Eliminates the need for deeply nested <code>.then()</code> chains, improving maintainability.</li> <li>Intuitive error handling: Uses <code>try/catch</code> instead of <code>.catch()</code>, making it more consistent with synchronous code.</li> </ul>"},{"location":"resources/frontend/3-event-driven/#understanding-the-connection-to-promises","title":"Understanding the Connection to Promises","text":"<p>The <code>async/await</code> syntax does not introduce a new asynchronous mechanism; rather, it is a syntactic transformation of Promise-based code. The JavaScript engine internally rewrites <code>async</code> functions into standard Promise chains.</p> <p>For example, the above <code>async/await</code> function is equivalent to the following Promise-based implementation written before.</p> <p>This transformation has been formally studied in continuation-passing style (CPS) transformation research, proving that <code>async/await</code> retains the same computational expressiveness as Promises while improving code clarity. The ability to represent asynchronous workflows more naturally makes <code>async/await</code> the preferred approach in modern JavaScript development.</p>"},{"location":"resources/project-management/wireframes/","title":"Wireframing and Prototyping: Visualizing User Experiences","text":"<p>Wireframing is a foundational practice within User Interface (UI) and User Experience (UX) design, enabling teams to visualize ideas quickly, set clear expectations, and efficiently communicate with internal members as well as external stakeholders and clients. It serves as a critical step in project management and human-centered design, helping teams align their design vision with user needs and practical constraints.</p>"},{"location":"resources/project-management/wireframes/#communicating-and-storyboarding-with-wireframes","title":"Communicating and Storyboarding with Wireframes","text":"<p>Wireframes significantly enhance communication within teams and facilitate alignment with external stakeholders by visually representing concepts that can be challenging to articulate through text alone. Written descriptions often leave room for interpretation, which may lead to misunderstandings or gaps in expectations. For instance, a product team might use text to describe an interactive onboarding flow, believing they have communicated clearly. However, when stakeholders see the final implementation, they may find it confusing, overly complicated, or different from what they had envisioned.</p> <p>These pitfalls highlight why wireframes are invaluable for catching misunderstandings early. By visually mapping out interfaces and interactions, wireframes clarify how a user will actually engage with the product. This process helps to uncover hidden assumptions or overlooked complexities, allowing teams to address these issues proactively rather than reactively.</p> <p>Storyboarding with wireframes is particularly powerful when dealing with user stories or agile epics. Teams can visually outline user journeys, demonstrating clearly how a feature or product experience progresses step by step. For example, a written description might imply a straightforward user sign-up flow. However, once visually storyboarded with wireframes, it becomes obvious that additional steps or screens are needed to provide necessary context or guidance, improving usability and the overall user experience.</p> <p>Additionally, wireframing enhances empathy within teams by providing a tangible reference point that mirrors real user interactions. Rather than abstractly imagining user experiences, teams can concretely visualize workflows, helping them to anticipate user frustrations or difficulties. This empathy-driven approach ensures that designs are not only technically feasible but also intuitive and user-friendly.</p> <p>Ultimately, effective use of wireframes in storyboarding saves time and resources by reducing misalignment and ensuring everyone involved has a shared and accurate understanding of what the final product will feel like to the end-user.</p>"},{"location":"resources/project-management/wireframes/#wireframing-fidelity-spectrum","title":"Wireframing Fidelity Spectrum","text":"<p>Understanding wireframing involves recognizing the varying levels of detail, often called fidelity. Each fidelity level offers unique benefits and serves specific purposes during the design and development process.</p>"},{"location":"resources/project-management/wireframes/#low-fidelity-lo-fi-wireframes","title":"Low-Fidelity (Lo-Fi) Wireframes","text":"<p>Low-fidelity wireframes are simple, rough sketches typically drawn by hand using markers, sharpies, or whiteboards. These wireframes focus purely on representing the general layout and core interactions of the interface without attention to visual details or styling. The strength of low-fidelity wireframes lies in their simplicity and speed\u2014they can be rapidly created and easily modified, making them ideal for initial brainstorming and team discussions. These early-stage wireframes facilitate quick concept validation and help teams collaboratively identify high-level design concerns before significant resources are committed. However, because these sketches lack detail, they may not effectively communicate intricate functionalities or aesthetics to stakeholders unfamiliar with the project.</p>"},{"location":"resources/project-management/wireframes/#mid-fidelity-mid-fi-wireframes","title":"Mid-Fidelity (Mid-Fi) Wireframes","text":"<p>Mid-fidelity wireframes introduce greater accuracy and detail by using digital tools like Balsamiq or Figma. They define layouts and interaction states more precisely than low-fidelity sketches and often include basic color schemes and placeholder content. Mid-fi wireframes strike a balance, providing sufficient detail for teams to discuss and validate functionality and usability without investing excessive time in visual refinement. They serve well in preliminary usability testing and internal reviews, clearly conveying navigation and user flow. Despite their increased clarity, mid-fi wireframes still might lack the detailed visual polish needed for definitive stakeholder approval or rigorous usability assessments.</p>"},{"location":"resources/project-management/wireframes/#high-fidelity-hi-fi-wireframes-prototypes","title":"High-Fidelity (Hi-Fi) Wireframes &amp; Prototypes","text":"<p>High-fidelity wireframes and prototypes closely resemble the final product in appearance and interaction, typically developed using advanced prototyping software such as Figma, Adobe XD, or Sketch. These prototypes incorporate realistic visual elements, including exact colors, typography, images, and brand identity, along with interactive components and transitions that simulate real-world interactions. High-fidelity prototypes are especially valuable for gaining precise feedback from stakeholders, performing formal usability testing, and ensuring that the final vision of the design is clearly communicated to development teams. However, the primary trade-off with high-fidelity prototypes is that they are more time-consuming and resource-intensive to create. Introducing them too early can unintentionally constrain creative exploration and lock in design decisions prematurely.</p>"},{"location":"resources/project-management/wireframes/#comparison-of-wireframe-fidelity-levels","title":"Comparison of Wireframe Fidelity Levels","text":"Fidelity Level Pros Cons Typical Uses Time/Cost to Produce Low-Fidelity (Lo-Fi) Fast, inexpensive, easy to modify Lacks detail, can be unclear for stakeholders Initial brainstorming, early internal discussions Low time, low cost Mid-Fidelity (Mid-Fi) Clearer detail, balances speed and accuracy Still lacks final polish, not ideal for final approvals Preliminary usability tests, internal reviews Moderate time, moderate cost High-Fidelity (Hi-Fi) Realistic appearance, precise feedback Time-consuming, expensive, limits early exploration Client presentations, formal usability testing, development documentation High time, high cost"},{"location":"resources/project-management/wireframes/#cost-efficiency-and-design-exploration","title":"Cost Efficiency and Design Exploration","text":"<p>Wireframing is also an economically efficient practice within project management. It empowers teams to experiment with multiple design approaches before committing to development, minimizing risks by uncovering issues early. Because wireframes have minimal overhead, they allow teams to iteratively refine and optimize the user experience without the high cost associated with reworking actual code. Ultimately, wireframing supports teams in balancing ideal user experiences with practical limitations, ensuring the end product is both innovative and realistically achievable.</p>"},{"location":"resources/project-management/wireframes/#wireframing-challenges-with-llm-chat-ai","title":"Wireframing Challenges with LLM &amp; Chat-AI","text":"<p>The emergence of Large Language Models (LLMs) and AI-driven conversational interfaces presents new challenges for traditional wireframing practices. These interfaces are inherently textual and conversational, making it difficult to capture nuanced interactions through visual wireframes alone. In these scenarios, interaction scripts and detailed textual scenarios become more valuable, clearly specifying conversational flows and anticipated user interactions. While wireframes remain useful, relying exclusively on them is insufficient when dealing with sophisticated, dialogue-heavy experiences.</p>"},{"location":"resources/project-management/wireframes/#key-takeaways","title":"Key Takeaways","text":"<p>Wireframing is an essential, cost-effective practice that supports clear communication, thorough exploration, and thoughtful validation of user interfaces and experiences. By progressing through different fidelity levels\u2014from simple sketches to highly detailed prototypes\u2014teams can efficiently identify and solve design problems, build empathy for users, and manage stakeholder expectations. Incorporating wireframing early and continuously into your design process greatly enhances the likelihood of delivering intuitive, user-centered products.</p>"},{"location":"tasks/tk00_devenv/","title":"Docker, VSCode/DevContainers, Copilot, and GitHub Education Setup","text":"<p>This course will depend upon a few powerful, industry tools to help us ensure a consistent development environment for everyone in the course.</p> <p>You will need to install:</p> <ol> <li>Docker Desktop (be sure you can successfully run the command <code>docker run hello-world</code> in your terminal)</li> <li>Visual Studio Code</li> <li>The DevContainer Extension for VSCode (Update your VSCode if you have not done so recently!)</li> <li>The Github Copilot Extension for VSCode (May be installed automatically in recent versions of VSCode)</li> </ol> <p>Additionally, you need to register for GitHub Student benefits to give you free AI coding assistant credits and other benefits. You can find the sign-up here: https://education.github.com/pack. Please note that the ID verification can be a little finicky. These are some steps that recently worked for one of our TAs:</p> <ol> <li>I found a dark background to put my ID up against (it failed when I was holding it in my hand)</li> <li>I taped my student ID to the dark background and held it up close to my computer </li> <li>I got approved in like 30 seconds</li> </ol> <p>Finally, you should update your GitHub Profile to include your real name and, ideally, a representative profile photo of yourself. This will help the COMP423 team, and your team mates, identify your profile. Further, it is common for recruiters and software engineering teams to review candidate profiles when interviewing. Treating your GitHub profile with some professionalism will serve you well.</p> <p>After you have completed your profile and have successfully signed up for the GitHub Student Developer pack, go back to VS Code and be sure you are logged into your GitHub Account. There is a little icon in the bottom left corner of VSCode with a person it it you can click to see what profile you are logged in with.</p> <p>If you have troubles, come see us during drop-in office hours.</p> <p>To test that Copilot is successfully installed, open your command palette, search for \"Chat: Open Chat\". Change the mode in the chat (first drop down) to \"Ask\" and select the model (second drop down) \"GPT-5 mini\". Try asking \"What are your capabilities in VSCode?\" Upload a screenshot of your copilot chat's response to the Gradescope assignment.</p>"},{"location":"tasks/tk01_write_adr/","title":"Task 01 - Write ADRs for Package Managers","text":""},{"location":"tasks/tk01_write_adr/#prelude","title":"Prelude","text":"<p>In an alternate timeline, you applied for job with Synercast.io. One of about one hundred applications you submitted. They followed up!</p> <p>From the Desk of Chad / Director of Talent Velocity &amp; Strategic People Outcomes</p> <p>Dear Future Synercaster,</p> <p>First of all: congratulations. Not on getting the job \u2014 not yet \u2014 but on being noticed. That alone puts you in what I like to call our Top-of-Funnel of Destiny.</p> <p>At Synercast.ai, we don\u2019t just hire people. We curate potential. We believe in auditioning for excellence, pressure-testing curiosity, and discovering fit through what we proudly call:</p> <p>The Pre-Hire Learning Engagement\u2122</p> <p>This is not an interview.</p> <p>This is not unpaid labor.</p> <p>This is not not unpaid labor.</p> <p>This is an opportunity.</p> <p>You\u2019ve been invited to complete a short architectural thinking exercise that mirrors the kinds of decisions our engineers face early, often, and occasionally five minutes before a meeting that definitely could have been an email.</p> <p>You will not be told the \u201cright\u201d answer. You will not be judged on trivia. We care far more about how you think than what you pick.</p> <p>The detailed instructions live in the project memo that follows. It contains more nuance and technical specificity than I\u2019m qualified to explain. I skimmed it, nodded a lot, and approved it.</p> <p>Take this seriously. Don\u2019t over-optimize. Write like someone else will have to live with your decision. Because they will. Possibly you.</p> <p>Proud of you, already, in a professional way.</p> <p>Synergistically yours,</p> <p>Chad</p> <p>Director of Talent Velocity &amp; Strategic People Outcomes / Synercast.ai</p>"},{"location":"tasks/tk01_write_adr/#pre-requisite-reading","title":"Pre-requisite Reading","text":"<p>You should read RD05 - Documenting Architecture Decisions and complete the guided reading questions in Gradescope before starting with this task.</p>"},{"location":"tasks/tk01_write_adr/#context","title":"Context","text":"<p>Modern software projects rely heavily on package managers to handle dependencies, environments, and reproducibility. In Python, this space has evolved rapidly, and multiple tools coexist with different philosophies and trade-offs.</p> <p>In this assignment, you will explore two modern Python package manager strategies: Poetry and uv. You will practice making decisions under uncertainty.</p> <p>Rather than \"picking the right tool,\" the goal is to:</p> <ul> <li>understand why teams might choose one approach over another,</li> <li>practice reading real documentation,</li> <li>engage an LLM as a learning tool with curiosity, not with laziness,</li> <li>and document decisions using Architectural Design Records (ADRs).</li> </ul> <p>You will ultimately submit two ADRs:</p> <ul> <li>one that justifies choosing Poetry</li> <li>one that justifies choosing uv</li> </ul> <p>Each assuming a plausible but different project context.</p>"},{"location":"tasks/tk01_write_adr/#learning-objectives","title":"Learning Objectives","text":"<p>By completing this assignment, you will gain experience with:</p> <ol> <li>Writing Architectural Design Records (ADRs).</li> <li>Reading and extracting meaning from open-source documentation.</li> <li>Understanding the role of package managers in Python projects.</li> <li>Using an LLM to learn unfamiliar technical concepts and clarify trade-offs.</li> <li>Reasoning about diverging project paths where multiple choices are defensible.</li> </ol>"},{"location":"tasks/tk01_write_adr/#your-task","title":"Your Task","text":""},{"location":"tasks/tk01_write_adr/#part-1-learn-the-landscape-research-exploration","title":"Part 1: Learn the Landscape (Research &amp; Exploration)","text":"<p>You should develop a working understanding of:</p> <ul> <li>What problems Python package managers aim to solve</li> <li>How Poetry and uv approach those problems differently</li> <li>What kinds of teams or projects might prefer each approach</li> </ul> <p>You are encouraged to:</p> <ul> <li>Read official documentation</li> <li>Skim README files and \"Getting Started\" guides</li> <li>Use an LLM to ask clarifying questions, generate summaries, and test your understanding</li> </ul> <p>\u26a0\ufe0f Important: If you use an LLM, you are still responsible for verifying claims against documentation. Treat the LLM with a healthy dose of skepticism, not an absolute authority.</p>"},{"location":"tasks/tk01_write_adr/#guided-questions-you-should-answer-before-writing-adrs","title":"Guided Questions You Should Answer Before Writing ADRs","text":"<p>You will not submit responses to these questions, but you should be able to answer the following questions in your own words before writing the ADRs. These questions are designed to guide your research into learning about package managers and poetry/uv.</p> <ol> <li>What core problems do package managers solve in a Python project?</li> <li>Why is dependency versioning essential for reproducibility?</li> <li>What risks or symptoms arise when dependencies are not managed consistently across environments?</li> <li>How does a lockfile differ from a dependency specification file, and why does that distinction matter?</li> <li>What is the purpose of a virtual environment in Python?</li> <li>How do environment isolation and dependency management relate, but differ?</li> <li>What challenges arise when multiple Python projects coexist on the same system?</li> <li>How does Poetry encourage reproducible builds, and what trade\u2011offs does that approach introduce?</li> <li>What problem space is uv primarily trying to optimize for, and how does its approach differ from all\u2011in\u2011one tools like Poetry?</li> </ol>"},{"location":"tasks/tk01_write_adr/#part-2-write-two-adrs-decision-documentation","title":"Part 2: Write Two ADRs (Decision Documentation)","text":"<p>You will write two short ADRs, each making a different decision:</p> <ul> <li>ADR A: Decide in favor of Poetry</li> <li>ADR B: Decide in favor of uv</li> </ul> <p>Each ADR should:</p> <ul> <li> <p>Assume a reasonable but distinct project context</p> </li> <li> <p>(e.g., a student team project, a production service, a fast-moving prototype, a research codebase)</p> </li> <li>Explicitly acknowledge trade-offs</li> <li>Be internally consistent and defensible</li> </ul> <p>The goal is not to argue which tool is \"better,\" but to show that both choices can be correct under different constraints.</p>"},{"location":"tasks/tk01_write_adr/#example-adr-for-a-devcontainer","title":"Example ADR for a DevContainer","text":"<p>To give you a sense of what a reasonable ADR might look like for another similar decision, here is an example, representative ADR that supports the decision to use DevContainers for a reproducible development environment:</p> <pre><code># ADR00: Standardize Python DevContainer\n\n## Context\n\nThis project needs a predictable and reproducible development\nenvironment.\n\nTeam members have machines capable of running Docker containers, and\nwe want a development setup that can also be used in hosted or\ncloud-based development environments.\n\nSeveral forces are in tension:\n\n- We want onboarding to be fast and reliable for new contributors.\n- We want local development to match CI closely to reduce\n  environment-specific failures.\n- We want to minimize maintenance work for the team over time.\n\n## Decision\n\nWe will use VS Code Dev Containers with a standard,\nMicrosoft-maintained Python base image.\n\nWe will keep the DevContainer definition lightweight by preferring\nconfiguration over customization and only add project-specific tooling\nwhen it is necessary.\n\nWe will use a standard Python .gitignore so temporary files, build\nartifacts, local environments, and secrets are not committed.\n\n## Considered Options\n\n- We could run directly on the host OS using a `venv` (or similar).\n  This reduces container overhead but increases variability across\n  developer machines.\n- We could build and maintain a custom Docker image. This gives more\n  control but increases the maintenance burden.\n\n## Consequences\n\n- New contributors can start development by opening the repository in\n  he container, which reduces setup steps and configuration drift.\n- Development and CI are more likely to use the same tooling\n  versions, which can reduce \u201cworks on my machine\u201d failures.\n- The project depends on Docker and VS Code Dev Containers, which may\n  add friction for developers who cannot or prefer not to run\n  containers.\n- Running in a container can reduce performance compared to running\n  irectly on the host OS, especially for file-heavy workflows.\n- Using an off-the-shelf base image reduces ongoing maintenance but\n  limits customization to what the base image supports.\n</code></pre>"},{"location":"tasks/tk01_write_adr/#adr-expectations","title":"ADR Expectations","text":"<p>Each ADR should be ~\u00bd to 1 page and include the following sections:</p> <ol> <li>Title</li> <li>Context<ul> <li>What kind of project is this?</li> <li>What constraints or priorities matter?</li> </ul> </li> <li>Decision<ul> <li>What tool is being chosen and why?</li> </ul> </li> <li>Considered Options<ul> <li>What alternatives were evaluated?</li> </ul> </li> <li>Consequences<ul> <li>What are the benefits of this choice?</li> <li>What are the costs or risks?</li> <li>What might be harder because of this decision?</li> </ul> </li> </ol> <p>You may reference:</p> <ul> <li>Documentation</li> <li>Reasoning informed by LLM conversations</li> <li>Comparisons to alternative approaches</li> </ul>"},{"location":"tasks/tk01_write_adr/#hand-in-on-gradescope","title":"Hand-in on Gradescope","text":"<p>You will submit your two written ADRs for this Task on Gradescope.</p>"},{"location":"tasks/tk02_dev_tools/","title":"Implement a Dependency Manager ADR with an Agent","text":""},{"location":"tasks/tk02_dev_tools/#prelude","title":"Prelude","text":"<p>In the alternate timeline, you previously submitted ADRs to Synercast.io. Chad selected you for the job!</p> <p>Subject: Welcome + First Engineering Task</p> <p>Internal Memo: Welcome to SynerCast.ai!!!</p> <p>From: Chadwick P. Ledger, CEO (HBS)</p> <p>To: Engineering (You)</p> <p>Subject: Welcome + First Engineering Task</p> <p>Hi and welcome!!!</p> <p>First off, congratulations on joining SynerCast.ai, one of the most exciting and futuristic companies in the software space right now. Our leadership team is made up almost entirely of Harvard Business School graduates, which means we think very strategically and also very big picture.</p> <p>Earlier this year, we made the bold decision to offshore engineering to the cloud. The idea was simple: if AI can write code faster than people, why slow things down with engineers? This freed us up to focus on what really matters\u2014vision, alignment, and pitch decks.</p> <p>Honestly, for a while, it was going great.</p> <p>Recently, though, we\u2019ve noticed a few small things:</p> <ul> <li>The app only runs on my laptop.</li> <li>Sometimes it runs, but the weather is for a different state.</li> <li>Fixing one bug often creates a different bug, which we assume is just how software works now.</li> <li>Nobody is totally sure which files are \"the real ones.\"</li> </ul> <p>So anyway, welcome! You are now the Engineering Department.</p> <p>To keep things simple, we are starting small. Your first task is to help us \"professionalize\" how we manage dependencies.</p> <p>Right now, installing the project is kind of vibes-based.</p> <p>We would like you to:</p> <ul> <li>Decide on a proper Python package manager for the project.</li> <li>Use the short Architecture Decision Record (ADR) you wrote in your application explaining that choice.</li> <li>Implement the ADR you wrote yourself, by offshoring whatever you need to the cloud.</li> </ul> <p>We\u2019ve been told by several very confident people on the internet that all the coolest new companies in Silicon Valley are using something called \"uv\", so that is probably the right answer. That said, we still need something written down that explains why we\u2019re doing it this way, in case an investor asks.</p> <p>You do not need to add features or redesign anything. We just want something we can point to and say, \"Yes, we have an engineering process now.\"</p> <p>If you could commit that ADR and make sure it says we chose uv, that would be perfect.</p> <p>Thanks, Chad CEO, SynerCast.ai</p>"},{"location":"tasks/tk02_dev_tools/#getting-started","title":"Getting Started","text":"<p>Get started by accepting the following GitHub Classroom Assignment: https://classroom.github.com/a/3UMeYIjj</p> <p>Accept Invitation to Join COMP423 Spring 2026 Organization</p> <p>After attempting to accept this first assignment, you may need to accept an invitation  to join our course organization. </p> <p>To do so, in a different tab, log in to Github, navigate to https://github.com/settings/organizations and look for an invitation to join the organization.</p> <p>Once your GitHub repository is created, you will need to clone it to your developer machine.</p> <p>Open the project directory in VSCode. It should prompt you to reopen the project in a Dev Container. Accept the suggestion and wait for the dev container to download and install. Once it has, you should see \"Dev Container: ...\" in the bottom left corner of VSCode. All work in this course will be completed in Dev Containers.</p>"},{"location":"tasks/tk02_dev_tools/#commit-branch-and-merge-expectations-of-this-task","title":"Commit, Branch, and Merge Expectations of this Task","text":"<p>A significant portion of this task's grade will be contingent upon following these <code>git</code> conventions.</p> <ol> <li> <p>You must create a branch named <code>adr001</code> and complete your work in that branch. Go ahead and do so. How can you verify you are on the correct branch?</p> </li> <li> <p>You must commit your ADR, following guidelines below, in a single commit on your <code>adr001</code> branch with a commit message of <code>doc: add adr001 use uv for dependency management</code>. You can see an example of this for the included ADR000 here.</p> </li> <li> <p>After reaching a point of generating configuration with an agent, reviewing it, and verifying its correctness, you will create a commit with message <code>feat: implement uv per adr001</code> with a commit body that contains the agentic model you used as well as the prompt. If additional prompting is needed, document your understanding of what was wrong or unsatisfactory along with the subsequent prompt. All prompts and reflections should be included in the commit message. If you find more than 5 iterations are needed, you are encouraged to give up on this attempt (reset the commit) and try to improve your original prompt or try a different underlying model. For an example of what such a commit might look like, you can see the commit for feature ADR000 here.</p> </li> <li> <p>Once you are ready to merge your branch, you should force a merge commit so that the history of this branch is clearly maintained. To do so, switch to <code>main</code> branch and merge your feature branch with the <code>--no-ff</code> flag. You can use the default merge commit message. Here's an example of how this history looked in the started code.</p> </li> </ol>"},{"location":"tasks/tk02_dev_tools/#your-initial-task","title":"Your Initial Task","text":"<p>The project has no dependency manager in the dev container and a key dependency is missing! How can you know? Try running: <code>python3 src/main.py</code>. The library named <code>requests</code>, a popular Python library for making API calls, is not installed in Python's environment.</p> <p>When you begin work on a new concept: branch. Go ahead and make the new branch described in Commit Expectation #1 above to begin working on <code>adr001</code>. You will add your written ADR from TK01 (in markdown), the one where you decided on <code>uv</code>, as an appropriately named markdown file in the <code>docs/arch</code> directory. Once you have created your ADR file and saved it, go ahead and make the commit as described in Commit Expectation #2 above.</p> <p>After you've made that commit, your task is to setup this project with <code>uv</code> such that when a new dev container is created (or rebuilt) the <code>uv</code> package manager is installed in the dev container, configured, and dependencies are <code>sync</code>'ed (\"synchronized\" or installed). Since installing and configuring a dependency manager in a dev container contains some non-critical configuration knowledge, it is a reasonable task to work with the GitHub Copilot Agentic AI on. Based on the strategies discussed in LS02, you should open a new agent prompt for this task. </p> <p>Don't forget to claim Copilot Pro</p> <p>If you run into trouble prompting Copilot within VSCode, make sure you've claimed your Copilot Pro coupon! This is a separate step from accepting Github's Student Developer Pack. To claim your coupon, head to GitHub and navigate to your profile &gt; Settings &gt; Billing and Licensing &gt; Education Benefits and follow the instructions, then restart VSCode for the changes to take effect.</p> <p>Try crafting an agentic prompt that includes the following context:</p> <ol> <li>Describes what you want the agent to do for you (hint: it's implementing your Architectural Design Record)</li> <li>Include your ADR001 file as input context explicitly, by referencing it with the <code>#</code> (hashtag) file search feature</li> <li>Some useful hints to consider using in your prompt:<ul> <li>Less is more when it comes to configuration, use adjectives to help describe \"minimal\", \"simple\", and \"well documented\"</li> <li>Keep the focus of the task small: \"Do not add any <code>uv</code> dependencies yet.\"</li> <li>Indicate your level of familiarity with <code>uv</code> and ask it to walk you through its changes</li> <li>Use a state of the art model like Claude Sonnet 4.5 or GPT 5.2</li> </ul> </li> </ol> <p>Do not rebuild your container until your agent task completes</p> <p>The agent should modify your project's <code>./devcontainer/devcontainer.json</code> file. As soon as it does, before the task actually completes, VS Code will surface a pop-up asking you to rebuild the container. Ignore it! You will be able to see that the agent task is still working through completing its attempt.</p> <p>When the changes complete, it is your job to evaluate what was changed and understand the lines of code added to your project in the agent's first attempt.</p> <p>You don't need a deep understanding of each change, but you should have a general understanding of what each change does. If you do not: it's a golden opportunity to learn! Rather than muddying the context of your agentic chat (and spending premium model requests...), you are encouraged to explore through web searches or a separate AI chat in the web browser.</p>"},{"location":"tasks/tk02_dev_tools/#verify-it-works-with-acceptance-tests","title":"Verify it Works with Acceptance Tests","text":"<p>Once you believe you have a general sense of the files and changes made, how will you know they actually work? By verifying them and proving to yourself they work.</p> <p>Developing a habit of asking yourself, \"how can I be confident my changes work,\" and testing your hypotheses, is fundamental to being a successful software engineer. You need to be confident your changes to a project work before submitting them for code review or to production. This was true before AI and even more true post-AI.</p> <p>Since you are coming up to speed on <code>uv</code>, we provide some manual acceptance tests for you to move through in your dev container should be able to complete.</p> <p>What to do when you encounter errors</p> <p>Does a step in your verification process produce errors? That's a part of the process! First: deep breath.</p> <p>Begin by trying to make sense of the error output. This is another great place where modern AI tools can explain cryptic error messages and possible solutions. There is often something small and manually actionable once you understand the error.</p> <p>If you think you are in too big of a mess to fix and want to start over, perhaps with a more refined prompt or model, this is why we use git! Use <code>git status</code> to see the files changed. You can reset the changes to existing files and delete new files to try again. Start a new context for the agent (+ icon) if you start over.</p> <p>First, try rebuilding your dev container via VS Code Command Palette \"Dev Containers: Rebuild Container\". Does the container successfully rebuild? This is the first and most critical acceptance test.</p> <p>Now that your dev container successfully builds, check to see if <code>uv</code> is installed in your shell. Open a new terminal in the dev container and try both: <code>which uv</code> and <code>uv help</code> and you should see output.</p> <p>Now, let's test adding a dependency via <code>uv</code>. Before doing so, to understand what happens when a dependency is added, you should go ahead and open up your <code>pyproject.toml</code> file and look for its <code>dependencies</code> configuration variable. It should be empty (if it's not empty because a clever agent added <code>requests</code> for you, try <code>uv remove requests</code> first). Back in your terminal, try <code>uv add requests</code>. Notice the output in terminal and the changes in your <code>pyproject.toml</code> file. The top-level requests dependency was added to <code>pyproject.toml</code> and sub-dependencies were added to <code>uv.lock</code> (if you open it). All were installed. If you try running <code>uv sync</code> now, you will see that everything is installed.</p> <p>Now, let's try running the program with <code>uv</code>'s managed environment: <code>uv run python src/main.py</code>. You should see the glorious Synercast.ai weather status for Chapel Hill printed. (If you see an error message regarding a timeout, that's an error on the server-side and not your fault. Try running again.)</p> <p>Finally, if you look in the hidden <code>.venv</code> directory in your project, and the <code>lib64</code> directory within it, you will see the <code>requests</code> package. You can see the source code for <code>requests</code> within, starting from the <code>__init__.py</code> module. The <code>.venv</code> directory is where all of your projects dependencies are installed. When you run <code>uv run python</code> it ensures Python's \"environment\" includes your project's <code>.venv</code> directory with top priority so that it uses these packages before any operating system or default packages.</p> <p>How to add .venv to your path to avoid needing to use <code>uv run</code></p> <p>If you try running <code>python run src/main.py</code> you will see that the import error still exists. This is because your shell is not using the project's <code>.venv</code> directory in its <code>PYTHONPATH</code> so Python does not know to look in <code>.venv</code> to load dependencies first.</p> <p>The common way to fix this is to \"activate\" the virtual environment in a shell. To do so, use the <code>uv venv</code> command and accept the replacement of your <code>.venv</code>. Since it blew up your <code>.venv</code> directory, will need to resync: <code>uv sync</code>. Then, try ending this terminal session (trash can) and starting a new one. You should see a first line of <code>source ...path.../.venv/bin/activate</code> which is a script that registers your venv in your shell environment. You should also notice the project name as a prefix to the shell prompt. Now try running <code>python src/main.py</code> and you will see the script works without <code>uv run</code>.</p> <p>This is common to other dependency managers that make use of virtual environments, including Poetry.</p> <p>Great work! Now all that is left is to make your commit per Commit Requirement #3 and merge without fast-forwarding per Commit Requirement #4. Then you'll want to push your changes to GitHub and submit your work to TK02 on GitHub.</p>"},{"location":"tasks/tk02_dev_tools/#questions-to-be-able-to-answer-following-this-task","title":"Questions to be able to answer following this task:","text":"<ul> <li>Where do my dependencies live in the directory structure for a project with a Python virtual environment?</li> <li>What file states what my project directly depends on?</li> <li>What file pins the exact versions of direct and indirect dependencies in a <code>uv</code> project?</li> <li>What command ensures my installed dependencies exactly match the project's pinned dependencies?</li> <li>How do I add a new dependency to a <code>uv</code> project? How do I remove it?</li> <li>How do I run commands so I\u2019m guaranteed to they use the <code>uv</code> environment?</li> </ul>"},{"location":"tasks/tk04_more_dev_tools/","title":"Professionalizing the Developer Environment","text":""},{"location":"tasks/tk04_more_dev_tools/#prelude","title":"Prelude","text":"<p>In your alternate timeline, you received the following email.</p> <p>Internal Memo: The Company Has Discovered \"Standards\"</p> <p>From: Chadwick P. Ledger, CEO (HBS)</p> <p>To: Engineering (You)</p> <p>Subject: Congratulations, You Now Own \"Quality\"</p> <p>Hi and welcome back!!!</p> <p>First off: outstanding work on the last task. I personally watched the terminal print weather for Chapel Hill, and I immediately forwarded a screenshot to three investors with the caption: \"We are basically profitable.\" This is what we at SynerCast.ai call \"traction\" and \"validation\". That \"timeout\" was definitely a fluke.</p> <p>Now that we have solved dependency management, leadership is ready to take the next strategic leap forward: we are professionalizing. I know it sounds intense, but please remember: professionalism is just vibes, except you can run it in the command line.</p> <p>Here\u2019s the situation. We\u2019ve noticed an alarming trend where code is being written in multiple styles, bugs are being introduced without any formal announcement, and the app sometimes works only if you believe in it hard enough. This is a risk to our brand, our roadmap, and most importantly, Chad\u2019s ability (me) to demo things live without sweating through a quarter-zip.</p> <p>So your next mission is simple: add the kind of automated guardrails that make a project feel like it belongs to a real company. Think: consistent formatting, helpful warnings before mistakes become folklore, and a way to prove things work that doesn\u2019t involve \"it ran once on my machine.\"</p> <p>Thanks in advance.</p> <p>Warmly,</p> <p>Chadwick P. Ledger, CEO (HBS)</p> <p>SynerCast.ai</p>"},{"location":"tasks/tk04_more_dev_tools/#1-overview","title":"1. Overview","text":"<p>In Task 2, you established the foundation of your project by selecting <code>uv</code> as your dependency manager and automatically installing it in your Dev Container. </p> <p>For this task, we are moving from a \"functioning\" project to a \"professional\" one. You will extend your developer tooling to include automated guardrails that ensure code quality, consistency, and correctness. You will again write Architectural Design Records (ADRs) in your own words to justify your choices and use Generative AI Agents (GitHub Copilot) to implement them following a specific <code>git</code> workflow.</p> <p>Your work in this task will follow your work in Task 2 and reuse the same git repository and dev container.</p>"},{"location":"tasks/tk04_more_dev_tools/#2-technical-aside-dev-vs-production-dependencies","title":"2. Technical Aside: Dev vs. Production Dependencies","text":"<p>In modern projects and dependency managers, it is critical to distinguish between what your app needs to run and what you need to build it or actively work on it.</p> <ul> <li>Production Dependencies: Packages required for the code to run in the hands of a user (e.g., <code>requests</code>).</li> <li>Development Dependencies: Tools used only during the development process (e.g., <code>pytest</code>).</li> </ul> <p>Why it matters: Including testing tools in a production environment increases the size of your deployment and creates unnecessary security \"attack surfaces.\" </p> <p>Action: When adding these tools, always use the <code>--dev</code> flag: <code>uv add --dev &lt;package_name&gt;</code></p> <p>After doing so, take a look in your <code>pyproject.toml</code> file to see how development dependencies are specified.</p> <p>All of the dependencies added in this task are development dependencies and should be installed as such.</p>"},{"location":"tasks/tk04_more_dev_tools/#3-the-quality-stack-requirements","title":"3. The Quality Stack Requirements","text":"<p>For each of the following three categories of tools, you will establish a new branch, perform some research, write an ADR with a specific commit, and then implement the ADR following another commit, verify your solution works, then merge your work back into main with a merge commit. Whereas in Task 2 we were lenient on this workflow, in this task we will expect comfort with the workflow and it will be a substantive portion of this task's grade.</p>"},{"location":"tasks/tk04_more_dev_tools/#4-the-strict-git-workflow","title":"4. The Strict Git Workflow","text":"<p>We were lenient on Git discipline in Task 2. For this task, the workflow is a core part of your grade.</p> <ol> <li>Create a Feature Branch: Branch off <code>main</code> (e.g., <code>adr002</code>).</li> <li>Research &amp; Commit ADRs: Decide on your tools. Write your ADRs in <code>docs/arch/</code>. You must commit these ADRs to your branch before writing any implementation code with a <code>doc:</code> prefix.</li> <li>Engage the Agent: Use GitHub Copilot (Agent Mode). Provide the ADRs as context and ask the agent to:<ul> <li>Install the tools as Development Dependencies using <code>uv</code>.</li> <li>Configure the devcontainer (<code>.devcontainer/devcontainer.json</code>) to enable the tools.</li> <li>Produce as simple and straightforward of an implementation as possible, like a senior software engineer.</li> </ul> </li> <li>Verification: Perform the manual and automated checks. When you face failures, you will need to investigate and learn where they are coming from.</li> <li>Committing Feature Work: Once your work is verified, you will create a <code>feat:</code> commit like in Task 02 that contains the prompts and models you used, as well as any insights or commentary on what you noticed when astray.</li> <li>The Intentional Merge: Merge your branch back into <code>main</code> using an explicit merge commit:     <pre><code>git merge --no-ff adr002\n</code></pre> We will be checking your Git graph for the \"diamond\" shape created by this merge.</li> </ol>"},{"location":"tasks/tk04_more_dev_tools/#a-adr002-source-code-formatting-linting","title":"A. ADR002 Source Code Formatting &amp; Linting","text":"<p>Before writing your ADR, you should read an overview of each of the tools and be able to answer the following questions:</p> <ul> <li>What is a code formatter?</li> <li>What is a linter?</li> <li>What value does each uniquely provide to a software engineering team?</li> <li>What plugin(s) exist in VSCode to automatically run these tools on your files when you save them?</li> </ul> <p>Tools to investigate and choose from:</p> <ol> <li><code>black</code> and <code>pylint</code></li> <li><code>ruff</code></li> </ol> <p>For this <code>git</code> branch, name it <code>adr002</code>. Add your second ADR in the <code>docs/arch</code> directory. Make a commit with <code>doc: ...</code> as the prefix and complete the commit line with appropriate description (replacing <code>...</code> with a meaningful title). When you get to implementing the ADR, setting up your project with these tools, you will ultimately produce a <code>feat: ...</code> commit with a descriptive subject line and a commit body that includes the models and prompts you used, as well as the iterations it took, as was expected in Task 02. This time it is on you to verify it works with acceptance tests. The things you should verify before merging, and continue iterating on until you can, are:</p> <ol> <li>Your tool(s) work in the command-line</li> <li>Your tool(s) are automatically run in VSCode when you save a file. For this verification, you have the burden of figuring out how to convince yourself it is working. This may require a VSCode plugin and some configuration which can be installed and configured in the devcontainer via your devcontainer.json file.</li> <li>Both of the above are verifiable after rebuilding your devcontainer.</li> </ol> <p>Finally, update your project's <code>README.md</code> to have a section that covers \"Formatting and Linting\" with instructions on how to run the tools. This is a task I would encourage experimenting with Agentic AI help on... but remember that you own output's correctness and readability!</p> <p>Once you have made your <code>feat:</code> commit with a verified implementation, merge it into your <code>main</code> branch with a merge commit.</p>"},{"location":"tasks/tk04_more_dev_tools/#b-adr003-static-type-checking","title":"B. ADR003 Static Type Checking","text":"<p>Before writing your ADR, you should read an overview of each of the tools and be able to answer the following questions:</p> <ul> <li>What is a static type checker?</li> <li>Why is a static type checker useful in Python?</li> <li>Why don't you need a separate tool for this in a language like Java?</li> <li>What plugin(s) exist in VSCode to automatically run these tools and highlight type checking errors in your editor?</li> </ul> <p>Tools to investigate and choose from:</p> <ol> <li><code>mypy</code></li> <li><code>pyright</code></li> </ol> <p>For this <code>git</code> workflow, follow the same naming conventions, standards, and expectations as the previous ADR. It will be on you to determine how to verify the tool you choose works in your devcontainer and vscode IDE integration. We will ask you to be able to prove this verification in the submission for this task.</p>"},{"location":"tasks/tk04_more_dev_tools/#c-adr004-testing-coverage","title":"C. ADR004 Testing &amp; Coverage","text":"<p>Before writing your ADR, you should read an overview of each of the tools and be able to answer the following questions:</p> <ul> <li>How much boilerplate (\"ceremony\") is required to write a test in this framework?</li> <li>What support does this testing framework have for extensions (like code coverage)?</li> <li>What support does this testing framework have for fixtures? What are the developer ergonomics?</li> </ul> <p>Tools to investigate and choose from:</p> <ol> <li><code>pytest</code> and the <code>pytest-cov</code> plugin</li> <li><code>unittest</code> combined with <code>coverage.py</code></li> </ol> <p>For this <code>git</code> workflow, follow the same conventions as earlier ADRs.</p> <p>To verify that your chosen tools are working in your dev container, you will need to add at least a basic, example test file to the repository. An agent can be helpful here, but use any agentic code to understand how it comes together! It's OK if it does not have 100% coverage of <code>src/main.py</code>, the important quality is that the tests run and you can see some coverage report of <code>main.py</code>.</p> <p>To verify your chosen tools work in VSCode, you should be able to run your tests from the VSCode testing pane.</p>"},{"location":"tasks/tk04_more_dev_tools/#6-guiding-questions-for-exploration","title":"6. Guiding Questions for Exploration","text":"<p>Use these questions to help formulate the \"Forces\" and \"Decision\" sections of your ADRs:</p> <ol> <li>Unified vs. Modular: Does a single tool reduce configuration \"toil\" compared to using multiple specialized tools?</li> <li>Boilerplate: Compare the code needed to write a test in <code>unittest</code> versus <code>pytest</code>. Which feels more maintainable for a growing team?</li> <li>Developer Velocity: Does a faster type-checker like Pyright provide enough of a productivity boost to justify moving away from the industry-standard tool like mypy?</li> </ol>"},{"location":"tasks/tk06_api_design/","title":"TK06. API Design and Implementation (Part 1)","text":"<p>In this task you will design an API for a URL shortening service.</p>"},{"location":"tasks/tk06_api_design/#breakdown-of-parts","title":"Breakdown of Parts","text":"<p>Part 1. You only need to implement the route decorators and function signatures, NOT the actual implementations of the API.</p> <p>Part 2. You will implement the API. (Not yet released!)</p>"},{"location":"tasks/tk06_api_design/#app-overview-url-shortener","title":"App Overview: URL Shortener","text":"<p>In this lab, you and a partner will collaborate to design and implement a service that implements a URL-shortening API (submit a long URL and receive a short, redirectable URL).</p> <p>This application will be able to generate shortened URLs, similar to UNC's <code>go.unc.edu</code> service which you can try out today. Here is an example of a shortened URL: https://go.unc.edu/Xj9b6</p> <p>This application design will feature three personas:</p> <ol> <li> <p>Sue Sharer is someone who wants to distribute content easily. She might be a marketing manager sharing links to organization resources. Sue values convenience, control, and customization, which is why she wants options like vanity URLs and expiration times.</p> </li> <li> <p>Cai Clicker is the person who receives and opens links. They might be a colleague reviewing content or a prospective customer. Cai values seamlessness and reliability\u2014when they click a link, they expect to be redirected without delay.</p> </li> <li> <p>Amy Admin is responsible for monitoring and managing all active links. She is a community manager. Amy values visibility, control, and order, ensuring that shared content remains appropriate and awareness of high-traffic links.</p> </li> </ol>"},{"location":"tasks/tk06_api_design/#user-journey-examples","title":"User Journey Examples","text":"<p>An journey may combine a few user stories in order to give a complete start-to-finish example of a feature's use. Since you may not be familiar with the point of a URL shortener, consider these journeys before reading the user stories in a more standalone presentation.</p>"},{"location":"tasks/tk06_api_design/#sue-sharer-creates-a-shortened-url","title":"Sue Sharer Creates a Shortened URL","text":"<ol> <li>Sue wants to a share a link to the company website https://comp423-26s.github.io/.  </li> <li>She submits the long URL below and a vanity path of <code>comp423</code>: <pre><code>https://comp423-26s.github.io/\n</code></pre></li> <li>The system generates a link with the vanity path and responds with the information Sue needs to share the URL. </li> <li>Sue shares this link with her friend: <code>https://&lt;your-apps-hostname&gt;/comp423</code> </li> <li>Cai Clicker clicks the link and is redirected to: <pre><code>https://comp423-26s.github.io/\n</code></pre></li> </ol>"},{"location":"tasks/tk06_api_design/#required-user-stories","title":"Required User Stories","text":"<ol> <li>As Sue Sharer, I want to create a shortened URL with an optional expiration time and the ability to request a custom vanity URL, so that I can control how long it is available and share a more meaningful link.</li> <li>As Cai Clicker, I want to open a shortened URL by clicking its unique link, so that I am automatically redirected to the original long URL.</li> <li>Amy Admin<ol> <li>As Amy Admin, I want to see a list of all active resources (shortened URLs) and filter by view counts greater than some low threshold, so that I can oversee what content is currently being shared.  </li> <li>As Amy Admin, I want to see how many times each resource has been accessed, so that I can monitor usage and identify high-traffic resources.</li> <li>As Amy Admin, I want to change the target of a shortened URL, so that I can correct or modify existing resources when necessary.  </li> <li>As Amy Admin, I want to delete any active resource from the system, so that I can remove content that should no longer be available.  </li> </ol> </li> </ol>"},{"location":"tasks/tk06_api_design/#path-requirement-specifications","title":"Path Requirement Specifications","text":"<p>User story 2 above is the only story which we will very specificaly share an API requirement, as follows:</p> <p>The method is <code>GET</code> and the <code>path</code> in FastAPI route path syntax is <code>/{short_code}</code>.</p> <ul> <li>When a user accesses a vanity URL, the system must temporarily redirect to a shortened URL.</li> </ul>"},{"location":"tasks/tk06_api_design/#no-authentication-enforced","title":"No Authentication Enforced","text":"<p>The concerns of how to authenticate a user, like Amy Admin, and authorize various actions, is beyond your concern in this initial API design. You should proceed with all routes publicly available, unprotected. Later, we'll learn strategies for authenticating and authorizing various actions at the HTTP API level.</p>"},{"location":"tasks/tk06_api_design/#phase-1-api-design","title":"Phase 1: API Design","text":""},{"location":"tasks/tk06_api_design/#getting-started","title":"Getting Started","text":"<p>To begin work on TK06, you and your partner will need to decide on one of your TK05 implementations to serve as the starting point. Begin by comparing approaches to unit and integration testing in TK05. If either of you did not reach 100% coverage for unit and integration testing, choose the partner's who did. If the approaches appear equivalent, flip a coin or play a game of rock-paper-scissors to decide whose will be the starting point. For instructional purposes, we will call the team mate whose TK05 repository will be the starting point the starter.</p> <p>The way TK06's setup will work is that when the starter accepts the assignment through GitHub Classroom, they will set the team name of <code>team_tk06_NN</code> where <code>NN</code> is your team number from the pairings sheet. The initial repository will be empty. In the starter's <code>tk05</code> repository, the empty <code>tk06</code> repository will be added as a remote and <code>main</code> will be pushed to it. From here, both team mates will clone the <code>tk06</code> repository to use for this next exercise.</p> <p>Starter team mate's repository steps:</p> <ol> <li>The starter (teammate whose TK05 repository will seed TK06's starting point) will accept the Github Classroom assignment here: https://classroom.github.com/a/TZfqFme7<ol> <li>*Carefully fill in your team name: <code>team_tk06_NN</code> where <code>NN</code> is your team's number from the pairings sheet.</li> <li>You may need to view your GitHub invitations to accept the TK06 repository invitation in order to join the repository.</li> </ol> </li> <li>The starter will use their terminal (local) to navigate to TK05 and add their new team repository as a remote repository:<ol> <li><code>git remote add tk06 [insert-tk06-repo-url-here]</code> - Replace the entire <code>[insert]</code> substring, square brackets included, with your empty TK06 repository URL.</li> <li><code>git switch main</code> - If you are not already on the <code>main</code> branch.</li> <li><code>git push tk06 main</code> - Push the <code>main</code> branch of your <code>tk05</code> to <code>tk06</code>.</li> </ol> </li> <li>Refresh the TK06 repository on GitHub to confirm it is no longer empty and contains your <code>tk05</code> repository's history as its starting point.</li> </ol> <p>Seeding New Repositories</p> <p>As a team, reflect on what was just completed. It's very neat! You seeded a new, empty repository by pushing a branch from an existing repository to it.</p> <p>This is commonly useful in real world scenarios and should make sense that it is possible with your understanding of what it means to <code>push</code> a branch to a remote. The neat feature of this particular scenario is the remote repository was empty and you were able to target it for your push by establishing a new remote named <code>tk06</code>.</p> <p>Both team mate's next steps:</p> <ol> <li>(Pre-step) The team mate who was not the starter should go ahead and accept the GitHub classroom assignment and carefully select the correct team from the list of existing teams.</li> <li>Using a terminal, both of you should navigate to the directory on your host machine's storage where you clone course projects.</li> <li>Both team members should clone their <code>tk06</code> repository URL from GitHub using their host machine's terminal.</li> <li>Open the TK06 repository in VSCode, then reopen the repository in a Dev Container.</li> <li>Now you are ready to start!</li> </ol> <p>Errors starting the Dev Containers in Windows</p> <p>If you face a problem opening the DevContainer in Windows, it's likely due to a line ending issue. In VSCode, change your line ending setting for the project to be just LF not CRLF. You will see a setting for this in the bottom right corner. Change to <code>LF</code> and try reopening the dev container.</p>"},{"location":"tasks/tk06_api_design/#create-a-branch-for-individual-api-design","title":"Create a Branch for Individual API Design","text":"<p>Clone the project, open your project in a dev container, and create a branch for your individual API design. Name your branch something that includes your onyen or github username.</p>"},{"location":"tasks/tk06_api_design/#individual-api-design","title":"Individual API Design","text":"<p>In your branch created above, go ahead and stub out an HTTP API, making use of FastAPI routes and Pydantic models as necessary, that satisfy the user stories. Use the <code>/docs</code> user interface to review your routes. Your objectives are:</p> <ul> <li>Define Endpoints: Specify all the HTTP routes required for the required user stories above.</li> <li>Design Data Models: Create Pydantic models that define the structure of request bodies and responses.</li> <li>Clear OpenAPI Documentation: Fully document important your API using the OpenAPI standards discussed below.</li> <li>Establish Conventions: Ensure consistent naming and documentation throughout your API design.</li> </ul>"},{"location":"tasks/tk06_api_design/#openapi-specification-requirements","title":"OpenAPI Specification Requirements","text":"<p>FastAPI and Pydantic have special constructs which allow you to more fully specify your API and its documentation to produce the standards-based <code>OpenAPI.json</code> spec powering the <code>/docs</code> user interface.</p> <p>You are required to add specification and documentation to your API design along each of the following dimensions. You can find examples of how each is done following this overview list:</p> <ul> <li>FastAPI Application: Ensure your app is properly instantiated with required metadata.</li> <li>Route-level: Always include a summary and description; document response bodies thoroughly.</li> <li>Route Parameters:<ul> <li>Path parameters: Must have descriptions and optional validations.</li> <li>Query parameters: Must have descriptions and can include validations.</li> <li>Body parameters: Must have descriptions and <code>openapi_examples</code> for clear request body documentation.</li> </ul> </li> <li>Pydantic Fields: Every field should include a description and an example (or examples) to aid API consumers.</li> </ul>"},{"location":"tasks/tk06_api_design/#fastapi-application-level-documentation","title":"FastAPI Application-level Documentation","text":"<p>Instantiate your FastAPI app using the <code>FastAPI</code> constructor. You must provide a <code>title</code>, <code>contact</code>, <code>description</code>, and <code>openapi_tags</code> (for organizing routes), as shown below. Notice that the description is markdown and you can use a docstring to give your API documentation </p> <p>Example:</p> <pre><code>app = FastAPI(\n    title=\"TK06 API Design\",\n    contact={\n        \"name\": \"Parter A, Partner B\",\n        \"url\": \"https://github.com/comp423-26s/&lt;your-team-repo&gt;\",\n    },\n    description=\"\"\"\n## Introduction\n\nYour introduction text to your API goes here, in **markdown**.\nWrite your own brief intro to what his API is about.\n\"\"\",\n    openapi_tags=[\n        {\"name\": \"Sue\", \"description\": \"Sue Sharer's API Endpoints\"},\n        {\"name\": \"Cai\", \"description\": \"Cai Clicker's API Endpoints\"},\n        {\"name\": \"Amy\", \"description\": \"Amy Admin's API Endpoints\"},\n    ],\n)\n</code></pre> <p>Run your dev server with <code>./scripts/run-dev-server.sh</code></p> <p>You have two paths toward running the development server. The easiest way is to run the <code>./scripts/run-dev-server.sh</code> script.</p> <p>Otherwise, you can run the <code>uvicorn</code> command manually:</p> <p><code>uv run uvicorn main:app --host 0.0.0.0 --reload</code></p> <p>While the server is running in your dev container, you can navigate to the ports tab in VS Code to look for the \"Forwarded Address\". This is the port that is forwarded to your host machine. You can either open a browser on your host machine directly to this location or click the globe icon next to the address in VSCode as a direct shortcut to do the same.</p> <p>Navigate to <code>/docs</code> in your browser to see the interactive OpenAPI documentation UI.</p> <p>After you've more fully configured your <code>app</code>, as shown above, try reloading your OpenAPI UI by navigating to <code>/docs</code> in your dev server. You should see the information above being used to improve the documentation generated. The tags added will allow you to organize your routes based on the intended user. In real APIs, tags are generally used to cluster endpoints for a specific feature together; here we're using them to organize by persona served.</p>"},{"location":"tasks/tk06_api_design/#route-level-decorator-specification","title":"Route-level Decorator Specification","text":"<p>Define endpoints using FastAPI\u2019s route decorators (e.g., <code>@app.get</code>, <code>@app.post</code>). Each route must include a summary, description, and tag. The tag corresponds to the <code>openapi_tags</code> you specified above and will be a persona name. If your route returns response codes besides <code>200</code>, such as <code>404</code>, you need to specify the responses field as shown below. For a given status code, the description is required and the model (Pydantic subclass) is only necessary if the response returns a body.</p> <p>Example:</p> <pre><code>from typing import Annotated\n\nclass MessageResponse(BaseModel):\n    message: Annotated[str, Field(\n        description=\"Information conveyed ot user\", examples=[\"Hi!\"]\n    )]\n\n# ...\n\n@app.get(\n    \"/items/{item_id}\",\n    summary=\"Retrieve an Item\",\n    description=\"Get details of an item by its ID.\",\n    responses={\n        404: {\n            \"description\": \"Item not found\",\n        }\n    },\n    tags=[\"Shopping\"]\n)\ndef get_item(\n    item_id: Annotated[int, Path(\n        description=\"The unique ID of the item\",\n        gt=0,\n        examples=[67, 68]\n    )]\n) -&gt; MessageResponse:\n    if item_id &gt; 0:\n        return MessageResponse(message=\"Item found!\")\n    else:\n        raise HTTPException(status_code=404, detail=\"Item not found!\")\n</code></pre>"},{"location":"tasks/tk06_api_design/#dynamic-path-parameters","title":"Dynamic Path Parameters","text":"<p>For dynamic segments in the URL (path parameters), use <code>Path</code>. Include a description and any additional keyword parameters found in the official documentation you believe would be helpful in specifying and documenting your path (useful ideas: 1. <code>examples</code> list of example values you might expect for the parameter, 2. validation such as <code>min_length</code> or <code>gt</code> (greater than) as shown below).</p> <p>Example:</p> <pre><code>from fastapi import FastAPI, Path\nfrom typing import Annotated\n\n# ...\n\n@app.get(\"/users/{user_id}\")\ndef get_user(\n    user_id: Annotated[int, Path(\n        description=\"The unique ID of the user\",\n        gt=0,\n        examples=[1, 423]\n    )]\n) -&gt; User:\n    ...\n</code></pre>"},{"location":"tasks/tk06_api_design/#query-parameters","title":"Query Parameters","text":"<p>For query parameters (appended to the URL), use <code>Query</code>. Each query parameter must include a description, should probably include a default value, and can optionally include additional examples and validation rules, if needed. See the official documentation on supported keyword parameters when specifying and documenting query parameters.</p> <p>Example:</p> <pre><code>from fastapi import FastAPI, Query\nfrom typing import Annotated\n\n# ...\n\n@app.get(\"/search\")\ndef search_items(\n    q: Annotated[str, Query(\n        description=\"The product search query\",\n        examples=[\"jordans\"]\n    )] = \"\" # Default value is empty string\n) -&gt; SearchResults:\n    ...\n</code></pre>"},{"location":"tasks/tk06_api_design/#documenting-pydantic-model-fields","title":"Documenting Pydantic Model Fields","text":"<p>Within your Pydantic models, use the <code>Field</code> function to document each field. Every field must have a description and examples list to aid API consumers.</p> <p>Example:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Annotated\n\nclass Item(BaseModel):\n    name: Annotated[str, Field(\n        description=\"Name of Product\",\n        examples=[\"UNC Jersey\", \"UNC Socks\"]\n    )]\n    price: Annotated[float, Field(\n        description=\"Sales Price\",\n        examples=[75.0, 20.0]\n    )]\n</code></pre>"},{"location":"tasks/tk06_api_design/#request-body-parameters","title":"Request Body Parameters","text":"<p>For request body parameters (used in <code>POST</code>/<code>PUT</code>/<code>PATCH</code> requests), define a Pydantic model and use <code>Body</code> to add metadata. The body parameter must include a description and openapi_examples. These examples help make testing out the API in <code>/docs</code> easier, as you will see when you try it out. You can also add validation rules if needed.</p> <p>Example:</p> <pre><code>from fastapi import FastAPI, Body\nfrom typing import Annotated\n\n# ... Same Item model as above ...\n\n@app.post(\"/items\")\ndef create_item(\n    item: Annotated[Item, Body(\n        description=\"The product to create\",\n        openapi_examples={\n            \"Air Jordans\": {\n                \"summary\": \"Air Jordan 1 Mid SE\",\n                \"description\": \"Sample product to create\",\n                \"value\": {\n                    \"name\": \"Air Jordan 1 Mid SE\",\n                    \"price\": 134.99\n                },\n            }\n        }\n    )]\n) -&gt; Item:\n    ...\n</code></pre>"},{"location":"tasks/tk06_api_design/#collaboration-for-phase-1","title":"Collaboration for Phase 1","text":"<p>Each of you should individually draft a design of your API in FastAPI on your own branches (branch naming specified after the Getting Started section above). You should both push your branches to GitHub.</p> <p>Once you are ready to merge your branches to form a unified API for your team, we do not recommend actually attempting a merge in <code>git</code>. You are welcomed to, but at your own peril. Since you both worked in <code>main.py</code>, and made design decisions independently, the merge conflict resolution will be gnarly.</p> <p>Instead of attempting a <code>git</code> merge, we strongly suggest pair programming, and starting over by going back to your <code>main</code> branch on one of your machines. Start a new branch based on <code>main</code> that is <code>pair-api-design</code>. On the other of your machines, have open both of your branches in GitHub to easily view how each of you approached the design and try to form a consensus on how to approach. You will be well served by each reading each other's design and then attempting to whiteboard your final approach before diving into code. Once you are complete, push your final <code>pair-api-design</code> to GitHub and submit your teams' reflection for Phase 1 on Gradescope.</p>"},{"location":"tasks/tk06_api_design/#sanity-checks","title":"Sanity Checks","text":"<p>Questions to consider in the context of your API:</p> <ul> <li>Have we ensured that our design addresses every required user story for Sue, Cai, and Amy?</li> <li>Are our naming conventions for endpoints, models, and fields consistent and descriptive enough for all personas?</li> <li>Are we including required metadata (e.g., summaries, descriptions, examples) for every endpoint and model field so that a developer can easily understand our API?</li> <li>How have we documented error responses (like 404 for missing resources) in our endpoints?</li> <li>Is the route design intuitive for both API users and maintainers?</li> </ul>"},{"location":"tasks/tk06_api_design/#phase-1-submission-and-reflection-questions","title":"Phase 1 Submission and Reflection Questions","text":"<ul> <li>Gradescope submission will include:<ul> <li>Permalink to branches of both partners</li> <li>Permalink to the final <code>pair-api-design</code> branch</li> </ul> </li> <li>Brief reflection question:<ul> <li>What challenges did we encounter when comparing our individual designs, developing a single design, and pair programming our joint, final design?</li> </ul> </li> </ul>"},{"location":"tasks/tk06_api_design/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Designing for Users and Use Cases</p> <p>Through this assignment, you\u2019ve gained experience designing an API that serves multiple types of users with distinct needs. You\u2019ve seen how clear, user-focused API design is essential\u2014not just for making the system functional but also for ensuring a smooth experience for different personas. This mirrors real-world software development, where balancing the needs of end users, system administrators, and stakeholders is key to building successful products.</p> </li> <li> <p>Writing Clear, Professional API Documentation</p> <p>By leveraging FastAPI\u2019s OpenAPI documentation, you\u2019ve practiced writing API specs that go beyond just making things work, you\u2019ve created an API that is easy for others to understand, test, and use. In industry, well-documented APIs are what enable teams to scale, integrate with other systems, and onboard new developers quickly. This attention to detail will serve you well in any software engineering or product development role.</p> </li> <li> <p>Collaborating on Software Design in a Team Environment</p> <p>By independently designing an API and then merging your ideas into a unified implementation, you\u2019ve practiced an essential part of professional software development: balancing individual contributions with collaborative decision-making. You\u2019ve navigated trade-offs, discussed design choices, and worked toward a shared vision. These skills are essential in any software engineering role.</p> </li> <li> <p>Designing the Interface First for Human-Centered Development</p> <p>By focusing on the API interface before implementation, you\u2019ve embraced a human-centered approach: prioritizing how users interact with the system rather than getting lost in internal details. This ensures the design is intuitive and valuable. A well-defined interface also enables parallel development: frontend teams can build against the spec while backend teams implement functionality, making collaboration more efficient. In real-world projects, this approach reduces wasted effort, improves usability, and accelerates development, ultimately leading to better software.</p> </li> </ol>"},{"location":"tasks/tk06_api_implementation_p2/","title":"API and Service-layer Implementation (Part 2)","text":""},{"location":"tasks/tk06_api_implementation_p2/#requirements","title":"Requirements","text":"<p>In the follow-on extension to Part 1 of Task 06, you will implement the following subset of three stories:</p> <ol> <li>As Sue Sharer, I want to create a shortened URL and the ability to request a custom vanity URL, so that I can share a more meaningful link.</li> <li>As Cai Clicker, I want to open a shortened URL by clicking its unique link, so that I am automatically redirected to the original long URL.</li> <li>As Amy Admin, I want to see a list of all active resources (shortened URLs).</li> </ol> <p>These stories are simplifications of stories you specified an API design for in Phase  Your API should still advertise the more sophisticated abilities, but you can choose to ignore the additional inputs or throw an unimplemented exception if detected. If you and your partner are looking for an added challenge, implementing these capabilities is a fun, educational extension, though! Specifically, the complexities of stories from Part 1 your API was designed to handle, that you do not need to implement in the underlying layers, are:</p> <ol> <li>As Sue Sharer, the optional expiration time of a shortened URL.</li> <li>As Amy Admin, the ability to filter active resources by their view counts.</li> </ol> <p>To implement these stories, you will establish a services layer that sits between your routes and the simple file storage implementation you wrote in previous tasks. These sevices will come together at runtime via dependency injection. </p> <p>For turn-in time saving purposes, you will only unit, integration, and end-to-end test Sue and Cai's limited stories #1 and #2. For additional practice, testing Amy's story is a good exercise, as well.</p>"},{"location":"tasks/tk06_api_implementation_p2/#getting-started","title":"Getting Started","text":"<p>You should branch off of your team's <code>pair-api-design</code> branch. Name the new branch <code>pair-api-implementation</code>. Go ahead and push this new branch name to your shared repository so you can both access it. As you progress on this assignment, you will create <code>wip</code> branches at your own direction (we recommend often!) and use the shared repository as a means for handing-off with one another.</p>"},{"location":"tasks/tk06_api_implementation_p2/#1-defining-routes-in-a-routing-module","title":"1. Defining Routes in a Routing Module","text":"<p>You should define your final API routes in an <code>APIRouter</code> that gets included in your <code>main.py</code>'s <code>app</code>. The point of an <code>APIRouter</code> is it allows you to define groups of routes together so that your app's <code>main</code> script does not grow too overwhelming. It's a divide-and-conquer approach.</p> <p>Some of you noticed the <code>src/routes/router.py</code> module and its inclusion in <code>src/main.py</code> and already organized your routes in this way. If you designed your API routes in <code>main.py</code>, you can easily move them to <code>src/routes/router.py</code> by cutting, pasting, and renaming your decorators from <code>@app.get(...)</code> and <code>@app.post(...)</code> to <code>@router.get(...)</code> and <code>@router.post(...)</code>, respectively. You can read the official documentation on <code>APIRouter</code> for additional guidance, as needed.</p>"},{"location":"tasks/tk06_api_implementation_p2/#2-establishing-a-configuration-module","title":"2. Establishing a Configuration Module","text":"<p>In any reasonably sized application that is designed for testing, developers and systems administrators need control over its configuration. For example, our <code>JSONFileIO</code> storage ultimately needs a string path to read and write its data file to. Our little system is quite simple, so this is the only configuration it really needs, but you can imagine for more sophisticated real-world apps there are many other places where configuration needs to be controlled.</p> <p>Rather than hard-coding specific string paths throughout our application's source code, we will introduce some indirection to allow us to dependency inject a <code>Config</code> object wherever it is useful. In doing so, we will also be able to easily provide our own test double for a <code>Config</code> object when writing automated tests that give us easy control over our application's configuration while testing it. This is a win-win for organization.</p>"},{"location":"tasks/tk06_api_implementation_p2/#3-establishing-configpy","title":"3. Establishing <code>config.py</code>","text":"<p>Begin by adding a file named <code>config.py</code> to your <code>src</code> directory with the following contents:</p> src/config.py<pre><code>\"\"\"Application configuration settings.\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Annotated, TypeAlias\n\nfrom fastapi import Depends\n\n\n@dataclass(frozen=True)\nclass Config:\n    \"\"\"Defines configuration values for the application.\n\n    Attributes:\n        links_path: Path to the JSON file that stores link data.\n    \"\"\"\n\n    links_path: str = \"data/links.json\"\n\n\nConfigDI: TypeAlias = Annotated[Config, Depends()]\n\"\"\"Dependency-injected Config type.\"\"\"\n</code></pre> <p>For more information on the <code>@dataclass</code> decorator, see the official Python documentation. Python's <code>dataclass</code> is very similar to Java's <code>record</code> classes.</p> <p>You will notice we establish a <code>TypeAlias</code> for dependency injection convenience that we will make use of soon.</p> <p>In the future, rather than hard-coding the <code>links_path</code> string here, we will use an environment variable that is read from our process' environment to enable loading this configuration from the outside world. This is more of a production and deployment concern, though, so we will address it then. By moving this configuration into its own class, which can be dependency injected, we are achieving our goal in making it easier to substitute configuration test doubles when we get to writing tests.</p>"},{"location":"tasks/tk06_api_implementation_p2/#3-establishing-data-directory-and-updating-gitignore","title":"3. Establishing <code>data/</code> directory and updating <code>.gitignore</code>","text":"<p>In your project, as a sibling to the <code>src</code> directory, go ahead and make a directory named <code>data</code>. You should be able to do this from the dev container's terminal after completing COMP211!</p> <p>Ultimately, this is where your <code>Config</code> class is configured to read and write <code>links.json</code> once your application is running. However, we don't actually want the data in <code>links.json</code> to be committed to the repository. Otherwise, you and your partner would frequently run into merge conflicts as you collaborate asynchronously. Let's be sure <code>git</code> ignores this file by adding it to your project's <code>.gitignore</code> file. Add a section at the end that specifically ignores <code>data/links.json</code> as a path.</p> <p>Finally, to ensure that this directory exists when a partner pulls or someone clones fresh in the future, add a file to the <code>data</code> directory named <code>.gitkeep</code>. Its contents will be blank. This is a conventional file name used in an otherwise empty directory to ensure the directory will be created by git. You can add this to a new commit and go ahead and make a commit.</p>"},{"location":"tasks/tk06_api_implementation_p2/#4-dependency-injection-factories-and-helper-types","title":"4. Dependency Injection Factories and Helper Types","text":"<p>Your <code>service</code> layer need to compose with your <code>store</code> layer's <code>LinkStore</code> class through dependency injection. However, your <code>LinkStore</code> class' constructor requires a path to the underlying data storage file (whose string file path we have established in <code>Config</code>). Thus, the dependency injection container can't automatically determine how to construct it. To get around this limitation without redesigning our <code>LinkStore</code>, we will create a factory function that uses dependency injection to get a handle on <code>Config</code>.</p>"},{"location":"tasks/tk06_api_implementation_p2/#41-adding-srcstorelink_store_factorypy","title":"4.1 Adding <code>src/store/link_store_factory.py</code>","text":"<p>The following file defines a factory function, which is really just a plain-old function that returns an instantiated object and whose parameters can all be dependency injected. Notice the single parameter is making use of the <code>ConfigDI</code> dependency injection type you setup in <code>config.py</code> above.</p> src/store/link_store_factory.py<pre><code>\"\"\"Dependency factory for creating link store instances.\"\"\"\n\nfrom pathlib import Path\nfrom typing import Annotated, TypeAlias\n\nfrom fastapi import Depends\n\nfrom config import ConfigDI\n\nfrom .json_file_io import JSONFileIO\nfrom .link_store import LinkStore\n\n\ndef link_store_factory(config: ConfigDI) -&gt; LinkStore:\n    \"\"\"Create a LinkStore wired to the configured JSON storage.\n\n    Args:\n        config: Application config containing the storage path.\n\n    Returns:\n        A LinkStore backed by the JSON file storage implementation.\n    \"\"\"\n\n    storage = JSONFileIO(Path(config.links_path))\n    return LinkStore(storage)\n\n\nLinkStoreDI: TypeAlias = Annotated[LinkStore, Depends(link_store_factory)]\n\"\"\"Dependency-injected LinkStore type.\"\"\"\n</code></pre> <p>Reading the implementation of <code>link_store_factory</code>, notice it simply reads the <code>links_path</code> from <code>config</code> and produces a new <code>LinkStore</code> object.</p> <p>Finally, notice that we are, once again, defining a <code>TypeAlias</code> for the convenience of specifying a dependency injectable <code>LinkStore</code>.</p>"},{"location":"tasks/tk06_api_implementation_p2/#42-exporting-the-linkstoredi-name-from-the-store-package","title":"4.2 Exporting the <code>LinkStoreDI</code> name from the <code>store</code> Package","text":"<p>As defined, if you wanted to use the <code>LinkStoreDI</code> type from our services layer, you would have to import it using the fully qualified module name it is defined in:</p> <p><code>from store.link_store_factory import LinkStoreDI</code></p> <p>However, this type makes sense to be able to import directly from the <code>store</code> package. We really want to be able to use the following import statement from other areas of our project:</p> <p><code>from store import LinkStoreDI</code></p> <p>In order to export the <code>LinkStoreDI</code> name from the <code>store</code> package, we need to add its name to the list of exported names at the package level. Python has a specific and peculiar way of doing so. If you open <code>src/store/__init__.py</code> you will see the convention.</p> <p>To add <code>LinkStoreDI</code> to the list of names available at the package level, add an import statement for the <code>LinkStoreDI</code> type and then add the name to the special dunderscore variable <code>__all__</code>. This will make the name importable from the <code>store</code> package.</p> src/store/__init__.py<pre><code>\"\"\"Persistence layer package.\"\"\"\n\nfrom store.json_file_io import JSONFileIO\nfrom store.link_store import LinkStore\nfrom store.link_store_factory import LinkStoreDI\n\n__all__ = [\"JSONFileIO\", \"LinkStore\", \"LinkStoreDI\"]\n</code></pre>"},{"location":"tasks/tk06_api_implementation_p2/#5-establishing-a-service-layer","title":"5. Establishing a Service Layer","text":"<p>In <code>src/services</code>, add a <code>link_service.py</code> module with the following starter code:</p> <pre><code>from models import Link\nfrom store import LinkStoreDI\n\n\nclass LinkService:\n    def __init__(self, link_store: LinkStoreDI):\n        self._link_store = link_store\n</code></pre> <p>Notice the <code>link_store</code> parameter is making use of the <code>LinkStoreDI</code> dependency injected type you just established.</p> <p>Now that you have seen a few examples of a the <code>DI</code>-style <code>TypeAlias</code> convention we are using, go ahead and define a <code>TypeAlias</code> for <code>LinkService</code> <code>DI</code> and be sure it is exported from the <code>services</code> package.</p>"},{"location":"tasks/tk06_api_implementation_p2/#6-injecting-services-in-routes","title":"6. Injecting Services in Routes","text":"<p>Now, finally, after establishing these definitions to support dependency injection, you can inject your <code>LinkService</code> service into your routes in <code>router.py</code>. First, be sure you import the <code>LinkServiceDI</code> type that you exported from <code>services</code>. Then, what do you need to do to inject a <code>LinkService</code> instance into your routes? You should be able to figure this out at this point; refer back to the readings as necessary.</p>"},{"location":"tasks/tk06_api_implementation_p2/#working-together","title":"Working Together","text":"<p>Our recommendation for paired workflow on this assignment is:</p> <ol> <li>Pair program on a single laptop to do the initial refactoring and getting started steps</li> <li>Pair program to complete the initial implementation of functionality of routes. Push and fetch to sync work across your machines.</li> <li>Split up the work of testing Sue and Cai's routes individually, push to <code>wip</code> branches, review each other's code, and merge back in</li> </ol>"},{"location":"tasks/tk06_api_implementation_p2/#testing-requirements","title":"Testing Requirements","text":"<p>Beyond the required story implementations described in Requirements, there are a few testing requirements to practice unit, integration, and end-to-end tests.</p> <p>For both Sue and Cai's required stories, you need to produce AAA tests following the conventions shown in the recent reading on API testing. For both routes and service methods, you should be able to demonstrate:</p> <ol> <li>Unit Tests for the route handlers and service class methods (separate test files, well organized in <code>test</code> directory) </li> <li>Integration Tests for the router, route handler, and service layers isolated from the storage layer. There are multiple viable strategies for overriding the storage layer with a mock, but you are encouraged to experiment with FastAPI's <code>dependency_overrides</code>. Look to online resources to learn more. Try overriding <code>link_store_factory</code> via <code>dependency_overrides</code>.</li> <li>End-to-end Tests from router through storage. Here you will want to use <code>dependency_overrides</code> to override your <code>Config</code> to use a <code>pytest</code> provided <code>tmp_path</code> fixture. Reminder, <code>tmp_path</code> returns a path of a temporary directory location so you will need to append an actual filename. See your integration tests for <code>JSONFileIO</code> to learn more. This is still an end-to-end test because your code reads and writes from disk, the override is just making setup/teardown smoother.</li> </ol> <p>For integration and end-to-end tests, you are encouraged to use VSCode's built-in debugger and test runner with debugger to step through a test line-by-line, to convince yourself of your testing subject's scope and convincing yourself you have isolated the correct targets.</p>"},{"location":"threads/career/","title":"Career and Professional Life","text":"<p>This thread focuses on the \"human\" side of being a developer. It includes topics such as professional ethics, setting long-term career goals, the habit of lifelong learning, and so on. </p> <p>Technology changes rapidly, but professional foundations do not. Learning how to manage your growth and navigate ethical dilemmas ensures you remain a high-value, responsible professional throughout your career. It helps you avoid burnout and stay relevant as technologies, tools, and languages evolve.</p> <ul> <li>Ethics</li> <li>Lifelong Learning</li> </ul>"},{"location":"threads/career/ethics/","title":"Ethics","text":"Readings Topic RD01 ACM Software Engineering Code of Ethics Lessons Topic LS01 Architectural Design Records (ADRs), Lifelong Learning"},{"location":"threads/career/ethics/acm_ethics_overview/","title":"ACM Software Engineering Code of Ethics","text":""},{"location":"threads/career/ethics/acm_ethics_overview/#software-engineering-code-of-ethics-and-professional-practice-short-version","title":"Software Engineering Code of Ethics and Professional Practice (Short Version)","text":""},{"location":"threads/career/ethics/acm_ethics_overview/#preamble","title":"PREAMBLE","text":"<p>The short version of the code summarizes aspirations at a high level of the abstraction; the clauses that are included in the full version give examples and details of how these aspirations change the way we act as software engineering professionals. Without the aspirations, the details can become legalistic and tedious; without the details, the aspirations can become high sounding but empty; together, the aspirations and the details form a cohesive code.</p> <p>Software engineers shall commit themselves to making the analysis, specification, design, development, testing and maintenance of software a beneficial and respected profession. In accordance with their commitment to the health, safety and welfare of the public, software engineers shall adhere to the following Eight Principles: </p> <ol> <li> <p>PUBLIC \u2013 Software engineers shall act consistently with the public interest.</p> </li> <li> <p>CLIENT AND EMPLOYER \u2013 Software engineers shall act in a manner that is in the best interests of their client and employer consistent with the public interest.</p> </li> <li> <p>PRODUCT \u2013 Software engineers shall ensure that their products and related modifications meet the highest professional standards possible.</p> </li> <li> <p>JUDGMENT \u2013 Software engineers shall maintain integrity and independence in their professional judgment.</p> </li> <li> <p>MANAGEMENT \u2013 Software engineering managers and leaders shall subscribe to and promote an ethical approach to the management of software development and maintenance.</p> </li> <li> <p>PROFESSION \u2013 Software engineers shall advance the integrity and reputation of the profession consistent with the public interest.</p> </li> <li> <p>COLLEAGUES \u2013 Software engineers shall be fair to and supportive of their colleagues.</p> </li> <li> <p>SELF \u2013 Software engineers shall participate in lifelong learning regarding the practice of their profession and shall promote an ethical approach to the practice of the profession.</p> </li> </ol>"},{"location":"threads/career/ethics/acm_ethics_overview/#software-engineering-code-of-ethics-and-professional-practice-full-version","title":"Software Engineering Code of Ethics and Professional Practice (Full Version)","text":""},{"location":"threads/career/ethics/acm_ethics_overview/#preamble_1","title":"PREAMBLE","text":"<p>Computers have a central and growing role in commerce, industry, government, medicine, education, entertainment and society at large. Software engineers are those who contribute by direct participation or by teaching, to the analysis, specification, design, development, certification, maintenance and testing of software systems. Because of their roles in developing software systems, software engineers have significant opportunities to do good or cause harm, to enable others to do good or cause harm, or to influence others to do good or cause harm. To ensure, as much as possible, that their efforts will be used for good, software engineers must commit themselves to making software engineering a beneficial and respected profession. In accordance with that commitment, software engineers shall adhere to the following Code of Ethics and Professional Practice.</p> <p>The Code contains eight Principles related to the behavior of and decisions made by professional software engineers, including practitioners, educators, managers, supervisors and policy makers, as well as trainees and students of the profession. The Principles identify the ethically responsible relationships in which individuals, groups, and organizations participate and the primary obligations within these relationships. The Clauses of each Principle are illustrations of some of the obligations included in these relationships. These obligations are founded in the software engineer\u2019s humanity, in special care owed to people affected by the work of software engineers, and the unique elements of the practice of software engineering. The Code prescribes these as obligations of anyone claiming to be or aspiring to be a software engineer.</p> <p>It is not intended that the individual parts of the Code be used in isolation to justify errors of omission or commission. The list of Principles and Clauses is not exhaustive. The Clauses should not be read as separating the acceptable from the unacceptable in professional conduct in all practical situations. The Code is not a simple ethical algorithm that generates ethical decisions. In some situations standards may be in tension with each other or with standards from other sources. These situations require the software engineer to use ethical judgment to act in a manner which is most consistent with the spirit of the Code of Ethics and Professional Practice, given the circumstances.</p> <p>Ethical tensions can best be addressed by thoughtful consideration of fundamental principles, rather than blind reliance on detailed regulations. These Principles should influence software engineers to consider broadly who is affected by their work; to examine if they and their colleagues are treating other human beings with due respect; to consider how the public, if reasonably well informed, would view their decisions; to analyze how the least empowered will be affected by their decisions; and to consider whether their acts would be judged worthy of the ideal professional working as a software engineer. In all these judgments concern for the health, safety and welfare of the public is primary; that is, the \u201cPublic Interest\u201d is central to this Code.</p> <p>The dynamic and demanding context of software engineering requires a code that is adaptable and relevant to new situations as they occur. However, even in this generality, the Code provides support for software engineers and managers of software engineers who need to take positive action in a specific case by documenting the ethical stance of the profession. The Code provides an ethical foundation to which individuals within teams and the team as a whole can appeal. The Code helps to define those actions that are ethically improper to request of a software engineer or teams of software engineers.</p> <p>The Code is not simply for adjudicating the nature of questionable acts; it also has an important educational function. As this Code expresses the consensus of the profession on ethical issues, it is a means to educate both the public and aspiring professionals about the ethical obligations of all software engineers.</p>"},{"location":"threads/career/ethics/acm_ethics_overview/#principles","title":"PRINCIPLES","text":""},{"location":"threads/career/ethics/acm_ethics_overview/#principle-1-public","title":"Principle 1: PUBLIC","text":"<p>Software engineers shall act consistently with the public interest. In particular, software engineers shall, as appropriate:</p> <p>1.01. Accept full responsibility for their own work.</p> <p>1.02. Moderate the interests of the software engineer, the employer, the client and the users with the public good.</p> <p>1.03. Approve software only if they have a well-founded belief that it is safe, meets specifications, passes appropriate tests, and does not diminish quality of life, diminish privacy or harm the environment. The ultimate effect of the work should be to the public good.</p> <p>1.04. Disclose to appropriate persons or authorities any actual or potential danger to the user, the public, or the environment, that they reasonably believe to be associated with software or related documents.</p> <p>1.05. Cooperate in efforts to address matters of grave public concern caused by software, its installation, maintenance, support or documentation.</p> <p>1.06. Be fair and avoid deception in all statements, particularly public ones, concerning software or related documents, methods and tools.</p> <p>1.07. Consider issues of physical disabilities, allocation of resources, economic disadvantage and other factors that can diminish access to the benefits of software.</p> <p>1.08. Be encouraged to volunteer professional skills to good causes and contribute to public education concerning the discipline.</p>"},{"location":"threads/career/ethics/acm_ethics_overview/#principle-2-client-and-employer","title":"Principle 2: CLIENT AND EMPLOYER","text":"<p>Software engineers shall act in a manner that is in the best interests of their client and employer, consistent with the public interest. In particular, software engineers shall, as appropriate:</p> <p>2.01. Provide service in their areas of competence, being honest and forthright about any limitations of their experience and education.</p> <p>2.02. Not knowingly use software that is obtained or retained either illegally or unethically.</p> <p>2.03. Use the property of a client or employer only in ways properly authorized, and with the client\u2019s or employer\u2019s knowledge and consent.</p> <p>2.04. Ensure that any document upon which they rely has been approved, when required, by someone authorized to approve it.</p> <p>2.05. Keep private any confidential information gained in their professional work, where such confidentiality is consistent with the public interest and consistent with the law.</p> <p>2.06. Identify, document, collect evidence and report to the client or the employer promptly if, in their opinion, a project is likely to fail, to prove too expensive, to violate intellectual property law, or otherwise to be problematic.</p> <p>2.07. Identify, document, and report significant issues of social concern, of which they are aware, in software or related documents, to the employer or the client.</p> <p>2.08. Accept no outside work detrimental to the work they perform for their primary employer.</p> <p>2.09. Promote no interest adverse to their employer or client, unless a higher ethical concern is being compromised; in that case, inform the employer or another appropriate authority of the ethical concern.</p>"},{"location":"threads/career/ethics/acm_ethics_overview/#principle-3-product","title":"Principle 3: PRODUCT","text":"<p>Software engineers shall ensure that their products and related modifications meet the highest professional standards possible. In particular, software engineers shall, as appropriate:</p> <p>3.01. Strive for high quality, acceptable cost and a reasonable schedule, ensuring significant tradeoffs are clear to and accepted by the employer and the client, and are available for consideration by the user and the public.</p> <p>3.02. Ensure proper and achievable goals and objectives for any project on which they work or propose.</p> <p>3.03. Identify, define and address ethical, economic, cultural, legal and environmental issues related to work projects.</p> <p>3.04. Ensure that they are qualified for any project on which they work or propose to work by an appropriate combination of education and training, and experience.</p> <p>3.05. Ensure an appropriate method is used for any project on which they work or propose to work.</p> <p>3.06. Work to follow professional standards, when available, that are most appropriate for the task at hand, departing from these only when ethically or technically justified.</p> <p>3.07. Strive to fully understand the specifications for software on which they work.</p> <p>3.08. Ensure that specifications for software on which they work have been well documented, satisfy the users\u2019 requirements and have the appropriate approvals.</p> <p>3.09. Ensure realistic quantitative estimates of cost, scheduling, personnel, quality and outcomes on any project on which they work or propose to work and provide an uncertainty assessment of these estimates.</p> <p>3.10. Ensure adequate testing, debugging, and review of software and related documents on which they work.</p> <p>3.11. Ensure adequate documentation, including significant problems discovered and solutions adopted, for any project on which they work.</p> <p>3.12. Work to develop software and related documents that respect the privacy of those who will be affected by that software.</p> <p>3.13. Be careful to use only accurate data derived by ethical and lawful means, and use it only in ways properly authorized.</p> <p>3.14. Maintain the integrity of data, being sensitive to outdated or flawed occurrences.</p> <p>3.15 Treat all forms of software maintenance with the same professionalism as new development.</p>"},{"location":"threads/career/ethics/acm_ethics_overview/#principle-4-judgment","title":"Principle 4: JUDGMENT","text":"<p>Software engineers shall maintain integrity and independence in their professional judgment. In particular, software engineers shall, as appropriate:</p> <p>4.01. Temper all technical judgments by the need to support and maintain human values.</p> <p>4.02 Only endorse documents either prepared under their supervision or within their areas of competence and with which they are in agreement.</p> <p>4.03. Maintain professional objectivity with respect to any software or related documents they are asked to evaluate.</p> <p>4.04. Not engage in deceptive financial practices such as bribery, double billing, or other improper financial practices.</p> <p>4.05. Disclose to all concerned parties those conflicts of interest that cannot reasonably be avoided or escaped.</p> <p>4.06. Refuse to participate, as members or advisors, in a private, governmental or professional body concerned with software related issues, in which they, their employers or their clients have undisclosed potential conflicts of interest.</p>"},{"location":"threads/career/ethics/acm_ethics_overview/#principle-5-management","title":"Principle 5: MANAGEMENT","text":"<p>Software engineering managers and leaders shall subscribe to and promote an ethical approach to the management of software development and maintenance . In particular, those managing or leading software engineers shall, as appropriate:</p> <p>5.01 Ensure good management for any project on which they work, including effective procedures for promotion of quality and reduction of risk.</p> <p>5.02. Ensure that software engineers are informed of standards before being held to them.</p> <p>5.03. Ensure that software engineers know the employer\u2019s policies and procedures for protecting passwords, files and information that is confidential to the employer or confidential to others.</p> <p>5.04. Assign work only after taking into account appropriate contributions of education and experience tempered with a desire to further that education and experience.</p> <p>5.05. Ensure realistic quantitative estimates of cost, scheduling, personnel, quality and outcomes on any project on which they work or propose to work, and provide an uncertainty assessment of these estimates.</p> <p>5.06. Attract potential software engineers only by full and accurate description of the conditions of employment.</p> <p>5.07. Offer fair and just remuneration.</p> <p>5.08. Not unjustly prevent someone from taking a position for which that person is suitably qualified.</p> <p>5.09. Ensure that there is a fair agreement concerning ownership of any software, processes, research, writing, or other intellectual property to which a software engineer has contributed.</p> <p>5.10. Provide for due process in hearing charges of violation of an employer\u2019s policy or of this Code.</p> <p>5.11. Not ask a software engineer to do anything inconsistent with this Code.</p> <p>5.12. Not punish anyone for expressing ethical concerns about a project.</p>"},{"location":"threads/career/ethics/acm_ethics_overview/#principle-6-profession","title":"Principle 6: PROFESSION","text":"<p>Software engineers shall advance the integrity and reputation of the profession consistent with the public interest. In particular, software engineers shall, as appropriate:</p> <p>6.01. Help develop an organizational environment favorable to acting ethically.</p> <p>6.02. Promote public knowledge of software engineering.</p> <p>6.03. Extend software engineering knowledge by appropriate participation in professional organizations, meetings and publications.</p> <p>6.04. Support, as members of a profession, other software engineers striving to follow this Code.</p> <p>6.05. Not promote their own interest at the expense of the profession, client or employer.</p> <p>6.06. Obey all laws governing their work, unless, in exceptional circumstances, such compliance is inconsistent with the public interest.</p> <p>6.07. Be accurate in stating the characteristics of software on which they work, avoiding not only false claims but also claims that might reasonably be supposed to be speculative, vacuous, deceptive, misleading, or doubtful.</p> <p>6.08. Take responsibility for detecting, correcting, and reporting errors in software and associated documents on which they work.</p> <p>6.09. Ensure that clients, employers, and supervisors know of the software engineer\u2019s commitment to this Code of ethics, and the subsequent ramifications of such commitment.</p> <p>6.10. Avoid associations with businesses and organizations which are in conflict with this code.</p> <p>6.11. Recognize that violations of this Code are inconsistent with being a professional software engineer.</p> <p>6.12. Express concerns to the people involved when significant violations of this Code are detected unless this is impossible, counter-productive, or dangerous.</p> <p>6.13. Report significant violations of this Code to appropriate authorities when it is clear that consultation with people involved in these significant violations is impossible, counter-productive or dangerous.</p>"},{"location":"threads/career/ethics/acm_ethics_overview/#principle-7-colleagues","title":"Principle 7: COLLEAGUES","text":"<p>Software engineers shall be fair to and supportive of their colleagues. In particular, software engineers shall, as appropriate:</p> <p>7.01. Encourage colleagues to adhere to this Code.</p> <p>7.02. Assist colleagues in professional development.</p> <p>7.03. Credit fully the work of others and refrain from taking undue credit.</p> <p>7.04. Review the work of others in an objective, candid, and properly-documented way.</p> <p>7.05. Give a fair hearing to the opinions, concerns, or complaints of a colleague.</p> <p>7.06. Assist colleagues in being fully aware of current standard work practices including policies and procedures for protecting passwords, files and other confidential information, and security measures in general.</p> <p>7.07. Not unfairly intervene in the career of any colleague; however, concern for the employer, the client or public interest may compel software engineers, in good faith, to question the competence of a colleague.</p> <p>7.08. In situations outside of their own areas of competence, call upon the opinions of other professionals who have competence in that area.</p>"},{"location":"threads/career/ethics/acm_ethics_overview/#principle-8-self","title":"Principle 8: SELF","text":"<p>Software engineers shall participate in lifelong learning regarding the practice of their profession and shall promote an ethical approach to the practice of the profession. In particular, software engineers shall continually endeavor to:</p> <p>8.01. Further their knowledge of developments in the analysis, specification, design, development, maintenance and testing of software and related documents, together with the management of the development process.</p> <p>8.02. Improve their ability to create safe, reliable, and useful quality software at reasonable cost and within a reasonable time.</p> <p>8.03. Improve their ability to produce accurate, informative, and well-written documentation.</p> <p>8.04. Improve their understanding of the software and related documents on which they work and of the environment in which they will be used.</p> <p>8.05. Improve their knowledge of relevant standards and the law governing the software and related documents on which they work.</p> <p>8.06 Improve their knowledge of this Code, its interpretation, and its application to their work.</p> <p>8.07 Not give unfair treatment to anyone because of any irrelevant prejudices.</p> <p>8.08. Not influence others to undertake any action that involves a breach of this Code.</p> <p>8.09. Recognize that personal violations of this Code are inconsistent with being a professional software engineer.</p> <p>This Code was developed by the ACM/IEEE-CS joint task force on Software Engineering Ethics and Professional Practices (SEEPP):</p> <p>Executive Committee: Donald Gotterbarn (Chair), Keith Miller and Simon Rogerson;</p> <p>Members: Steve Barber, Peter Barnes, Ilene Burnstein, Michael Davis, Amr El-Kadi, N. Ben Fairweather, Milton Fulghum, N. Jayaram, Tom Jewett, Mark Kanko, Ernie Kallman, Duncan Langford, Joyce Currie Little, Ed Mechler, Manuel J. Norman, Douglas Phillips, Peter Ron Prinzivalli, Patrick Sullivan, John Weckert, Vivian Weil, S. Weisband and Laurie Honour Werth.</p> <p>This Code may be published without permission as long as it is not changed in any way and it carries the copyright notice. Copyright (c) 1999 by the Association for Computing Machinery, Inc. and the Institute for Electrical and Electronics Engineers, Inc.</p>"},{"location":"threads/career/learning/","title":"Lifelong Learning","text":"Lessons Topic LS01 Architectural Design Records (ADRs), Lifelong Learning"},{"location":"threads/course/","title":"Course Logistics","text":"<p>While course logistics are accidental to learning about software engineering, they are nonetheless important for succeeding within the context of this course. Additionally, assessments serve as a form of \"acceptance verification\" that you are gaining knowledge and achieving the course learning objectives.</p> Readings Topic RD00 Course Syllabus RD13 How AI Impacts Skill Formation Lessons Topic LS00 Foundations of Software Engineering QZ00 Unit 0 Quiz SD00              Snow Day - Class Cancelled           SD01              Snow Day - Class Cancelled           QZ01 Unit 1 Quiz QZ02              Unit 2 Quiz           QZ03              Unit 3 Quiz           FN00              Final &amp; Final Presentations (4pm-7pm)"},{"location":"threads/design/","title":"Design Process","text":"<p>Before writing code, you must understand the \"why\", \"how\", and \"for who?\" This thread covers requirements analysis, writing technical design documents, and following intentional design processes and methodologies.</p> <p>Building the wrong thing is the most expensive mistake a company can make. Learning to analyze requirements and document plans before you build ensures that your work aligns with user needs and organizational goals. This is a critical skill for moving into senior-level roles where planning is as important as execution, if not more.</p> <p>We will learn more about design process as the course builds up.</p> Readings Topic RD09 Communicating in the Software Development Lifecycle Tasks Topic TK06 API Design (Part 1)"},{"location":"threads/sde/","title":"Software Engineering Skills","text":"<p>This thread moves beyond \"just making it work.\" You will verify software meets business requirements, practice conducting professional code reviews, and documenting technical choices using Architecture Decision Records (ADRs). </p> <p>These skills are what separate a \"coder\" from an \"engineer.\" By learning how to verify quality and communicate your work with appropriate documentation, you help teams maintain a healthy codebase.</p> <ul> <li>Acceptance Testing</li> <li>Architecture Design Records</li> <li>Code Review</li> </ul>"},{"location":"threads/sde/adr/","title":"Architecture Design Records","text":"Readings Topic RD05 Documenting Architecture Decisions Lessons Topic LS01 Architectural Design Records (ADRs), Lifelong Learning Tasks Topic TK01 Write ADRs for Package Managers TK02 Implement a Dependency Manager ADR with an Agent TK04 Professionalizing the Developer Environment"},{"location":"threads/sde/adr/rd_documenting_architecture_decisions/","title":"Documenting Architecture Decisions","text":"<p>Reading Link: https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions.html</p>"},{"location":"threads/sde/code-review/","title":"Code Review","text":"Lessons Topic LS02 Dependency Managers, LLMs, and Agentic IDEs"},{"location":"threads/sde/testing/","title":"Acceptance Testing","text":"Readings Topic RD12 MagicMock Documentation TK05 Unit and Integration Testing Exercise Lessons Topic LS04 Testing Foundations: From Requirements to Automation LS05 Designing with Layers and Composition LS06 Arrange-Act-Assert Testing and Mocking LS07 Testing Concepts: Patching and Fixtures LS08 Testing Practice and Exercises LS09 Pydantic"},{"location":"threads/sde/testing/rd_magic_mock/","title":"MagicMock Documentation","text":"<p>TBD</p>"},{"location":"threads/sde/testing/tk_testing_storage/","title":"Tk testing storage","text":"<p>Note that we are implementing a store that is not very scalable or generally best practice. We are choosing it because it has clear boundaries and highlights some of the fundamental testing concerns that exist in a larger system where a data store is implemented with a much more sophisticated system (such as a relational database or key-value storage engine).</p> <p>Try running tests in tests explorer (and try it with coverage button!)</p> <p>Students should go through a guided code read (with GRQs):   * README.md   * scripts/run-qa.sh   * models/link.py   * store/json_file_io.py   * store/link_store.py   * test/store/*   * conftest.py   * AGENTS.md</p> <p>Follow a very specific simple get routine: switch to a wip-branch for the feature, implement the feature, continue.</p>"},{"location":"threads/sde/testing/tk_testing_storage/#implement-linkstore-methods-and-unit-test-with-mocking-to-avoid-dependency-on-underlying-jsonfileio","title":"Implement LinkStore methods and unit test (with mocking!) to avoid dependency on underlying JSONFileIO","text":"<p>NOTE FOR KRIS: Update descriptions of methods in assignment to be more precise on expectations (list must not expose internal state or produce a mutable reference as a return value to the caller).</p> <p>Requirement: do not change <code>JSONFileIO</code> during this process!!! Do not integration test <code>LinkStore</code>!!!</p> <ul> <li> <p>Start with implementing <code>LinkStore#list</code> unit test only, mock persistence concerns.</p> <ul> <li>Must not expose external state or offer a mutable reference as a return value to caller</li> <li>Start with doing this one by hand.</li> </ul> </li> <li> <p>Continue with testing <code>LinkStore#put</code> unit test only, mock persistence concerns. </p> <ul> <li>Try having the AI help with this one and _very closely audit every line of source code and test code it produces.</li> <li>We may need to hint at what <code>JSONFileIO#persist</code> expects and how <code>link.model_dump()</code> works in Pydantic.</li> </ul> </li> <li> <p>Continue with testing <code>LinkStore#delete</code> unit test only, mock persistence concerns. </p> <ul> <li>Try having the AI help with this one and _very closely audit every line of source code and test code it produces.</li> </ul> </li> </ul> <p>Your implementation of <code>link_store</code> should not have any duplicated logic in its methods. Please refactor duplicated logic into a well named helper method with a prefix underscore. See <code>LinkStore#_load_data</code> for an example.</p>"},{"location":"threads/sde/testing/tk_testing_storage/#implement-jsonfileio-persist-method-with-an-integration-test-to-verify-first","title":"Implement JSONFileIO persist method with an integration test to verify first","text":"<p>Integration test must be marked as such; confirm by running regular unit test suite and seeing that your test is skipped.</p> <p>Integration test should make use of <code>tmp_path</code> fixture in pytest.</p> <p>Add unit tests to JSONFileIO Persist. Here you will need to patch behavior. You may need to know about json.dump (link to standard library).</p>"},{"location":"threads/sde/testing/tk_testing_storage/#implement-integration-tests-with-linkstore","title":"Implement Integration Tests with LinkStore","text":"<p>Add one integration test that verifies list works after loading data</p> <p>Add one integration test that verifies put persists</p> <p>Add one integration test that verifies delete persists</p> <p>Organize redundant arrange code into a fixture</p>"},{"location":"threads/sde/testing/tk_testing_storage_final/","title":"Unit and Integration Testing Exercise","text":"<p>This assignment is focused on implementing the storage layer for a URL shortener. You will be completing the implementation of and fully unit and integration testing two classes: <code>JSONFileIO</code> (low-level JSON file persistence) and <code>LinkStore</code> (logic for storing links).</p> <p>In future weeks, we will implement the FastAPI routes and service layers. They are not our concern right now. Our goal is to ensure data can be saved to and retrieved from a JSON file reliably.</p>"},{"location":"threads/sde/testing/tk_testing_storage_final/#learning-objectives","title":"Learning Objectives","text":"<p>After successfully completing this exercise, you should be able to:</p> <ul> <li>Read and navigate a new codebase, understanding its structure and developer tooling.</li> <li>Explain the value of test-driven development (TDD) and apply the Red \u2192 Green \u2192 Refactor cycle.</li> <li>Write unit tests that verify individual components in isolation.</li> <li>Use mocks to isolate the subject under test from its composed dependencies.</li> <li>Use patches to intercept built-in library calls and test logic without side effects.</li> <li>Write integration tests that verify components work together with real dependencies.</li> </ul> <p>These skills are foundational to the upcoming quiz and will be assessed in the context of your work on this task.</p>"},{"location":"threads/sde/testing/tk_testing_storage_final/#test-driven-development-tdd","title":"Test-driven Development (TDD)","text":"<p>Test-driven development (TDD) is a workflow where you write a small, automated unit test before you write the code that makes it pass. The goal is to build software in tiny, verifiable steps, using tests as a feedback loop and a safety net.</p> <p>In this project, you will practice the Red \u2192 Green \u2192 Refactor cycle:</p> <ul> <li>Red: Write a test that describes the behavior you want. Run it and watch it fail (because the feature does not exist yet).</li> <li>Green: Write the simplest code that makes the test pass. Run the tests until they are all green.</li> <li>Refactor: Improve the design and readability of your code without changing behavior. Re-run tests to confirm you did not break anything.</li> <li>Repeat: Go back to Red and focus on your next behavior to test and implement.</li> </ul> <p>Done well, TDD helps ensure your tests prove real functionality and gives you a clear \"definition of done.\" It keeps you focused on small, incremental behaviors\u2014preventing you from becoming overwhelmed or trying to implement too much at once. It also encourages you to think critically about your interface design. If your interface feels clunky while writing a test, that's a signal to reconsider your design choices. (Note: In this project, we are providing a sensible enough design that's suitable for testing. When the time comes for you to design your own interfaces, classes, and methods, this is worth remembering, though!)</p>"},{"location":"threads/sde/testing/tk_testing_storage_final/#project-setup","title":"Project Setup","text":"<p>This project begins from a new repository, setup very similar to where yours left off in TK04.</p> <p>You can begin the task by accepting the following GitHub Classroom assignment: https://classroom.github.com/a/ItT31Yka</p> <p>If cloning fails...</p> <p>If cloning the repository once you've accepted the assignment fails, then you may need to accept an invitation to the repository.</p> <p>Click on your Notifications of Github.com (the inbox icon) and look for the invitation to accept the repository there. If you do not see it there, look under your profile at \"Organizations\" or \"Repositories\" as alternate places to find the invite. (There should have been an email notification to accept, as well.)</p> <p>Errors starting the Dev Containers in Windows</p> <p>If you face a problem opening the DevContainer in Windows, it's likely due to a line ending issue. In VSCode, change your line ending setting for the project to be just LF not CRLF. You will see a setting for this in the bottom right corner. Change to <code>LF</code> and try reopening the dev container.</p> <p>From a terminal, in whatever directory on your host machine you keep your coursework projects, you should clone your repository. Open that directory as a workspace in VSCode and then reopen the workspace in a dev container.</p>"},{"location":"threads/sde/testing/tk_testing_storage_final/#1-guided-code-read","title":"1. Guided Code Read","text":"<p>Before writing any code, you should review the existing repository to understand the context. Read these files in the following order:</p> <ol> <li> <p>README.md     Overview of the project, technical stack (Python 3.14, FastAPI), and developer tooling commands (<code>uv</code>, <code>pytest</code>, etc.).</p> </li> <li> <p>scripts/run-qa.sh     The master script that runs all checks (linting, formatting, type checking, tests). Open and read the commands of this <code>bash</code> script to understand what it does. Run this frequently and always before merging back into main! Go ahead and try running this script to be sure your dev container installed properly by running this command in the built-in terminal: <code>./scripts/run-qa.sh</code></p> </li> <li> <p>src/models/link.py     Defines the <code>Link</code> data model using Pydantic. Note the fields <code>slug</code> and <code>target</code>.</p> </li> <li> <p>src/store/json_file_io.py     A low-level wrapper around Python's <code>json</code> module. Note that <code>load()</code> is implemented, but <code>persist()</code> is just a stub.</p> </li> <li> <p>src/store/link_store.py     The main class you will be implementing. It uses <code>JSONFileIO</code> to save data. Note the existing <code>_load_data</code> helper and the empty method stubs.</p> </li> <li> <p>test/store/test_json_file_io_unit.py &amp; test/store/test_link_store_unit.py     Examples of how to unit test these classes using <code>unittest.mock</code> to isolate them from the file system.</p> </li> <li> <p>test/conftest.py     Configuration for <code>pytest</code>. Notice how it handles the <code>--integration</code> flag to skip slow tests by default.</p> </li> <li> <p>AGENTS.md     When you get to the steps that ask you to use the copilot agentic AI while working in this repo, this file will be added to the context window of all interactions. This file gives an agent key context for successfully contributing to the file.</p> </li> </ol>"},{"location":"threads/sde/testing/tk_testing_storage_final/#2-implementation-and-testing-steps","title":"2. Implementation and Testing Steps","text":"<p>Follow these steps to complete the assignment.</p> <p>Required <code>git</code> Commit Workflow for this Task</p> <p>Work for EVERY STEP below must be done on its own branch!</p> <p>For steps 1 and 2 specifically, add your failing unit tests (not yet implementing the method) and make a commit with message (e.g. for step 1 \"test: LinkStore.list() unit test failing\"). Then implement <code>LinkStore.list()</code> and get to a green passing test. You should have 100% coverage when running the <code>run-qa.sh</code> script. Make a commit with this implementation (use a <code>feat:</code> prefix to the commit message and describe what you did). Finally, merge into <code>main</code>.</p> <p>There are seven steps in this task and each needs its own branch and merge commit!</p> <p>You are free to chose your own branch names, as long as they are descriptive, but if you'd like some conventional names here are some fine examples:</p> <pre><code>1. `wip-linkstore-list-unit`\n2. `wip-linkstore-put-unit`\n3. `wip-linkstore-delete-unit`\n4. `wip-jsonfileio-persist-integration`\n5. `wip-jsonfileio-persist-unit`\n6. `wip-linkstore-integration`\n7. `wip-fixture-refactor`\n</code></pre>"},{"location":"threads/sde/testing/tk_testing_storage_final/#phase-1-linkstore-unit-test-and-implement","title":"Phase 1: <code>LinkStore</code> - Unit Test and Implement","text":"<p>In this phase, you will implement the logic in <code>LinkStore</code>. Do not edit <code>JSONFileIO</code> yet. We will mock the persistence layer to ensure our logic is correct without relying on real files.</p>"},{"location":"threads/sde/testing/tk_testing_storage_final/#step-1-implement-linkstorelist","title":"Step 1: Implement <code>LinkStore.list()</code>","text":"<p>You are practicing test-driven development in this step. Implement your unit test(s) for <code>list</code> and implementation without copilot agentic assistance. Be sure your test(s) for <code>list</code> fail. Commit your failing tests following the instructions in the notice box above titled \"Required <code>git</code> Commit Workflow for this Task\".</p> <p>Now that you have failing test(s), work on improving your implementation and getting it to a passing test.</p> <p>Once you have passing tests and 100% coverage, try using a copilot agent to code review your unit test(s) and implementation of <code>list</code>, without making any direct changes to your code, and seeing if you agree with any critiques enough to use them. If there are critiques you are unsure of, use this as an opportunity to learn and reflect on your initial implementation or testing strategy. </p> <ol> <li>Establish a branch for this step.</li> <li>Write unit test(s) for <code>list()</code> in <code>test/store/test_link_store_unit.py</code>.<ul> <li>Mock <code>JSONFileIO</code> to return some sample data.</li> <li>Assert that <code>list()</code> returns a dictionary with the correct <code>Link</code> objects.</li> <li>Crucial: Test that <code>list()</code> returns a copy or new dictionary, not a reference to internal state.</li> <li>Run the test to be sure it fails.</li> </ul> </li> <li>Commit your failing test(s)</li> <li>Implement <code>LinkStore.list()</code> to pass the test. Be sure the <code>run-qa.sh</code> script runs successfully and has 100% coverage.</li> <li>Commit your passing test(s).</li> <li>Have copilot perform a code review of just your implementation and tests and act on any suggestions you believe improve your code after investing the time and thought to understand them.</li> <li>Make a final commit (if needed).</li> <li>Merge back into <code>main</code> with a merge commit.</li> </ol>"},{"location":"threads/sde/testing/tk_testing_storage_final/#step-2-implement-linkstoreput","title":"Step 2: Implement <code>LinkStore.put()</code>","text":"<p>Follow the exact same process for implementing the <code>put</code> method as you did the <code>list</code> method above. Write your initial test(s) and implementation without the assistance of the copilot agent.</p> <ol> <li>Establish a branch for this step.</li> <li>Write a unit test for <code>put()</code> in <code>test/store/test_link_store_unit.py</code>.<ul> <li>use <code>Link</code> model to create a valid link.</li> <li>Call <code>put()</code>.</li> <li>Verify that the internal state is updated.</li> <li>Crucial: Verify that <code>self._storage.persist()</code> was called with the correct data. You may need to inspect how <code>link.model_dump()</code> works in Pydantic documentation.</li> <li>Run the test to be sure it fails.</li> </ul> </li> <li>Commit your failing test(s).</li> <li>Implement <code>LinkStore.put()</code> to pass the test.</li> <li>Commit your passing test(s). Be sure the <code>run-qa.sh</code> script runs successfully and has 100% coverage.</li> <li>Have copilot perform a code review of just your implementation and tests and act on any suggestions you believe improve your code after investing the time and thought to understand them.</li> <li>Make a final commit (if needed).</li> <li>Merge back into <code>main</code> with a merge commit.</li> </ol>"},{"location":"threads/sde/testing/tk_testing_storage_final/#step-3-implement-linkstoredelete","title":"Step 3: Implement <code>LinkStore.delete()</code>","text":"<p>For this step, you will gain experience working with a coding agent to help you draft unit tests that fail, before drafting an implementation that passes. You are following the same workflow as above, however, this time your goal is to have copilot write unit tests that fail first, you review them and be sure the fail, then have copilot draft an implementation.</p> <ol> <li>Establish a branch for this step.</li> <li>Prompt copilot write a unit test for <code>delete()</code> in <code>test/store/test_link_store_unit.py</code> without modifying the implementation of <code>LinkStore.delete</code> because you want the tests to fail first.<ul> <li>Be sure copilot does not implement <code>delete</code>'s functionality yet. If it does, revert the implementation and reject that change.</li> <li>Audit the test function(s) copilot generated and convince yourself they are correct and make sense. If you cannot, try getting to a point of feeling convinced whether that is through editing them yourself, attempting one more generation, or rewriting yourself by hand.</li> <li>Delete should persist the changes via JSONFileIO (similar to how <code>put</code> does).</li> <li>Run the test to be sure it fails.</li> </ul> </li> <li>Commit your failing test(s)</li> <li>Prompt copilot to implement <code>LinkStore.delete()</code> and run the qa script until all tests pass with 100% coverage.</li> <li>Review the implementation. Modify and/or improve upon it if needed. Run the QA script yourself to convince yourself of any changes you made.</li> <li>Commit the passing implementation(s).</li> <li>Merge back into <code>main</code> with a merge commit.</li> </ol>"},{"location":"threads/sde/testing/tk_testing_storage_final/#phase-2-jsonfileio-persistence-integration-then-unit-tests","title":"Phase 2: JSONFileIO Persistence - Integration then Unit Tests","text":"<p>Now we will implement the actual JSON file writing capability.</p> <p>Rather than implementing unit tests for <code>JSONFileIO</code> first, which will require <code>patch</code>'ing underlying behavior, it is actually more productive to </p>"},{"location":"threads/sde/testing/tk_testing_storage_final/#step-4-integration-test-and-implementation-for-jsonfileiopersist","title":"Step 4: Integration Test and Implementation for <code>JSONFileIO.persist()</code>","text":"<p>Write this integration test without agentic assistance. Searching the web or engaging an LLM chat with curiosity is fine, as needed.</p> <ol> <li>Establish a branch for this step.</li> <li>Open <code>test/store/test_json_file_io_integration.py</code>.</li> <li>Write a AAA test for <code>persist</code> marked with <code>@pytest.mark.integration</code>.<ul> <li>Use the <code>tmp_path</code> fixture (provided by pytest) to create a temporary file path.</li> <li>Initialize <code>JSONFileIO</code> with this path.</li> <li>Call <code>persist()</code> with some data.</li> <li>Read the file back manually (using standard python functionality) and verify the content matches.</li> </ul> </li> <li>Run <code>uv run pytest</code> (notice your test is skipped because it is marked as an integration test!).</li> <li>Run <code>uv run pytest --integration</code> (your test should fail, because <code>persist</code> is not implemented).</li> <li>Make a commit with your failing tests.</li> <li>Implement the <code>persist</code> method in <code>src/store/json_file_io.py</code>.</li> <li>Run the integration tests to be sure they pass. Run the QA script and be sure you have coverage.</li> <li>Make a commit with your passing tests.</li> <li>Engage with the copilot agent to code review your implementation and test. Act on any suggestions you understand and believe improve upon your first attempt.</li> <li>Make a final commit, if needed.</li> <li>Merge back into main with a merge commit.</li> </ol>"},{"location":"threads/sde/testing/tk_testing_storage_final/#step-5-unit-test-for-jsonfileiopersist","title":"Step 5: Unit Test for <code>JSONFileIO.persist()</code>","text":"<p>Now that you have a working implementation of <code>persist</code> that passes integration tests, your task moves to unit testing <code>persist</code>. This will require a <code>patch</code> of the underlying file system functionality like we discussed in class. You should complete a first draft of this testing without using copilot agent and can refer to the unit test for <code>load</code> for inspiration.</p> <p>Reflection question: Why might you want to add unit tests to <code>persist</code> even though you already have 100% coverage with integration tests? For some motivation, try running just your unit tests with a coverage report and see what you find:</p> <ul> <li><code>uv run pytest --cov=src --cov-report=term-missing</code></li> </ul> <p>Since you already have a working implementation of <code>persist</code>, you cannot follow the red-green-refactor workflow of test-driven development. That's OK. You can work toward getting 100% test coverage with your unit tests and this has value (you should be able to reason through why it has value).</p> <p>Rather than prescribing the exact set of steps here, go ahead and follow a branch/commit/merge workflow based on what we have done so far. Work toward writing unit tests for your implementation of <code>persist</code> with a goal of reaching 100% coverage using the command above. Once you have a working implementation yourself, try using the agent to code review. Merge your work back into <code>main</code> with a merge commit when done.</p>"},{"location":"threads/sde/testing/tk_testing_storage_final/#phase-3-linkstore-integration-tests","title":"Phase 3: LinkStore Integration Tests","text":"<p>Finally, ensure <code>LinkStore</code> works with the real <code>JSONFileIO</code> instance via integration testing.</p> <p>You hopefully cannot follow a red-green-refactor test-driven development pattern here and should be able to describe why not with confidence. That said, you should also be able to describe why this integration test will provide unique value to your test suite.</p>"},{"location":"threads/sde/testing/tk_testing_storage_final/#step-6-integration-tests","title":"Step 6: Integration Tests","text":"<ol> <li>Go ahead and setup a branch for completing your integration testing of LinkStore. You should form commits regularly as you make progress.</li> <li>Working in <code>test/store/test_link_store_integration.py</code>.</li> <li>Add tests marked with <code>@pytest.mark.integration</code>.</li> <li>Test 1: Write a test that verifies <code>list</code> works with empty and non-empty initial files.</li> <li>Test 2: Write a test that verifies <code>put</code> works and persists the deletion. </li> <li>Test 3: Write a test that verifies <code>delete</code> works and persists the deletion.</li> <li>Run the qa script to be sure all of your tests are passing.</li> <li>Merge your work back into <code>main</code> with a merge commit.</li> </ol> <p>Important Reflection Question</p> <p>Think back to before these implementations were completed. Suppose you were tasked with implementing and testing <code>LinkStore</code> while your teammate is tasked with implementing and testing <code>JSONFileIO</code> at the same time.</p> <ol> <li>Why would you start with unit tests?</li> <li>Why would your teammate start with integration tests?</li> <li>Why would it have been challenging for your teammate to start with unit tests on <code>JSONFileIO</code>?</li> <li>Generally, in future work, when testing, how would you decide whether to start with unit or integration tests?</li> <li>Even though you had 100% coverage of <code>JSONFileIO</code> before adding 100% coverage with unit tests, and you had 100% unit test coverage of <code>LinkStore</code> before going back and adding integration tests, what unique value did those tests add, respectively?</li> </ol> <p>These are important concepts to feel develop some intuition and confidence in heading into the next quiz.</p>"},{"location":"threads/sde/testing/tk_testing_storage_final/#phase-3-refactoring","title":"Phase 3: Refactoring","text":""},{"location":"threads/sde/testing/tk_testing_storage_final/#step-7-refactoring-tests-with-fixtures-andor-helper-methods","title":"Step 7: Refactoring Tests with Fixtures and/or Helper Methods","text":"<p>Perform a code review of the four files you modified in this task.</p> <p>In the test files, look for redundancy in the Arrange step of your tests in a given file. Find at least one redundant set of steps and refactor those steps out to a fixture. Retest to confirm everything is still green, then form a new commit. You do not need to branch for this. You must write and valuably use at least one custom fixture in your tests.</p> <p>Finally, go back and review your implementations of <code>LinkStore</code> and <code>JSONFileIO</code>. If there is any redundant code, refactor it out to a helper method. You may have already done this and, if so, there is no additional work to do here. If not, again test for coverage after the refactor, and make a commit (no branch needed). We will be looking to ensure no redundancy.</p>"},{"location":"threads/sde/testing/tk_testing_storage_final/#3-hand-in","title":"3. Hand-in","text":"<p>Hand in will open by Wednesday.</p>"},{"location":"threads/system/","title":"System Design","text":"<p>This is the \"big picture\" view of software components coming together to form a production system. In this thread you will learn how the front end, back end, database (persistence), and other production/cloud components are orchestrated to create a functioning system.</p> <p>Understanding how systems of components interact allows you to build applications that are fast, secure, and capable of handling many users. In the industry, system design knowledge is essential for architecting modern platforms and is a major focus of high-level engineering interviews.</p> <p>System design concerns will pick-up as we get deeper into the course.</p> Readings Topic RD09 Communicating in the Software Development Lifecycle RD10 Advancements in Communication between Computing Systems RD11 Human Communication and API Design: A Shared Foundation RD14 Key Concepts in HTTP RD15 Toward Designing and Formally Specifying APIs RD16 FastAPI and Pydantic Tutorial RD17 Dependency Injection RD19 Unit, Integration, and E2E Testing API Routes Lessons Topic LS05 Designing with Layers and Composition LS10 HTTP API Design LS11 Dependency Injection Lab LS12 API Design Working Day LS13              Lesson 13           LS14              Lesson 14           Tasks Topic TK06 API Design (Part 1) TK06 API and Service-layer Implementation (Part 2)"},{"location":"threads/system/apis/","title":"Communicating for Shared Understanding between Humans and Systems (APIs)","text":"<p>Communication is the foundation of software engineering. It\u2019s what allows groups of people with different roles, expertise, and perspectives to come together to achieve something bigger than any one person could accomplish alone. Whether it\u2019s a product manager explaining a feature request to an engineering team, designers aligning with software engineers to bring a vision to life, or site reliability engineers ensuring seamless deployments, every step of the software development life cycle (SDLC) relies on effective communication. </p> <p>This same principle applies to software systems. Application Programming Interfaces (APIs) are the tools that enable different pieces of software to exchange information, coordinate actions, and work together. Just as humans need structured communication to succeed in a project, systems need structured interfaces to achieve interoperability. </p> <p>In this sequence of readings, we\u2019ll explore how communication, both among humans and between systems, is the key to creating shared understanding. We\u2019ll emphasize that learning to communicate effectively in both domains is a skill that will not only make you a better software engineer but also prepare you for success in any career.</p> Readings Topic RD09 Communicating in the Software Development Lifecycle RD10 Advancements in Communication between Computing Systems RD11 Human Communication and API Design: A Shared Foundation RD14 Key Concepts in HTTP RD15 Toward Designing and Formally Specifying APIs RD16 FastAPI and Pydantic Tutorial Lessons Topic LS05 Designing with Layers and Composition"},{"location":"threads/system/apis/1-communication/","title":"1. Communication in the Software Development Lifecycle","text":"<p>Effective communication is critical throughout the software development life cycle (SDLC). Whether a client is requesting a feature or multiple teams are collaborating on a project, intentional communication strategies help ensure that everyone remains on the same page. Poor communication, on the other hand, can lead to misunderstandings, delays, and even project failure.</p>"},{"location":"threads/system/apis/1-communication/#how-communication-enables-collective-success","title":"How Communication Enables Collective Success","text":"<p>One of the most remarkable aspects of the SDLC is how groups of people with very different backgrounds and roles band together to achieve a goal bigger than anyone could take on individually. A successful project relies on shared understanding\u2014each person must know enough to contribute meaningfully, even if their expertise lies in a specific area. Everyone, from the client to the project manager, to the engineers and designers, needs to clearly understand the goals and desired outcomes of the project.</p> <p>Good communication helps:</p> <ul> <li>Clarify Goals: Ensuring everyone has a unified understanding of what success looks like.</li> <li>Distribute Knowledge: Allowing team members to understand the context they need to make informed decisions.</li> <li>Align Efforts: Making sure that all work, no matter how specialized, contributes to the same broader goal.</li> </ul>"},{"location":"threads/system/apis/1-communication/#intentional-communication-strategies","title":"Intentional Communication Strategies","text":"<p>Some high-level strategies and concerns are pervasive in software development. We will explore these in more depth as the course goes on, but it is worth highlighting a few now:</p> <ol> <li> <p>Using Shared Resources (e.g. artifacts like files and documents)</p> <ul> <li>Architectural Design Records, Design documents, technical specifications, and wireframes help clarify ideas.</li> <li>These artifacts act as shared resources, ensuring all stakeholders are able to rally around the same ideas before they are built. This idea is not new to software development, think of blue prints and artistic renderings of buildings and spaces in the construction industry as a predecessor.</li> </ul> </li> <li> <p>Choosing the Right Medium for the Message</p> <ul> <li>Synchronous Communication: Lessons, video calls, or real-time collaboration are great for brainstorming or addressing urgent issues.</li> <li>Asynchronous Communication: Emails, project management tools, and documentation are better for detailed updates and tracking progress.</li> </ul> </li> <li> <p>Adjusting Formality Based on Context</p> <ul> <li>Formal communication, such as signed contracts or requirements documents, cement guarantees and expectations. This is especially important between two disparate parties or firms.</li> <li>Informal discussions, like Slack chats or quick hallway conversations, can promote collaboration and generate ideas. These are more useful internally, within a team or organization.</li> </ul> </li> <li> <p>Tightening Feedback Loops</p> <ul> <li>Regular check-ins, code reviews, and demos ensure that misunderstandings are caught early.</li> <li>Feedback helps teams refine their work, aligning closer to the original intent.</li> </ul> </li> </ol>"},{"location":"threads/system/apis/1-communication/#communication-across-roles","title":"Communication Across Roles","text":"<p>Each role in the SDLC brings unique perspectives and needs, making effective communication even more important.</p> <ul> <li> <p>Client to Project Manager: Clients communicate high-level goals, such as desired features or outcomes. A project manager translates these goals into actionable tasks for the development team. Without clarity, the team may deliver something that doesn\u2019t meet the client\u2019s expectations.</p> </li> <li> <p>Designers to Software Engineers: User interface (UI) designers create wireframes or mockups that software engineers implement. Miscommunication about design elements, like color schemes or interaction behaviors, can result in poor user experiences.</p> </li> <li> <p>Software Engineers to Site Reliability Engineers: Software engineers rely on site reliability engineers (SREs) to deploy software to production environments. Poorly communicated deployment requirements can lead to configuration errors, downtime, or failed releases.</p> </li> </ul> <p>As a foreshadowing, we will soon turn our attention toward communication between different layers of a software system, such as front-end and back-end. One hand-waiving analogy of a system layer is like a different role in a team: it has its own concerns and jobs different from the others yet it still needs to work in coordination with the others. Sometimes these layers are implemented in different programming languages and we will need to pay careful attention to how communication between these layers ensures no information is lost in translation.</p>"},{"location":"threads/system/apis/1-communication/#what-happens-when-communication-breaks-down","title":"What Happens When Communication Breaks Down","text":"<p>Poor communication can have serious consequences at every stage of development:</p> <ul> <li>Client Dissatisfaction: Vague requirements or misaligned priorities lead to deliverables that don\u2019t meet the client\u2019s needs.</li> <li>Missed Deadlines: Unclear expectations create confusion about what tasks need to be completed and when.</li> <li>Team Frustration: Miscommunication fosters blame and reduces morale, impacting productivity.</li> <li>Technical Debt: Lack of clarity around implementation can result in rushed, poorly designed solutions that need to be fixed later.</li> </ul>"},{"location":"threads/system/apis/1-communication/#benefits-of-good-communication","title":"Benefits of Good Communication","text":"<p>When communication is intentional and well-structured, it:</p> <ul> <li>Clarifies Goals and Outcomes: Ensures everyone knows what they\u2019re working toward and why it matters.</li> <li>Prevents Scope Creep: Clear requirements help avoid last-minute changes that derail timelines.</li> <li>Improves Collaboration: Shared understanding across teams reduces friction and promotes teamwork.</li> <li>Minimizes Rework: Aligned expectations mean less time spent correcting misunderstandings.</li> <li>Builds Trust: Transparent communication fosters trust between clients and development teams.</li> </ul> <p>Fred Brooks on Communication</p> <p>\"The hardest single part of building a software system is deciding precisely what to build. No other part of the conceptual work is so difficult as establishing the detailed technical requirements, including all the interfaces to people, to machines, and to other software systems. No other part of the work so cripples the resulting system if done wrong. No other part is more difficult to rectify later.\" </p> <p>\u2014 Fred Brooks, \"No Silver Bullet: Essence and Accidents of Software Engineering,\" 1986</p>"},{"location":"threads/system/apis/2-api-history/","title":"2. Advancements in Communication between Computing Systems","text":"<p>As we turn our attention to communication between systems, that software engineers design and implement, it is helpful to have some historical context. Just as structured communication enables humans to align on shared goals, advances in computing systems have focused on making the exchange of information between machines more structured, scalable, and reliable. This history lays the foundation for understanding how modern APIs play a central role in today\u2019s interconnected digital world.</p>"},{"location":"threads/system/apis/2-api-history/#the-early-days-batch-processing-and-punch-cards","title":"The Early Days: Batch Processing and Punch Cards","text":"<p>In the 1950s and 60s, computers were massive, room-sized machines, and communication with them was painstakingly slow. Users prepared instructions using punch cards\u2014thin cardboard sheets with holes representing binary commands. The cards were fed into the computer in batches, and the machine would process them before producing an output, often printed on paper. </p> <p>While revolutionary at the time, this method lacked interactivity. Communication was strictly one-way, requiring users to wait for results before making adjustments.</p>"},{"location":"threads/system/apis/2-api-history/#the-shift-to-real-time-interaction-time-sharing-systems","title":"The Shift to Real-Time Interaction: Time-Sharing Systems","text":"<p>In the 1960s, time-sharing systems introduced real-time interaction with computers. This is when shells, like the command-line interfaces software engineers (and you, in 211 and this course!) still use today, rose in prominence.</p> <p>These systems allowed multiple users to work on the same machine simultaneously via terminals. Communication became more dynamic, enabling developers to write, test, and debug programs interactively.</p> <p>This period also saw the rise of early message-passing protocols, as systems began to share data across connected machines. However, these interactions were often bespoke and required deep technical knowledge, limiting their accessibility to a small group of experts. There were no standards for information exchange between two systems or programs.</p>"},{"location":"threads/system/apis/2-api-history/#networking-revolution-the-arpanet-and-protocols","title":"Networking Revolution: The ARPANET and Protocols","text":"<p>The creation of the ARPANET in 1969\u2014a precursor to the internet\u2014marked a significant milestone in system communication. For the first time, computers located miles apart could exchange data. Early protocols like NCP (Network Control Protocol) and later TCP/IP (Transmission Control Protocol/Internet Protocol) laid the groundwork for modern networking by standardizing how systems should format and transmit messages.</p> <p>With standardized protocols, communication across systems became more predictable and scalable. These innovations enabled the development of distributed systems, where multiple computers could collaborate on a single task. It\u2019s not unlike how the advent of telephone systems unlocked new forms of collaboration and communication among people.</p>"},{"location":"threads/system/apis/2-api-history/#the-rise-of-the-web-http-and-html","title":"The Rise of the Web: HTTP and HTML","text":"<p>The invention of the World Wide Web in the 1990s transformed how systems\u2014and people\u2014interacted. HTTP (Hypertext Transfer Protocol) became the standard for requesting and transmitting resources over the web, while HTML (Hypertext Markup Language) provided a consistent way to display those resources.</p> <p>This period also saw the emergence of APIs in their earliest forms. Web APIs allowed applications to request data or functionality from other services, albeit in a relatively unstructured and inconsistent manner compared to today.</p>"},{"location":"threads/system/apis/2-api-history/#modern-apis-rest-and-beyond","title":"Modern APIs: REST and Beyond","text":"<p>In the early 2000s, REST (Representational State Transfer) emerged as a simpler, more flexible approach to API design. RESTful APIs leveraged familiar web methods, such as \"retrieving\" or \"updating\" resources, to create predictable and scalable communication between clients and servers.</p> <p>Today, APIs are fundamental to how most modern web and mobile applications function. Applications are often split into at least two conceptual parts: the front-end, which is what users interact with, and the back-end, which processes data and handles the core functionality. These two parts communicate via APIs. A front-end might send a request for a user's profile data, and the back-end would respond with the necessary details structured in an agreed-upon format. When you use an app like this, your request is communicated over the internet to a data center tens or hundreds of miles away, and the backend responds to it, all in a split second. It's a marvel of communication!</p> <p>APIs also power many popular services you use daily. Music services like Spotify use APIs to send requests from your app to their servers, fetching your playlists or suggesting new tracks. Social media platforms work similarly\u2014your app communicates with their API to load your feed, post updates, or send messages. </p> <p>Beyond individual applications, APIs also enable cross-application integration. For instance, logging into a platform using credentials from Google, Facebook, or GitHub is made possible by authentication APIs. Fitness apps syncing data with health dashboards or e-commerce platforms coordinating with payment processors are other examples of how APIs allow disparate systems to integrate with each other.</p>"},{"location":"threads/system/apis/2-api-history/#lessons-from-history","title":"Lessons from History","text":"<p>Each step in the evolution of communication in computing systems reflects a broader goal: making it easier for machines and humans to exchange information. Standardized protocols, structured formats, and accessible APIs have all contributed to this progress, ensuring that systems can collaborate effectively. These are key tools toward arriving at shared understanding between systems and people.</p>"},{"location":"threads/system/apis/3-api-design/","title":"3. Human Communication and API Design: A Shared Foundation","text":"<p>Building on the importance of communication in the software development life cycle, we now turn our attention to APIs\u2014Application Programming Interfaces. Just as intentional communication strategies help teams of humans align and collaborate, APIs serve as the structured communication layer that allows different software systems to work together effectively.</p>"},{"location":"threads/system/apis/3-api-design/#communication-as-a-bridge","title":"Communication as a Bridge","text":"<p>In both human and system communication, shared understanding is built through structured exchanges of information. Consider how people communicate:</p> <ul> <li>A sender conveys a message.</li> <li>A receiver interprets the message.</li> <li>A shared language or set of conventions ensures both parties understand each other.</li> <li>Feedback confirms whether the message was received as intended.</li> </ul> <p>Now consider how client-server APIs classically function:</p> <ul> <li>A client (the sender) sends a request to a server.</li> <li>The server (the receiver) processes the request.</li> <li>Both client and server rely on shared protocols and formats, such as predefined rules for requests and responses, to ensure clarity. API requests typically include:<ul> <li>\"who\" the recipient is. The who of an API request is typically a server address (e.g. <code>api.instagram.com</code>) and not at all \"who\" in the human sense.</li> <li>\"where\" the resource is found via routing the request once it reaches the server. Typically this includes some identifying information (e.g. a path like <code>/profiles/therealkrisjordan</code> or <code>/post/1234</code>).</li> <li>\"what\" the action being requested on this resource is (verb). Is the action asking for data about resource? Creating new or updating existing resources? Deleting a resource?</li> <li>Additional information needed to process the specific request. This information tends to be either metadata (such as what kind of format you would like in response or some identifying information of you, the sender) or data about the resource (such as a new profile bio when saving your social service profile).</li> </ul> </li> <li>The server\u2019s response serves as feedback, confirming the outcome of the interaction.<ul> <li>Status codes: did the request succeed or fail? If there was an error, what kind of error?</li> <li>Data requested: most requests are looking for information, so the response includes the data in the format requested.</li> <li>Metadata about the response that will be useful to the client when interpreting it.</li> </ul> </li> </ul> <p>The structure in both cases\u2014human and system\u2014is essential to avoiding misunderstandings and ensuring smooth collaboration.</p>"},{"location":"threads/system/apis/3-api-design/#analogies-in-communication","title":"Analogies in Communication","text":"<p>To help solidify these concepts, let\u2019s draw some direct parallels between communication in the SDLC and API design:</p> <ul> <li>Project Specifications and API Specifications: Just as an operating agreement or design document clarifies expectations for human collaborators, an API specification defines how systems should interact. An API can specify which operations are available, what input is required, and what kind of response to expect.</li> </ul> <p>OpenAPI Initiative</p> <p>In the past decade, there has been a serious push toward API specification standards via the Open API Initiative. We will be making use of OpenAPI standards soon in this course!</p> <ul> <li> <p>Language and Shared Formats: In human communication, language provides the structure for expressing ideas. In system communication, shared formats define how data is packaged, such as using standard data encodings (e.g. JSON - JavaScript Obect Notation or XML eXtensible Markup Language) to make the information predictable and easy to interpret.</p> </li> <li> <p>Feedback Loops in Teams and Systems: Teams rely on feedback to refine their work, whether through design critiques or user testing. Similarly, APIs provide feedback through structured responses that indicate success, failure, or additional actions required.</p> </li> </ul>"},{"location":"threads/system/apis/3-api-design/#empathy-in-api-design","title":"Empathy in API Design","text":"<p>Empathy is just as critical in API design as it is in human-to-human communication. In both, structure minimizes ambiguity and reduces the effort required to interpret messages. Imagine receiving a vague email like, \"Please fix it ASAP,\" without knowing what \"it\" refers to or how urgent the issue really is. Similarly, an API that returns an error message like, \u201cSomething went wrong,\u201d leaves developers guessing about what needs to be fixed.</p> <p>API designers must consider the needs, constraints, and workflows of the developers who will use their APIs. This means thinking beyond technical functionality to focus on usability and developer experience. Here are some best practices when designing APIs that are a joy to use:</p> <ul> <li> <p>Clear Documentation: Developers should be able to understand how to use an API without guessing. Documentation should be well-organized and provide examples that illustrate typical use cases.</p> </li> <li> <p>Meaningful Feedback: If something goes wrong, an API should return detailed and actionable messages. For instance, instead of a generic \u201cInvalid request,\u201d it should specify, \u201cMissing required field: city.\u201d</p> </li> <li> <p>Consistency and Predictability: Endpoints and request structures should follow predictable patterns, reducing the mental effort needed to learn and use the API.</p> </li> <li> <p>Anticipating User Needs: Think about common workflows or challenges developers face and design the API to simplify these tasks. For example, providing optional filters or flexible ways to retrieve data can save time and effort for users.</p> </li> </ul> <p>These practices aren't restricted to API Design, they're generally applicable principles of human-centered design across many domains!</p> <p>Empathy ensures that APIs are not only functional but also intuitive, reducing frustration and increasing developer productivity. When designers think about the people behind the code, and who their systems serve, they create tools that foster collaboration and innovation.</p>"},{"location":"threads/system/apis/4-http/","title":"4. Key Concepts in HTTP","text":"<p>Have you ever wondered how your favorite apps communicate with servers behind the scenes? When you leave a comment on Instagram, how does the server know which post you\u2019re commenting on and what you wrote? In this section, we'll explore how modern software systems communicate via HTTP (Hypertext Transfer Protocol), focusing on APIs (Application Programming Interfaces) and the structure that makes it all work.</p>"},{"location":"threads/system/apis/4-http/#key-terms","title":"Key Terms","text":"<p>Understanding these concepts forms the foundation of all API interactions, whether you're building a simple weather app or a complex social media platform.</p> <ol> <li> <p>Sender (The Client): This is who initiates the communication - like your mobile app or web browser. The client is responsible for:</p> <ul> <li>Formatting requests correctly</li> <li>Managing user interactions</li> <li>Handling responses appropriately</li> <li>Retrying failed requests when necessary</li> </ul> </li> <li> <p>Receiver (The Server): This processes the request and sends back information. The server\u2019s responsibilities include:</p> <ul> <li>Validating incoming requests</li> <li>Processing data and managing resources</li> <li>Ensuring security</li> <li>Providing appropriate responses</li> </ul> </li> <li> <p>Medium (The Channel): How the message travels - in modern APIs, this is typically HTTP. The medium:</p> <ul> <li>Ensures reliable delivery of messages</li> <li>Handles connection management</li> </ul> </li> <li> <p>Message (The Request/Response): The actual information being exchanged. Messages must be properly formatted and complete with all necessary information.</p> </li> <li> <p>Feedback (The Server\u2019s Response): Confirmation that the message was received and processed. Good feedback:</p> <ul> <li>Confirms success or failure</li> <li>Provides helpful error messages</li> <li>Returns requested data</li> <li>Includes relevant metadata</li> </ul> </li> </ol> <p>A common, helpful analogy for APIs at a conceptual level is to think of it like ordering food at a restaurant:</p> <ul> <li>You (the sender/client) place an order.</li> <li>The waiter (the medium) carries the message back to the kitchen.</li> <li>Your order (the message) contains what you want.</li> <li>The kitchen (the receiver/server) processes it.</li> <li>The waiter (the medium) carries the food from the kitchen to you.</li> <li>The food arriving (the feedback) confirms your order was received and accurately processed.</li> </ul>"},{"location":"threads/system/apis/4-http/#http-the-medium-of-modern-apis","title":"HTTP: The Medium of Modern APIs","text":"<p>HTTP provides a standardized way for clients and servers to talk to each other. Understanding HTTP important for any software engineer working with web or mobile applications.</p>"},{"location":"threads/system/apis/4-http/#resources-the-nouns-of-http","title":"Resources: The Nouns of HTTP","text":"<p>In HTTP, everything is a resource - think of these as the \"things\" your API can interact with. Resources are fundamental to REST (Representational State Transfer), the most common architectural style for modern APIs.</p> <p>Examples of resources:</p> <ul> <li>A user profile</li> <li>A social media post</li> <li>A collection of photos</li> <li>A comment thread</li> </ul> <p>Each resource has its own URL (Uniform Resource Locator). URLs are technically opaque strings, but in real-world applications tend to be structured hierarchically, inspired by directory path strings, like:</p> <pre><code>https://api.instagram.com/users/123/posts/456/comments\n</code></pre> <p>Breaking down this URL:</p> <ul> <li><code>https://</code>: This is the protocol of the connection (cryptographically secure HTTP)</li> <li><code>api.instagram.com</code>: This is the domain that addresses \"who\" the request is sent to.</li> <li><code>users/123</code>: Identifies a specific user.</li> <li><code>posts/456</code>: A specific post by that user.</li> <li><code>comments</code>: The collection of comments on that post.</li> </ul>"},{"location":"threads/system/apis/4-http/#http-methods-the-verbs","title":"HTTP Methods: The Verbs","text":"<p>HTTP methods define what action you want to perform on a resource. Understanding each method\u2019s purpose and proper use is valuable:</p> HTTP Method Safety Idempotency Description GET Safe Idempotent Fetches information without modifying any resources on the server POST Not Safe Not Idempotent Creates new resources on the server, with each identical request potentially creating multiple resources PUT Not Safe Idempotent Completely replaces an existing resource, with repeated identical requests having the same effect PATCH Not Safe Idempotent Partially modifies an existing resource, only updating specified fields DELETE Not Safe Idempotent Removes a resource from the server, with repeated identical requests having the same effect <p>Safety in web APIs is like reading a book versus writing in it. When a method is \"safe,\" it means it only reads or looks at data without changing anything on the server - like how reading a book doesn't change its contents. GET is the only safe method since it just retrieves information, while methods like POST, PUT, PATCH, and DELETE are not safe because they modify data on the server, just like how writing in a book changes its contents.</p> <p>Idempotency is about whether doing something multiple times has the same effect as doing it once. It's a very important property in many system designs. Think of it like pressing the button at a crosswalk: the second or third time you push it before the lights change has no additional effect (as much as we wish it sped the process up!). Similarly, saving a file many times in a row in your editor does not create multiple files. Importantly, for well implemented online checkout systems, pressing \"Complete Purchase\" many times does not result in duplicate orders going through. These operations are idempotent. GET, PUT, PATCH, and DELETE are idempotent because repeating the same request should produce you the same result. Contrast this with POST, which is not idempotent, because each request creates something new, like pressing \"Download\" many times on a website may download multiple copies of the same file.</p>"},{"location":"threads/system/apis/4-http/#get","title":"GET","text":"<ul> <li>Examples:<ul> <li>Viewing a user's profile page</li> <li>Retrieving a list of blog posts</li> <li>Fetching search results</li> </ul> </li> <li>Common Use Cases:<ul> <li>Search operations</li> <li>Data retrieval</li> <li>Reading resources</li> <li>Querying system status</li> </ul> </li> </ul>"},{"location":"threads/system/apis/4-http/#post","title":"POST","text":"<ul> <li>Examples:<ul> <li>Adding a new comment to a blog post</li> <li>Creating a new user account</li> <li>Uploading a file to a server</li> </ul> </li> <li>Common Use Cases:<ul> <li>Form submissions</li> <li>File uploads</li> <li>Resource creation</li> <li>Data processing</li> </ul> </li> </ul>"},{"location":"threads/system/apis/4-http/#put","title":"PUT","text":"<ul> <li>Examples:<ul> <li>Updating an entire user profile</li> <li>Replacing a document with a new version</li> <li>Setting a complete configuration</li> </ul> </li> <li>Common Use Cases:<ul> <li>Full resource updates</li> <li>Complete replacements</li> <li>Configuration updates</li> <li>Version management</li> </ul> </li> </ul>"},{"location":"threads/system/apis/4-http/#patch","title":"PATCH","text":"<ul> <li>Examples:<ul> <li>Updating just a user's email address</li> <li>Modifying specific fields in a document</li> <li>Updating part of a configuration</li> </ul> </li> <li>Common Use Cases:<ul> <li>Partial updates</li> <li>Field-specific modifications</li> <li>Resource property adjustments</li> <li>Incremental changes</li> </ul> </li> </ul>"},{"location":"threads/system/apis/4-http/#delete","title":"DELETE","text":"<ul> <li>Examples:<ul> <li>Removing a comment from a post</li> <li>Deleting a user account</li> <li>Removing a file from storage</li> </ul> </li> <li>Common Use Cases:<ul> <li>Resource removal</li> <li>Cleanup operations</li> <li>Account deletion</li> <li>Content management</li> </ul> </li> </ul>"},{"location":"threads/system/apis/4-http/#anatomy-of-api-communication","title":"Anatomy of API Communication","text":"<p>Let\u2019s mock up the big ideas of what happens when you comment on a post on Instagram.</p>"},{"location":"threads/system/apis/4-http/#the-request-client-server","title":"The Request (Client \u2192 Server)","text":"<pre><code>POST https://api.instagram.com/posts/12345/comments\nContent-Type: application/json\nAuthorization: Bearer eyJhbGc...\n\n{\n    \"comment\": \"Great photo!\",\n    \"timestamp\": \"2024-01-26T10:30:00Z\",\n    \"source\": \"mobile_app\"\n}\n</code></pre> <p>This is a simple example of the textual \"envelope\" of an HTTP request contains and what is transmitted over the network from client to server.</p> <ul> <li> <p>Method and URL: The POST method indicates we\u2019re creating a new comment. The URL specifies exactly which resource (the post) we\u2019re interacting with.</p> </li> <li> <p>Headers: Headers provide essential metadata. Examples:</p> <ul> <li><code>Content-Type</code>: Tells the server what format the data is in.</li> <li><code>Authorization</code>: Proves who you are (usually a token).</li> </ul> </li> <li> <p>Body: Contains additional data about the request. In this case, it includes the comment text, the source of the action, and a timestamp.</p> </li> </ul>"},{"location":"threads/system/apis/4-http/#the-response-server-client","title":"The Response (Server \u2192 Client)","text":"<pre><code>HTTP/1.1 201 Created\nContent-Type: application/json\nCache-Control: no-cache\n\n{\n    \"success\": true,\n    \"message\": \"Comment added successfully\",\n    \"comment_id\": 789,\n    \"timestamp\": \"2024-01-26T10:30:01Z\"\n}\n</code></pre> <p>Key components explained:</p> <ul> <li> <p>Status Line: Indicates the HTTP version, status code (201 Created), and a brief status message. We will look at status codes in more depth shortly.</p> </li> <li> <p>Headers: Includes metadata about the response. Examples:</p> <ul> <li><code>Content-Type</code>: Format of the response (JSON).</li> <li><code>Cache-Control</code>: Instructions for how clients should cache the response (or not!)</li> </ul> </li> <li> <p>Body: Contains the actual response data, confirming the action was successful and providing additional context (e.g., the new comment ID).</p> </li> </ul>"},{"location":"threads/system/apis/4-http/#request-response-sequence-diagram","title":"Request Response Sequence Diagram","text":"<p>The above request, response flow can be visualized as follows:</p> <pre><code>sequenceDiagram\n    participant App as Instagram App (Client)\n    participant Server as Instagram Server\n\n    Note over App: User Leaves a Comment\n    App-&gt;&gt;Server: POST /posts/12345/comments\n    Note over Server: Comment Saved Successfully\n    Server--&gt;&gt;App: 201 Created (Success!)\n    Note over App: Comment Shows as Saved </code></pre> <p>This diagram shows:</p> <ol> <li>The client sending the comment to the server.</li> <li>The server processing the comment.</li> <li>A success response being returned to the client.</li> </ol>"},{"location":"threads/system/apis/4-http/#status-codes-convey-servers-handling-of-request","title":"Status Codes Convey Server's Handling of Request","text":"<p>When the server sends a response, the response starts with a status code. You've seen these in the wild: <code>404 Not Found</code> or <code>500 Internal Server Error</code>. There is an intention design to these \"century\" numberings. Responses within each 100-level range share something in common.</p> <ul> <li> <p>Informational (1xx): These are rarely used by API developers and are more for lower-level connection handling.</p> <ul> <li>101 Switching Protocols: Used when an HTTP connection is upgraded to a WebSocket connection.</li> </ul> </li> <li> <p>Success (2xx): Indicates that the request was successfully processed by the server.</p> <ul> <li>200 OK: Request succeeded.</li> <li>201 Created: Resource was created successfully.</li> </ul> </li> <li> <p>Redirection (3xx): Indicates the client needs to take additional action to complete their request.</p> <ul> <li>301 Moved Permanently: The resource has permanently moved to a new URL.</li> <li>302 Found: The resource is temporarily at a different URL.</li> <li>307 Temporary Redirect: Like 302, but the request method must not change.</li> </ul> </li> <li> <p>Client Errors (4xx): Indicates there is something wrong with what the client sending the request is requesting.</p> <ul> <li>400 Bad Request: Invalid syntax or parameters.</li> <li>401 Unauthorized: The request could not be authenticated to a user or the user no longer exists.</li> <li>403 Forbidden: The user making the request does not have permission to perform the requested action.</li> <li>404 Not Found: Resource doesn\u2019t exist.</li> </ul> </li> <li> <p>Server Errors (5xx): Indicates an error happened on the server's end but the problem was not the client's fault.</p> <ul> <li>500 Internal Server Error: Unexpected server error.</li> </ul> </li> </ul> <p>Now that you have a handle on the fundamental concepts of what comprises an HTTP request, we'll take at modern conventions and best practices for designing APIs with these concepts and following best practices.</p>"},{"location":"threads/system/apis/5-api-spec/","title":"5. Toward Designing and Formally Specifying APIs","text":"<p>Now that you have seen the key elements of HTTP at a conceptual level, across the entire request-response journey, let's focus in on how API designers generally use the inputs from a request. Additionally, let's look at how responses are designed.</p> <p>When working with HTTP APIs, there are multiple ways to provide input to a request, each serving a different purpose:</p> <ul> <li>Methods define the type of action the API request is performing.</li> <li>Paths define the structure of an API and uniquely identify resources.</li> <li>Query Parameters allow clients to filter, sort, or refine results without changing the resource's identity.</li> <li>Bodies are used to send structured data in requests, typically for creating or updating resources.</li> <li>Headers provide metadata about the request, such as authentication details or content type preferences.</li> </ul> <p>Understanding these inputs is essential for designing HTTP APIs that are intuitive, efficient, and scalable.</p>"},{"location":"threads/system/apis/5-api-spec/#introduction-to-rest","title":"Introduction to REST","text":"<p>REST (Representational State Transfer) defines a set of architectural principles for designing HTTP APIs that focus on resources and how to design interactions with them. </p> <p>REST emerged from Roy Fielding's 2000 PhD dissertation, where he formalized the architectural principles that had guided his work on the HTTP specifications. While working on HTTP standards, he observed that many distributed systems at the time were tightly coupled, requiring detailed knowledge of each other's internal implementations. His dissertation introduced REST as an architectural style that embraced the web's fundamental design - particularly its use of hyperlinks as resource identifiers, standard methods (such as GET, POST, PUT, DELETE, and so on), and stateless communication. What made REST revolutionary was that it showed how to build distributed systems that could evolve independently by following these principles, allowing clients and servers to evolve and change without breaking each other. This was in stark contrast to earlier approaches that required lockstep changes between clients and servers.</p> <p>A key characteristic of REST is that it's stateless\u2014each request from client to server must contain all the information needed to understand and process the request. The server shouldn't need to remember anything about previous requests.</p> <p>HTTP APIs that embrace this architectural style are often described as being RESTful.</p>"},{"location":"threads/system/apis/5-api-spec/#resources-and-their-role-in-restful-apis","title":"Resources and Their Role in RESTful APIs","text":"<p>In RESTful APIs, the nouns in your API are resources. Each resource is uniquely identified by a URL (Uniform Resource Locator), meaning every resource has its own address.</p> <p>You may know these as web addresses, like:</p> <ul> <li>https://csxl.unc.edu/api/organizations \u2192 Returns a list of CS student organizations in JSON.</li> <li>https://csxl.unc.edu/api/academics/section/term/26S \u2192 Returns a list of CS classes offered in Spring 2026.</li> </ul> <p>Could you easily see the structure of the JSON in the URLs above?</p> <p>Most web browsers don't have nice, automatic formatting of JSON data. If you open the links above and it's not easy to see the structure of the data, we strongly recommend installing a web browser plugin to format JSON data. They are super handy when designing and building your own APIs!</p> <p>In Google Chrome, for example, we recommend JSON Formatter. Other browsers have similar tools\u2014find one with good reviews and many installations.</p>"},{"location":"threads/system/apis/5-api-spec/#request-input-specifications","title":"Request Input Specifications","text":""},{"location":"threads/system/apis/5-api-spec/#methods-defining-actions","title":"Methods: Defining Actions","text":"<p>The method of an HTTP request tells the server what action is being performed on the resource. You previously read about the fundamental API methods in the last section, including GET, POST, PUT, PATCH, and DELETE. While we won't go into detail here, it's important to recognize that the method is an essential input to an HTTP request and characterizes the request's intent to read, write, delete, or update.</p>"},{"location":"threads/system/apis/5-api-spec/#paths-identifying-and-routing-requests","title":"Paths: Identifying and Routing Requests","text":"<p>The path of a URL is technically just an opaque string of characters. You could design APIs with any scheme you'd like, but most REST API designers today model it as a logical hierarchy, much like folders in a file system:</p> <ul> <li><code>/api/users</code> \u2192 Might represent a collection of users.</li> <li><code>/api/users/42</code> \u2192 Might represent a specific user with ID 42.</li> <li><code>/api/users/42/orders</code> \u2192 Might represent orders placed by the user with ID 42.</li> </ul> <p>A path tells you which resource you want to work with in an API. For example, <code>/api/users/42</code> points to a specific user.</p> <p>Paths often contain both static and dynamic parts. In the example above:</p> <ul> <li><code>/api/users/</code> is static - it's always written exactly this way</li> <li><code>42</code> is dynamic - it changes based on which user you want</li> </ul> <p>When you build an API, your framework (e.g. FastAPI in COMP423) handles routing - matching each incoming request to the right piece of code. The framework looks at two key inputs:</p> <ol> <li>The path (what resource you want)</li> <li>The HTTP method (what you want to do with it)</li> </ol> <p>For instance, sending <code>GET</code> vs <code>DELETE</code> to <code>/api/users/42</code> will trigger different actions, even though the path is the same. GET might retrieve the user's information, while <code>DELETE</code> might remove their account.</p> <p>Using a common delimiter character, like the <code>/</code> character in a path, communicates subparts of a path to both humans and the system. This convention allows us to visually read the parts of a path and it also allows for programs to parse the path in a straightforward manner by breaking the string up by its <code>/</code> characters.</p>"},{"location":"threads/system/apis/5-api-spec/#query-parameters-refining-requests","title":"Query Parameters: Refining Requests","text":"<p>HTTP API designers aim to use query parameters when you need to customize what data is returned back from an API resource. They are added to the end of a URL after the path and tell the server how to filter, sort, or modify the data before sending it back.</p> <p>Here's how they work in a URL:</p> <pre><code>www.bookstore.com/books?genre=fantasy&amp;sort=newest\n</code></pre> <p>The path of this request is <code>/books</code>, a resource that lists all books being sold by a hypothetical book store.</p> <p>The query parameter begans at the <code>?</code> following the path:</p> <ul> <li>Each parameter has a name and value joined by <code>=</code> </li> <li>Multiple parameters are connected with <code>&amp;</code></li> </ul> <p>Like parameter passing to a function, query parameters allow the same resource to inform its behavior and response. Common uses include:</p> <ul> <li>Filtering: <code>?genre=fantasy</code> (only return fantasy books)</li> <li>Sorting: <code>?sort=newest</code> (arrange by newest first) </li> <li>Pagination: <code>?page=2&amp;limit=10</code> (get 10 items from page 2)</li> <li>Searching: <code>?search=dragon</code> (find items containing \"dragon\")</li> </ul> <p>The server reads these parameters and adjusts what data it sends back based on them.</p> <p>APIs that use query parameters will often have default values for each parameter so that requests without them will still succeed. For example, when loading the first page of a product listing API, your request may not need the <code>page</code> parameter, but on pages <code>2</code> and <code>3</code> beyond, it does.</p> <p>Beyond APIs, query parameters are a fundamental part of the web. A common example is a Google search URL:</p> <ul> <li><code>https://www.google.com/search?q=REST%20API</code> \u2192 Searches Google for 'REST API'.\u00a0Here the <code>q</code> is the name of the query parameter, Google's short-hand for query, and <code>REST API</code> is the value.</li> </ul> <p>You, as the designer and implementer of an API, will decide whether any of your routes utilize query parameters and be responsible for modifying responses based on their values.</p>"},{"location":"threads/system/apis/5-api-spec/#path-and-query-parameter-design-conventions","title":"Path and Query Parameter Design Conventions","text":"<p>Over the past 25 years, API designers have found many conventions in deciding how to name parts of a path, and how to use path and query parameters together.</p> <p>\ud83c\udfaf Target nouns, not verbs - resources are things, not actions: <pre><code>\u2705 /articles\n\u274c /getArticles\n</code></pre></p> <p>\ud83c\udfaa Use plural nouns for collections: <pre><code>\u2705 /users\n\u274c /user-list\n\u2705 /users/123\n\u274c /user/123\n</code></pre></p> <p>\ud83c\udfad Pick a form and stick to it - be consistent with plurals: <pre><code>\u2705 /posts/123/comments/456\n\u274c /posts/123/comment/456\n</code></pre></p> <p>\ud83c\udf32 Don't nest too deeply: <pre><code>\u2705 /articles/123/comments\n\u274c /articles/123/comments/456/replies/789/likes\n</code></pre></p> <p>\ud83d\udc2a Use kebab-case or lowercase: <pre><code>\u2705 /blog-posts or /blogposts\n\u274c /blogPosts or /blog_posts\n</code></pre></p> <p>\ud83d\udd0d Filtering goes in queries - use parameters for modifiers: <pre><code>\u2705 /articles?status=published&amp;sort=date\n\u274c /published-articles-sorted-by-date\n</code></pre></p> <p>\ud83c\udfaf IDs belong in paths, not query parameters - directly access resources: <pre><code>\u2705 /users/123\n\u274c /users?id=123\n</code></pre></p> <p>\ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66 Family trees make sense - use nesting for real hierarchies: <pre><code>\u2705 /teams/123/members\n\u274c /members?teamId=123\n</code></pre></p> <p>As you get deeper into API design and implementation there are a few other best practice, as well, but if you merely followed these for the purposes of this course you will be doing great.</p>"},{"location":"threads/system/apis/5-api-spec/#bodies-sending-data-in-requests","title":"Bodies: Sending Data in Requests","text":"<p>For POST (creating) and PUT (updating) method requests, we need to send more complex data to the server. This data is sent in the body of the HTTP request, often in JSON format:</p> <pre><code>{\n  \"name\": \"Alice\",\n  \"email\": \"alice@example.com\"\n}\n</code></pre> <p>When designing the specification for an API, we will need to clearly define a schema for bodies. A schema defines exactly what \"shape\" of data our API expects in the request body, such as field names, data types, and structure. In the example above, the structure is a single JSON Object (denoted by the surrounding curly braces), the field names are <code>name</code> and <code>email</code>, and both fields expect <code>string</code> values assigned to them.</p> <p>Unlike paths or query parameters, bodies are not visible in the URL and can contain more complex structures like lists and nested objects. As such, sending <code>POST</code>/<code>PUT</code> requests to APIs is more involved than <code>GET</code> requests. Either you have to use a tool, which we'll look at an example of very soon, or you have to write some API client code.</p>"},{"location":"threads/system/apis/5-api-spec/#headers-metadata-for-requests","title":"Headers: Metadata for Requests","text":"<p>HTTP headers carry additional standard metadata about the request. The names of the example headers shown below are all defined by the HTTP standard. The API designer's use of headers is primarily choosing which standard headers to utilize, if any. </p> <ul> <li>Authentication is used to prove the request is made by an authorized user (e.g. Bearer Tokens for security):</li> </ul> <pre><code>Authorization: Bearer abc123xyz\n</code></pre> <ul> <li>Content-Type tells the server the data format in the request body (e.g., specifying JSON data format):</li> </ul> <pre><code>Content-Type: application/json\n</code></pre> <ul> <li>Accept indicates the client expects a certain data type in the response body (e.g., requesting a response in a specific format):</li> </ul> <pre><code>Accept: application/json\n</code></pre> <p>In the initial APIs you design, you will not be overly concerned with headers. Why? We will start with building some public APIs that do not attempt to authenticate a user. Additionally, all of our APIs will standardize around the <code>application/json</code> request body and response format. It's become pretty standard and is nice to work with. If you spend enough time implementing APIs, you may find it interesting to learn more about content negotiation which allows the client to request the same resource be responded in a specific data type (such as <code>XML</code> or <code>HTML</code> rather than <code>JSON</code>).</p> <p>Headers are also commonly used to help out with content caching, where clients give servers a hash of their cached content and servers can response \"nothing's changed, reuse your cached copy and we're not sending anything else back.\" Additionally, headers are used for some security features as well, like preventing an old phishing attack where an unsuspecting person would be presented with what looked like a benign form, press submit on the form, and actually have the user submitting a POST request sent to their bank's web site to transfer money from their bank account or make a purchase on an web store. That is referred to as \"cross-site request forgery\" or CSRF. Headers can play a role in protecting APIs against it.</p> <p>In some avante garde APIs, designers may introduce their own custom headers, which you can spot as different from standard headers using an <code>X-</code> prefix, such as <code>X-API-KEY</code>, but this is rare and certainly not something you should consider when designing early APIs. If you were ever to think, \"hmm should I design a custom header for this?\" the answer is probably no and the extra information should be bundled in a request either as a query parameter or additional field in the request body.</p>"},{"location":"threads/system/apis/5-api-spec/#response-output-specifications","title":"Response Output Specifications","text":"<p>After a client makes a request to a server, the client needs to know what kind of response(s) to expect back in order to properly handle the response. What is the happy path? What is expected when something goes wrong? Typically response specifications are pairings of response code and response body schema. For example, if a client provides a well formed request to search for a user in a directory, it can expect to receive back a <code>200 OK</code> response code and a JSON response body with a user schema. If a client fails to provide a necessary query parameter, perhaps a <code>400 Bad Request</code> response code is sent with a JSON response body that includes an error schema with different fields than a user schema indicating what went wrong.</p>"},{"location":"threads/system/apis/5-api-spec/#response-status-code","title":"Response Status Code","text":"<p>These were covered in more detail in the previos reading. Typically your job as an API designer is to specify which codes a resource routes (method + path) may respond with. </p> <p>Success Responses (200-299):</p> <ul> <li>You must specify what data will be returned (the response schema)</li> <li>Example: A 200 response returning user data needs a schema defining the user object structure</li> </ul> <p>Redirection Responses (300-399):</p> <ul> <li>No schema needed</li> <li>The next location is sent in the response headers instead</li> <li>Example: A 301 response includes a header pointing to the new URL</li> </ul> <p>Client Error Responses (400-499):</p> <ul> <li>Need a schema for error information</li> <li>Many API frameworks provide default error schemas</li> <li>Example: A 400 response needs a schema showing how error messages are formatted</li> </ul> <p>Server Error Responses (500-599):</p> <ul> <li>These represent unexpected failures</li> <li>Examples: Server crashes, out-of-memory errors, database connection failures</li> <li>Generally don't need custom schemas since these are unplanned errors</li> <li>Your framework typically handles how these errors are formatted</li> </ul> <p>As an API designer, you'll mainly focus on defining schemas for success responses (200s) and client errors (400s), since these are the planned, normal operations of your API.</p>"},{"location":"threads/system/apis/5-api-spec/#response-body-schemas","title":"Response Body Schemas","text":"<p>Just like with request body schemas discussed above, an API Designer is tasked with specifying the shape of data clients can expect in response to their request. This is very analagous to specifying the return type of a function or method when programming. In most modern API frameworks, including the one you will learn in this course (FastAPI) the way we specify request and response body schemas is exactly the same and will be very comfortable to you. Why? Because they'll just be class definitions!</p>"},{"location":"threads/system/apis/5-api-spec/#response-headers","title":"Response Headers","text":"<p>Like request headers, your initial API design work will not be overly concerned with response headers. Why? We'll always respond with a content-type of JSON. Additionally, many advanced response headers (cache control and compression) are handled by middleware and configuration, not explicitly our concern when designing the structure of an API. It's unlikely you'll need 300-level response codes in your first APIs, but these would be a reason you would care about headers: when you redirect a client because a resource URL has moved that information is sent back via a <code>Location</code> header. When the time comes, trust a quick documentation search or LLM prompt will fill you in.</p>"},{"location":"threads/system/apis/5-api-spec/#kris-can-we-please-just-code-up-an-api","title":"Kris, can we please just code up an API?","text":"<p>Yes! I promise if I believed starting with writing API code first, before these concepts were introduced and vaguely familiar, that it is easy to fail to get lost in the details. Modern API frameworks, like FastAPI, go to great lengths to make the developer experience as convenient and terse as possible. Some of these inputs (like the difference between a dynamic path part and a query parameter) can be hard to spot the differences of, or know why to use one over the other, when writing your first API implementation code. Keep these inputs and specification concerns in mind as we dive in next!</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/","title":"6. FastAPI and Pydantic Tutorial","text":"<p>FastAPI is a modern, fast (high-performance), standards-first web framework for Python. It's designed around modern Python features such as type annotations (like you used in COMP110). FastAPI helps you both specify and build RESTful HTTP APIs quickly.</p> <p>Pydantic is a library used by FastAPI for data modeling and validation. It is how we will specify the schemas for request and response body data. It enforces type hints at runtime and yields user-friendly errors.</p> <p>Since you are now comfortable with HTTP methods, paths, query parameters, and so on, from the previous parts of this reading, you're in great shape to dive in!</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#1-getting-started","title":"1. Getting Started","text":"<p>In a terminal on your host machine, outside of any other <code>git</code> repositories, follow the following steps:</p> <ol> <li> <p>Clone the tutorial repository: Start by cloning the repository at https://github.com/comp423-26s/fastapi-tutorial.git.</p> </li> <li> <p>Open the repository in a VS Code Dev Container. The dev container is based on a modern Microsoft Dev Container image, which we have already used once in this course, so it should load quickly and install the necessary dependencies from <code>requirements.txt</code>. This repo uses an older style package manager (<code>pip</code>) than what we have emphasized this semester (<code>uv</code>), but both tools and setups are valid and common \"in the wild.\"</p> </li> <li> <p>Open main.py. This is the entrypoint of our API app and where the tutorial starts!</p> </li> </ol>"},{"location":"threads/system/apis/6-fast-api-tutorial/#2-first-route-hello-world","title":"2. First Route: Hello World","text":"<p>There's only one way to venture into new territory in programming: hello, world! Let\u2019s start with the simplest possible route. Update your <code>main.py</code>:</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root() -&gt; str:\n    return \"Hello, world!\"\n</code></pre>"},{"location":"threads/system/apis/6-fast-api-tutorial/#what-does-appget-mean","title":"What Does <code>@app.get(\"/\")</code> Mean?","text":"<ul> <li>Decorator: If you\u2019re new to decorators, think of <code>@something</code> as a way to wrap or register the function that follows. In this case:<ul> <li><code>@app.get(\"/\")</code> tells FastAPI that this function (<code>read_root</code>) handles GET requests to the root path (<code>\"/\"</code>).</li> <li>The function name <code>read_root</code> is arbitrary\u2014choose a meaningful name for your own clarity.</li> </ul> </li> <li>When you return a string (like <code>\"Hello, world!\"</code>), FastAPI automatically converts it into an HTTP response with the body containing that string.</li> </ul>"},{"location":"threads/system/apis/6-fast-api-tutorial/#3-running-the-development-server","title":"3. Running the Development Server","text":"<p>To run your app in development, use the following command (from within the <code>fastapi-tutorial</code> folder):</p> <pre><code>fastapi run main.py --reload\n</code></pre> <p>By default, FastAPI\u2019s dev server:</p> <ul> <li>Runs at <code>http://127.0.0.1:8000</code> (port 8000). Note: If you have any other dev servers running on this same port (e.g. your MkDocs project's dev server) see the Ports tab in VSCode to learn what port this container's 8000 was mapped to on your host machine.</li> <li>The <code>--reload</code> argument causes the server to watch your files. If you make changes, it auto-reloads so you don\u2019t have to stop and restart the server on every change you make to your code.</li> </ul> <p>Behind the scenes, FastAPI is using a Python package called Uvicorn to handle lower-level HTTP concerns. This is beyond your concern, but if you see anything about <code>uvicorn</code> when reading about FastAPI just know it's a foundational HTTP layer that FastAPI sits above in the architecture. </p> <p>Whenever a request hits <code>GET /</code>, it calls our <code>read_root()</code> function.</p> <p>Take a look at your Python code and be sure you can identify where the following HTTP API dimensions are specified: the HTTP method (1), the path (2), and the response body schema (3). Click the annotation icon, the plus symbol, to expand the answers.</p> <ol> <li> <p>The HTTP method is specified in the <code>@app.get</code> annotation (<code>GET</code>). If it makes it easier to remember, HTTP method specification in FastAPI is implemented as a method call on the FastAPI <code>app</code> object.</p> </li> <li> <p>The path is <code>/</code>, commonly called a root path since it has no parts beyond the slash, and it is specifed as the first parameter of the <code>@app.get()</code> method call.</p> </li> <li> <p>The response body schema is specified as the return type of the route handler function. In this case it is <code>str</code> as the returned value is <code>\"Hello, world!\"</code>.</p> </li> </ol>"},{"location":"threads/system/apis/6-fast-api-tutorial/#4-adding-another-static-route","title":"4. Adding Another Static Route","text":"<p>Let\u2019s add a second route, say, <code>GET /about</code> which returns some simple text. Update your <code>main.py</code>:</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root() -&gt; str:\n    return \"Hello, world!\"\n\n@app.get(\"/about\")\ndef read_about() -&gt; str:\n    return \"This is a simple HTTP API.\"\n</code></pre> <p>Try visiting <code>http://localhost:8000/about</code>. You should see the alternate message!</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#5-introducing-a-pydantic-model-and-listing-posts","title":"5. Introducing a Pydantic Model and Listing Posts","text":"<p>Next, let\u2019s introduce a Pydantic model to represent our data. These models serve a dual purpose: first they give us a Python class we can use throughout our server-side code. Second, in conjunction with FastAPI, they will automatically create a schema for our API specifications.</p> <p>We\u2019ll use a simple \"Post\" resource as an example throughout this tutorial. Let's start by returning a list of posts from a global dictionary that we\u2019ll pre-populate with a couple sample posts.</p> <ol> <li>Define the <code>Post</code> model as a subclass of <code>pydantic.BaseModel</code>. Be sure to add the <code>import</code> statement for <code>BaseModel</code>. Define it to have two attributes: <code>id</code> and <code>content</code>.</li> <li>Create a global dictionary <code>posts_db</code> containing two posts keyed by their IDs.</li> <li>Add a route <code>GET /posts</code> to list all posts.</li> </ol> <p>Update <code>main.py</code> with the following:</p> <pre><code>from fastapi import FastAPI\nfrom pydantic import BaseModel, Field\nfrom typing import Annotated\n\napp = FastAPI()\n\nclass Post(BaseModel):\n    id: Annotated[int, Field()]\n    content: Annotated[str, Field()]\n\n# Prepopulate dictionary of posts\nposts_db = {\n    1: Post(id=1, content=\"Hello FastAPI!\"),\n    2: Post(id=2, content=\"Writing my second post!\")\n}\n\n@app.get(\"/\")\ndef read_root() -&gt; str:\n    return \"Hello, world!\"\n\n@app.get(\"/about\")\ndef read_about() -&gt; str:\n    return \"This is a simple HTTP API.\"\n\n@app.get(\"/posts\")\ndef list_posts() -&gt; list[Post]:\n    return list(posts_db.values())\n</code></pre>"},{"location":"threads/system/apis/6-fast-api-tutorial/#how-this-works","title":"How This Works","text":"<ul> <li>We store two example posts in a global dictionary, <code>posts_db</code>, keyed by their ID.</li> <li>The route <code>GET /posts</code> returns <code>list(posts_db.values())</code>, which effectively returns all posts as a list.</li> <li>Notice how each value in <code>posts_db</code> is already an instance of <code>Post</code>. When FastAPI sees these objects, it converts them to JSON automatically.</li> </ul> <p>Notice the return type of the <code>list_posts</code> function is a <code>list</code> of <code>Post</code> objects. This is specifying the response body schema. Try visiting this route in your browser to confirm it is working. If you do not see well formatted JSON that is easy to read, try going back to the previous part of this reading and installing a JSON Viewer plugin in your web browser.</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#6-adding-a-dynamic-route-to-get-a-single-post","title":"6. Adding a Dynamic Route to Get a Single Post","text":"<p>Now let\u2019s introduce our first dynamic route. For a URL like <code>\"/posts/1\"</code>, we want to look up the post with <code>id=1</code> in our dictionary and return the <code>Post</code> object with this ID. </p> <p>Add the following import and route definition to your <code>main.py</code> file:</p> <pre><code># ... Update FastAPI Imports ...\nfrom fastapi import FastAPI, HTTPException, Path\nfrom typing import Annotated\n\n# ... Earlier App Stays Same ... \n\n@app.get(\"/posts/{post_id}\")\ndef get_post(post_id: Annotated[int, Path(title=\"ID of Post\")]) -&gt; Post:\n    if post_id in posts_db:\n        return posts_db[post_id]\n    raise HTTPException(status_code=404, detail=\"Post not found\")\n</code></pre>"},{"location":"threads/system/apis/6-fast-api-tutorial/#try-a-happy-path","title":"Try a Happy Path","text":"<p>Try navigating to <code>/posts/1</code> and <code>/posts/2</code> and convince yourself you can trace the flow of information. Specifically, look at how the path is specified with a dynamic part named <code>post_id</code> and how that path part corresponds to the function parameter of the same name. The value is then used to lookup a post with a given ID in the dictionary.</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#try-an-unhappy-path","title":"Try an Unhappy Path","text":"<p>Try navigating to <code>/posts/3</code> and seeing the 404 Response. Your browser won't show you the response code directly, but you can open up your browser's Developer Tools and look at your Network history (try reloading) to see the 404 is being sent. Notice this is achieved programatically in FastAPI by raising an <code>HTTPException</code> with a <code>status_code</code> keyword parameter.</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#try-an-invalid-path","title":"Try an Invalid Path","text":"<p>Finally, navigate to <code>/posts/abc</code>. Because we declared <code>post_id: int</code>, FastAPI automatically checks if <code>\"abc\"</code> can be converted to an integer. It cannot, so the framework responds with an HTTP 422 Unprocessable Entity error, including a helpful error message about the invalid type. This automatic validation is one of the many reasons FastAPI is a joy to work with compared to its predecessors! Edge case handling like this used to require more boilerplate code from engineers.</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#7-understanding-routing-in-modern-api-frameworks","title":"7. Understanding Routing in Modern API Frameworks","text":"<p>Now that you\u2019ve seen both a static route (<code>\"/posts\"</code>) and a dynamic route (<code>\"/posts/{post_id}\"</code>), let\u2019s briefly discuss how routing works in a modern framework like FastAPI. At a high-level, the routing algorithm works like this:</p> <ol> <li>Match the HTTP method (GET, POST, PUT, DELETE, etc.).</li> <li>Match the path pattern (<code>\"/\"</code>, <code>\"/about\"</code>, <code>\"/posts\"</code>, <code>\"/posts/{post_id}\"</code>, etc.).<ul> <li>Routes are checked in the order they are defined which can be surprising. If you define a route like <code>/posts/{post_id}</code> and then a route like <code>/posts/stats</code> follows it, the first route will always be matched (and error). To avoid this common issue, specify routes with static path parts before the dynamic path parts.</li> </ul> </li> <li>Handle parameters (like <code>post_id</code>) including type conversion and validation.</li> <li>Call the function associated with that route.</li> <li>Return a response which might be JSON, HTML, or something else.</li> </ol> <p>Like everything, there is a bit more more machinery behind the scenes, but understanding routing at this level of details is sufficient for now.</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#8-automatic-documentation-with-openapi","title":"8. Automatic Documentation with OpenAPI","text":"<p>One major benefit of FastAPI is its automatic generation of OpenAPI documentation. OpenAPI was previously known as Swagger, which was an objectively awful name, so this is a welcomed development in the community. By default, FastAPI sets up:</p> <ul> <li>An OpenAPI specification at <code>/openapi.json</code>.</li> <li>An OpenAPI-based web interface at <code>/docs</code>.</li> </ul> <p>With your FastAPI dev server is running, navivate to:</p> <ul> <li><code>/docs</code> \u2014 a graphical user interface where you can see all endpoints, query them, and see sample requests and responses.</li> <li><code>/openapi.json</code> \u2014 the raw JSON specification for your API.</li> </ul> <p>Because we used <code>pydantic.BaseModel</code> for <code>Post</code>, the schema's model shape will be visible in <code>/docs</code>, including field types and potential validation error states.</p> <p>Why is an OpenAPI spec valuable?</p> <ul> <li>It standardizes your API contract, so other developers or tools (like code generators) know exactly how to consume your endpoints.</li> <li>The <code>/docs</code> interface provides a quick way to try out your endpoints. This will be valuable in the next section.</li> </ul>"},{"location":"threads/system/apis/6-fast-api-tutorial/#openapi-ui-in-the-wild-csxluncedu","title":"OpenAPI UI in the Wild: CSXL.unc.edu","text":"<p>To hopefully drive home the point that what you are learning is both real and used in the wild, try opening up this URL in a new tab: https://csxl.unc.edu/docs.</p> <p>This is the API for the CSXL web application. Many routes require an authentication token. If you want to try those routes, in a separate tab open up the CSXL website, login, and go to your user profile. Under profile actions, click \"Copy\" on the Bearer Token (which is an authorization key for your user). Paste that in to the <code>/docs</code> unlock screen. Then try running the <code>GET /api/profile</code> API endpoint and you should see your data.</p> <p>Some other fun routes include public ones like listing student organizations or classes in a given semester.</p> <p>If you've used office hours via the CSXL, or applied to be a TA, or reserved a room or checked into the XL Coworking space for a desk to work at... you've already used this API without knowing it! If you scroll around you can see the API end points powering coworking, office hours, and more.</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#9-adding-a-post-route","title":"9. Adding a POST Route","text":"<p>Let\u2019s make our API a bit more dynamic by allowing clients to create new posts. We\u2019ll maintain our dictionary <code>posts_db</code> but now add a route for POST. We can do something like this:</p> <pre><code># Update FastAPI Imports\nfrom fastapi import FastAPI, HTTPException, status, Body\n\n# ... Keep other routes the same ...\n\n@app.post(\"/posts\", status_code=status.HTTP_201_CREATED)\ndef create_post(post: Annotated[Post, Body()]):\n    if post.id in posts_db:\n        raise HTTPException(status_code=400, detail=\"Post with this ID already exists\")\n    posts_db[post.id] = post\n    return post\n</code></pre>"},{"location":"threads/system/apis/6-fast-api-tutorial/#walkthrough-of-the-post-route","title":"Walkthrough of the POST Route","text":"<ul> <li>Request Body: FastAPI automatically parses the incoming JSON body into a <code>Post</code> object (thanks to Pydantic). Notice how simple this is! We specified a parameter to the function of a Pydantic model type, there is no conflicting name in a dynamic path part, so FastAPI convention infers this must be the schema of the data in the request body.</li> <li>We \"store\" that post in <code>posts_db</code> using the post\u2019s <code>id</code> as the key.</li> <li>By specifying <code>status_code=status.HTTP_201_CREATED</code>, FastAPI will return a 201 Created status code upon success.</li> <li>We also added a small check to ensure that an existing post with the same ID doesn\u2019t get overwritten.</li> </ul> <p>Open your browser to the API UI page <code>/docs</code>, or reload it (this page will not automatically refresh upon saving your work in the editor). Scroll to the <code>POST /posts</code> endpoint. You can:</p> <ol> <li>Click Try it out.</li> <li>Provide a sample JSON body, e.g.:    <pre><code>{\n  \"id\": 3,\n  \"content\": \"My brand new post!\"\n}\n</code></pre></li> <li>Click Execute and see the response information.</li> </ol> <p>You can then go to the <code>GET /posts/{post_id}</code> endpoint in <code>/docs</code> (or directly at <code>/posts/3</code>) to verify the newly created post.</p> <p>There are a few other activities for you to try here:</p> <ol> <li>Try posting the same JSON and seeing the response code.</li> <li>Try posting a JSON body that is just <code>{\"id\": 4}</code> and seeing the response FastAPI produces. (WOW!)</li> <li>Look at the specific response status code of the happy path (201) in the <code>/docs</code> UI. Notice where this is coming from in the definition. Take a look at how this is specified in the decorator as an additional parameter. There are other ways of responding with a specific status code, but this is preferred in a case like this.</li> </ol> <p>Your 'Database' of Posts Will Reset</p> <p>We are not actually using a \"database\" in this tutorial; just a dictionary stored in our module's global memory a sa simplification. As such, every time your FastAPI server stops and restarts, this dictionary is reset to its initialized contents. That means each time you change your <code>main.py</code> file below, and the server automatically reloads, you will lose any changes made via the API.</p> <p>Soon, we will learn how to connect our API to persistent databases that live in a layer outside of our code such that when we stop and restart our server the data is securely stored and accessible again as soon as our server starts back up.</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#10-adding-put-and-delete","title":"10. Adding PUT and DELETE","text":"<p>Finally, let\u2019s round out our basic CRUD functionality (Create, Retrieve, Update, Delete) with PUT (update) and DELETE HTTP method routes. Here\u2019s a simple approach, using our dictionary to check for existence by key:</p> <pre><code># ... previous code remains the same ...\n\n@app.put(\"/posts/{post_id}\")\ndef update_post(post_id: Annotated[int, Path()], updated_post: Annotated[Post, Body()]) -&gt; Post:\n    if post_id not in posts_db:\n        raise HTTPException(status_code=404, detail=\"Post not found\")\n    posts_db[post_id] = updated_post\n    return updated_post\n\n@app.delete(\"/posts/{post_id}\", status_code=status.HTTP_204_NO_CONTENT)\ndef delete_post(post_id: Annotated[int, Path()]) -&gt; None:\n    if post_id not in posts_db:\n        raise HTTPException(status_code=404, detail=\"Post not found\")\n    del posts_db[post_id]\n    return None # 204 = No Content\n</code></pre>"},{"location":"threads/system/apis/6-fast-api-tutorial/#put-update-a-resource","title":"PUT: Update a Resource","text":"<p>At the HTTP specification level, PUT is meant to replace the resource at the specified URL. Here, our resource is <code>\"/posts/{post_id}\"</code>. When the client requests <code>PUT /posts/5</code>, for example, we expect the request body to provide the new <code>id</code> and <code>content</code> fields for post <code>5</code> (or whichever post ID is specified). If that post doesn\u2019t exist, we respond with a 404 Not Found.</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#delete-remove-a-resource","title":"DELETE: Remove a Resource","text":"<p>Similarly, DELETE aligns directly with the idea of removing the resource at the URL. When a client requests <code>DELETE /posts/5</code>, we remove post <code>5</code> from our <code>posts_db</code>. A successful removal returns a 204 No Content, which communicates that the request succeeded, but there\u2019s no response body.</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#testing-put-and-delete-in-the-openapi-ui","title":"Testing PUT and DELETE in the OpenAPI UI","text":"<ol> <li>Open the documentation: Navigate to <code>http://localhost:8000/docs</code>. You\u2019ll see your new <code>PUT</code> and <code>DELETE</code> endpoints under the <code>/posts/{post_id}</code> section.</li> <li>Try PUT:<ul> <li>Expand PUT /posts/{post_id}.</li> <li>Click Try it out.</li> <li>Enter a valid <code>post_id</code> (e.g., <code>1</code>) in the path parameter box.</li> <li>Provide a JSON body with the <code>id</code> and <code>content</code> fields. For instance:     <pre><code>{\n\"id\": 1,\n\"content\": \"Updated content via PUT!\"\n}\n</code></pre></li> <li>Execute the request and verify that the response shows the updated post.</li> <li>Try using the <code>GET</code> routes (list or by ID) to confirm the update is reflected following the update.</li> </ul> </li> <li>Try DELETE:<ul> <li>Expand DELETE /posts/{post_id}.</li> <li>Click Try it out.</li> <li>Enter the <code>post_id</code> for the post you want to remove.</li> <li>Execute, and you\u2019ll see a 204 response indicating success (no body returned).</li> <li>If you try a <code>post_id</code> that doesn\u2019t exist, you\u2019ll get a 404 Not Found.</li> <li>Try using the <code>GET</code> routes (list or by ID) to confirm the <code>POST</code> as deleted from your in-memory \"database\".</li> </ul> </li> </ol> <p>With PUT and DELETE, you now have the full set of HTTP operations to manage a simple resource.</p>"},{"location":"threads/system/apis/6-fast-api-tutorial/#summary-and-next-steps","title":"Summary and Next Steps","text":"<p>Congratulations! You\u2019ve:</p> <ol> <li>Declared routes with static paths (<code>\"/\"</code>, <code>\"/about\"</code>).</li> <li>Introduced Pydantic models (<code>Post</code>) and used a global dictionary to store and retrieve posts.</li> <li>Created routes to list all posts, get a specific post by ID (dynamic route), and handled invalid IDs.</li> <li>Learned how FastAPI automatically validates path parameters.</li> <li>Explored how FastAPI auto-generates OpenAPI docs at <code>\"/docs\"</code>.</li> <li>Implemented POST, PUT, and DELETE to complete the CRUD operations set.</li> </ol>"},{"location":"threads/system/apis/6-fast-api-tutorial/#best-practices-beyond-this-tutorial","title":"Best Practices Beyond This Tutorial","text":"<ul> <li>Organize your files: Real projects separate routers, models, and database logic into different modules.</li> <li>Use databases: Instead of an in-memory dictionary, integrate a real database system for persistence.</li> <li>Validation and error handling: Explore more Pydantic features to ensure robust data validation. One place where we did not fully specify our APIs above, for the sake of not getting bogged down in error cases, is when we responded with error status codes but did not specify this in the route decorator. In the next assignment, we will fully specify all expected response types in our route handler functions.</li> </ul> <p>With these fundamentals, you have a solid handle on building a basic API with FastAPI. Enjoy experimenting, and happy coding!</p>"},{"location":"threads/system/backend/","title":"Backend Concerns","text":"Readings Topic RD17 Dependency Injection RD19 Unit, Integration, and E2E Testing API Routes Lessons Topic LS10 HTTP API Design LS11 Dependency Injection Lab LS12 API Design Working Day LS13              Lesson 13           LS14              Lesson 14"},{"location":"threads/system/backend/1-dependency-injection/","title":"1. Introduction to Dependency Injection in FastAPI","text":""},{"location":"threads/system/backend/1-dependency-injection/#what-is-dependency-injection","title":"What is Dependency Injection?","text":"<p>Dependency Injection (DI) is a widely used design pattern that promotes modular, testable, and maintainable code. It is a core principle in many modern application frameworks across various programming languages, including Java (Spring), Python (FastAPI), and TypeScript (Angular). The primary idea behind DI is instead of you constructing dependencies inside a function or class body, you declare them as special parameters. When the application framework calls your function, such as a route, its DI system constructs the argument values behind the scenes and \"injects\" them as arguments. This process is called dependency injection.</p> <p>This concept plays a role in modern layered architectures like as we are exploring in this course. Dependency Injection provides a clean, structured way to introduce and manage dependencies between layers, keeping them loosely coupled and testable. We will introduce a business logic services layer that encapsulates domain-specific logic and separates it from the routing layer (which handles HTTP requests and responses). The services layer is dependency injected into the routing layer.</p>"},{"location":"threads/system/backend/1-dependency-injection/#why-use-dependency-injection","title":"Why Use Dependency Injection?","text":"<p>Dependency Injection helps solve common software design challenges, making applications:</p> <ul> <li>More Maintainable: By loosely coupling dependencies between parts of a system, changes in one part of the application don\u2019t require modifying other parts. Loosely coupled components make it easier to replace or upgrade individual parts of a system without affecting the rest. This reduces the risk of unintended side effects and promotes a more modular and extensible architecture.</li> <li>Easier to Test: With DI, dependencies can be replaced with mock implementations, making unit tests isolated and reliable.</li> <li>More Flexible: By programming to an interface rather than a concrete implementation, different implementations of a dependency can be injected dynamically, allowing for easy configuration changes.</li> <li>Reduces Code Duplication: Centralizing dependency management prevents repeated instantiation of services throughout the codebase.</li> </ul> <p>You\u2019ve actually already encountered DI in FastAPI! Every time a request includes path parameters, query parameters, or request bodies... where did the argument values come from? FastAPI injected them into your route handlers automatically! Now, let\u2019s take this one step further: what if we wanted to inject a custom service into our application to handle business logic?</p>"},{"location":"threads/system/backend/1-dependency-injection/#how-does-dependency-injection-work-in-fastapi-routing","title":"How Does Dependency Injection Work in FastAPI Routing?","text":"<p>When a request is received in a FastAPI application, the following steps occur:</p> <ol> <li>Request Routing: FastAPI matches the incoming request's URL and HTTP method to the appropriate route handler function.</li> <li>Dependency Resolution: Before calling the route function, FastAPI checks for any declared dependencies using <code>Depends()</code>. It determines what dependencies are needed and resolves how to instantiate them in a correct order. An injected dependency may have its own injected dependencies that need to be resolved and instantiated first!</li> <li>Dependency Instantiation: If a dependency is a class or function, FastAPI instantiates it (if needed) and injects it into the route function.</li> <li>Function Execution: The route function is called with the injected dependencies passed in as arguments to the routed function's parameters.</li> </ol>"},{"location":"threads/system/backend/1-dependency-injection/#tutorial-dependency-injection-in-fastapi","title":"Tutorial: Dependency Injection in FastAPI","text":"<p>To follow along with this quick tutorial on dependency injection, from your host machine's terminal clone the course FastAPI Tutorial repository again, but name the cloned directory <code>di-tutorial</code>:</p> <pre><code>git clone https://github.com/comp423-26s/fastapi-tutorial di-tutorial\n</code></pre> <p>The last argument of <code>di-tutorial</code> is what causes <code>git</code> to clone to a specific directory name on your machine.</p> <p>GitHub's <code>gh</code> CLI Program</p> <p>Now that you are comfortable with fundamental <code>git</code> commands, you may want to install GitHub's <code>gh</code> tool on your host machine. Instructions here: https://cli.github.com</p> <p>The <code>gh</code> tool allows you to interact with GitHub's REST API from your command line. You can do things like create new GitHub repositories, list issues, and nearly anything you can do from the GitHub web page.</p> <p>Once you have <code>gh</code>, you can achieve the clone command above with:</p> <pre><code>gh repo clone comp423-26s/fastapi-tutorial di-tutorial\n</code></pre> <p>Open the repo directory in VS Code and then open the repo in a Dev Container.</p>"},{"location":"threads/system/backend/1-dependency-injection/#step-1-defining-models","title":"Step 1: Defining Models","text":"<p>Let's implement a Rock, Paper, Scissors API! Create a new file in the project's root directory named <code>models.py</code>. We'll define our Pydantic data models here. Review the code below and then copy it into <code>models.py</code>:</p> models.py<pre><code>from enum import Enum\nfrom datetime import datetime\nfrom typing import Annotated, TypeAlias\nfrom pydantic import BaseModel, Field\n\n\nclass Choice(str, Enum):\n    rock = \"rock\"\n    paper = \"paper\"\n    scissors = \"scissors\"\n\n\nChoiceField: TypeAlias = Annotated[\n    Choice,\n    Field(\n        description=\"Choice of rock, paper, or scissors.\",\n        examples=[\"rock\", \"paper\", \"scissors\"],\n    ),\n]\n\n\nclass GamePlay(BaseModel):\n    user_choice: ChoiceField\n\n\nclass GameResult(BaseModel):\n    timestamp: Annotated[datetime, Field(description=\"When the game was played.\")]\n    user_choice: ChoiceField\n    api_choice: ChoiceField\n    user_wins: Annotated[bool, Field(description=\"Did the user win the game?\")]\n</code></pre> <p>Using a <code>TypeAlias</code> for repeated type annotations</p> <p>Notice <code>ChoiceField</code> is defined as a <code>TypeAlias</code> for the annotated type of <code>Choice</code> such that it contains the <code>Field</code> information with an API description.</p> <p>When you find an annotated type is repeated in multiple places in your Pydantic models or FastAPI routes, using a <code>TypeAlias</code> to cut down on the repetition and make the code more readable is a best practice.</p> <p>In Python, a protocol is similar to an interface in Java. It defines a contract that a class must follow without enforcing inheritance. This allows for better flexibility and testability.</p>"},{"location":"threads/system/backend/1-dependency-injection/#step-2-defining-a-game-service","title":"Step 2: Defining a <code>Game</code> Service","text":"<p>Now let's define a service class that handles the serious \"business logic\" of rock paper scissors. Review the contents below and copy the contents to a new file named <code>services.py</code>:</p> services.py<pre><code>from datetime import datetime\nfrom random import choice as random_choice\nfrom models import GamePlay, GameResult, Choice\n\n\nclass GameService:\n    \"\"\"Service for processing game plays.\n\n    This class provides functionality to simulate a game between a user and the API.\n    \"\"\"\n\n    def play(self, gameplay: GamePlay) -&gt; GameResult:\n        \"\"\"Play a game round.\n\n        Args:\n            gameplay (GamePlay): An object encapsulating the user's choice.\n\n        Returns:\n            GameResult: The outcome of the game including user and API choices, and win flag.\n        \"\"\"\n        api_choice: Choice = self._random_choice()\n\n        return GameResult(\n            timestamp=datetime.now(),\n            user_choice=gameplay.user_choice,\n            api_choice=api_choice,\n            user_wins=self._does_user_win(gameplay.user_choice, api_choice),\n        )\n\n    def _random_choice(self) -&gt; Choice:\n        \"\"\"Select a random choice for the API.\n\n        Returns:\n            Choice: A randomly chosen game option.\n        \"\"\"\n        return random_choice(list(Choice))\n\n    def _does_user_win(self, user_choice: Choice, api_choice: Choice) -&gt; bool:\n        \"\"\"Determine if the user wins based on choices.\n\n        Args:\n            user_choice (Choice): The user's chosen option.\n            api_choice (Choice): The API's chosen option.\n\n        Returns:\n            bool: True if the user wins, False otherwise.\n        \"\"\"\n        result: tuple[Choice, Choice] = (user_choice, api_choice)\n        winning_results: set[tuple[Choice, Choice]] = {\n            (Choice.rock, Choice.scissors),\n            (Choice.paper, Choice.rock),\n            (Choice.scissors, Choice.paper),\n        }\n        return result in winning_results\n</code></pre> <p>Notice that this <code>services.py</code> module knows nothing about HTTP or FastAPI. Its imports are data models and some library functionality for randomization. This is just plain-old Python! This is the \"core\" logic of our little app, though, and you can easily imagine how writing unit tests for it would be straightforward.</p>"},{"location":"threads/system/backend/1-dependency-injection/#step-3-establishing-a-fastapi-route-to-play-the-game","title":"Step 3: Establishing a FastAPI Route to Play the Game!","text":"<p>Now that we have a service defined, how do we add a route that uses dependency injection to utilize it? Similar to how we declare parameters of routes that are populated by dynamic <code>Path</code> parts, <code>Query</code> parameters, or <code>Body</code> payloads.</p> <p>What HTTP method would you choose for the game playing REST API endpoint?</p> <p>We will model playing a round of this game with a <code>POST</code> method. Even though we are not (yet) storing a history of games or creating anything, playing a game is not idempotent: we get back a new result each time we play. Not only do the <code>api_choice</code> and <code>user_wins</code> fields update, the <code>timestamp</code> reflects the latest game play.</p>"},{"location":"threads/system/backend/1-dependency-injection/#starting-without-dependency-injection","title":"Starting Without Dependency Injection","text":"<p>Update your <code>main.py</code> file to reflect the following. Note: this example does not yet rely upon dependency injection! We will refactor this to make use of dependency injection next.</p> main.py<pre><code>\"\"\"FastAPI main entrypoint file.\"\"\"\n\nfrom typing import Annotated, TypeAlias\nfrom fastapi import FastAPI, Body, Depends\nfrom models import GamePlay, GameResult\nfrom services import GameService\n\napp = FastAPI()\n\n\n@app.post(\"/play\")\ndef play(\n    user_choice: Annotated[\n        GamePlay,\n        Body(description=\"User's choice of rock, paper, or scissors.\"),\n    ],\n) -&gt; GameResult:\n    # Here we construct a GameService *without* dependency injection...\n    game_svc: GameService = GameService() # (1)!\n    return game_svc.play(user_choice)\n</code></pre> <ol> <li>Notice that fully inside the body of this function is where we declare a local variable of type <code>GameService</code> and construct it. If we later wanted to use a different object, which conformed to the interface <code>GameService</code> implements, how would we do so? We couldn't without changing this source code! Being able to swap out implementations is very useful in one common software engineering practice we will soon embrace: unit testing. In unit testing, to isolate the behavior of a single function, dependency injection gives you the ability to substitute fake dependencies in such that you are only testing the unit(s) of code you care about.</li> </ol> <p>Check for understanding: Why is line 19 problematic? Why do we want to use dependency injection instead?</p> <p>Try to answer this question for yourself before clicking the annotation symbol at the end of line 19 to reveal the answer.</p>"},{"location":"threads/system/backend/1-dependency-injection/#refactor-to-dependency-injection","title":"Refactor to Dependency Injection","text":"<p>The updated definition of the <code>play</code> function provides an example with FastAPI's dependency injection utilized:</p> main.py<pre><code>@app.post(\"/play\")\ndef play(\n    user_choice: Annotated[\n        GamePlay,\n        Body(description=\"User's choice of rock, paper, or scissors.\"),\n    ],\n    game_svc: Annotated[GameService, Depends()],\n) -&gt; GameResult:\n    return game_svc.play(user_choice)\n</code></pre> <p>Be sure to run the FastAPI server and try out the route from the OpenAPI <code>/docs</code> user interface!</p> <p>Notice on line 17 we added an additional parameter to the <code>play</code> function definition. Its type is <code>Annotated[GameService, Depends()]</code>. The <code>Depends()</code> call is what declaratively signals to FastAPI this is a dependency injected parameter. How does it know to construct an instance of <code>GameService</code>? Because it's annotating the type <code>GameService</code>. </p> <p>There are other ways of using <code>Depends</code>, too, like giving it a factory function and specifying the construction elsewhere. You can also specify the annotated type to be a <code>Protocol</code> (similar to a Java interface) and giving a concrete classname as an argument to <code>Depends</code>. That's how COMP301 should have taught you to approach a similar problem. However, we will adhere to a software engineering goal: don't overengineer until you have good reason to!</p> <p>This is dependency injection! There is a HUGE win here: your dependency is now a parameter passed in, or injected, from the outside. It is not hardwired in to the route body. Thus, if you wanted to unit test this function, you could easily supply a mock instance of a <code>GameService</code> and isolate the function's behavior. That said, this example is so trivial that the notion of isolating it for a unit test is a bit silly. </p>"},{"location":"threads/system/backend/1-dependency-injection/#step-4-adding-functionality","title":"Step 4. Adding Functionality","text":"<p>Let's record a history of games played since the service was last restarted. We will use global module memory for this, but realize this is only a stopgap solution until we learn more about data persistence.</p> services.py<pre><code># ... the import statements above remain the same ...\n\n# This is *NOT* a database, just a hack for now...\n_db: list[GameResult] = []\n\nclass GameService:\n    \"\"\"Service for processing game plays.\n\n    This class provides functionality to simulate a game between a user and the API.\n    \"\"\"\n\n    def play(self, gameplay: GamePlay) -&gt; GameResult:\n        \"\"\"Play a game round.\n\n        Args:\n            gameplay (GamePlay): An object encapsulating the user's choice.\n\n        Returns:\n            GameResult: The outcome of the game including user and API choices, and win flag.\n        \"\"\"\n        api_choice: Choice = self._random_choice()\n        result = GameResult(\n            timestamp=datetime.now(),\n            user_choice=gameplay.user_choice,\n            api_choice=api_choice,\n            user_wins=self._does_user_win(gameplay.user_choice, api_choice),\n        )\n        _db.append(result)\n        return result\n\n    def get_results(self) -&gt; list[GameResult]:\n        \"\"\"Get all game results.\n\n        Returns:\n            list[GameResult]: A list of all game results.\n        \"\"\"\n        return _db\n\n    # ... the \"private\" helper methods remain the same ...\n</code></pre> <p>Notice, we are using a simple global variable in the module to store results. Why not use an instance variable in the <code>GameService</code>? FastAPI's dependency injection system constructs a new instance of <code>GameService</code> on each request. With a little more effort we could get around this with something like the singleton design pattern, to ensure only one instance of <code>GameService</code> is shared across all requests, but that's beyond the scope of this tutorial.</p> <p>Let's add a route for listing the history of games played to <code>main.py</code>.</p> main.py<pre><code>@app.get(\"/results\")\ndef log(game_svc: Annotated[GameService, Depends()]) -&gt; list[GameResult]:\n    return game_svc.get_results()\n</code></pre> <p>After saving, your FastAPI server reloads so global memory is cleared. Try playing a few games and then trying out your <code>/results</code> route. You can access it both from the <code>/docs</code> UI as well as from the browser directly, since the route's method is <code>GET</code>.</p>"},{"location":"threads/system/backend/1-dependency-injection/#cleaning-up-the-types","title":"Cleaning Up the Types","text":"<p>There's one last minor tweak to make to help clean this up. You will find this useful as you write many routes which depend on the same service. Let's use a <code>TypeAlias</code> for our dependency injected <code>GameService</code> rather than repeat this annotated type everywhere. Try making the following changes in <code>main.py</code>:</p> main.py<pre><code>\"\"\"FastAPI main entrypoint file.\"\"\"\n\nfrom typing import Annotated, TypeAlias\nfrom fastapi import FastAPI, Body, Depends\nfrom models import GamePlay, GameResult\nfrom services import GameService\n\napp = FastAPI()\n\nGameServiceDI: TypeAlias = Annotated[GameService, Depends()]\n\n\n@app.post(\"/play\")\ndef play(\n    user_choice: Annotated[\n        GamePlay,\n        Body(description=\"User's choice of rock, paper, or scissors.\"),\n    ],\n    game_svc: GameServiceDI,\n) -&gt; GameResult:\n    return game_svc.play(user_choice)\n\n\n@app.get(\"/results\")\ndef log(game_svc: GameServiceDI) -&gt; list[GameResult]:\n    return game_svc.get_results()\n</code></pre> <p>Ah, that's not only a little easier on the eyes, but since the <code>TypeAlias</code> is defined in one place we could now customize <code>Depends()</code> and have more control over how the <code>GameService</code> dependency gets injected across all of these routes. If we wanted to move toward a singleton pattern, for example, we could do so here.</p>"},{"location":"threads/system/backend/1-dependency-injection/#the-power-of-dependency-injection-in-fastapi","title":"The Power of Dependency Injection in FastAPI","text":"<p>Congratulations on completing a first foray into dependency injection (DI) in FastAPI\u2014starting from its core principles and working through a hands-on example with a Rock, Paper, Scissors. You\u2019ve now seen how DI promotes modularity, testability, and maintainability in your applications. Instead of hardwiring dependencies, we leveraged FastAPI's <code>Depends()</code> function to keep our code clean and flexible.</p> <p>Through this tutorial, you've learned how to:</p> <ul> <li>Define business logic services that remain independent of the HTTP framework.</li> <li>Use FastAPI\u2019s DI system to inject dependencies in a structured way.</li> <li>Improve testability by allowing easy substitution of dependencies.</li> <li>Reduce code duplication and enhance maintainability.</li> </ul> <p>In time, you will learn some more advanced uses of DI in FastAPI:</p> <ul> <li>Services which inject other services into their constructors. This works just like you'd expect, but still feels magical! Since our services will ultimately depend on a database layer, we will inject the database dependencies into the service.</li> <li>A nicer way of declaring routes which have many query parameters using <code>Depends</code>. Read more here.</li> <li>Singleton dependencies which have only one instance shared across all requests.</li> </ul> <p>By adopting dependency injection, you're setting yourself up for scalable and maintainable application development. Whether you're working on a small personal project or a large-scale system, mastering DI ensures that your code remains clean and modular.</p>"},{"location":"threads/system/backend/2-testing-apis/","title":"Unit, Integration, and E2E Testing API Routes","text":"<p>This tutorial follows the previous tutorial on Dependency Injection in FastAPI. Go ahead and reopen that tutorial to pick up from there.</p> <p>This tutorial assumes you are versed in mocking and patching, which covered in the last unit of the course.</p> <p>As a reminder, this tutorial implemented a simple RESTful game of Rock, Paper, Scissors. Our goal in this reading is to unit test the <code>/play</code> route handler in isolation and integration test the routing layer (route registration and route handler) and service layers. Finally, we will look at end-to-end testing the backend without patching over side-effects.</p> <p>In good practice, go ahead and add your current changes to stage and form a commit as a checkpoint before moving on in this tutorial. Commit made? Let's get started!</p>"},{"location":"threads/system/backend/2-testing-apis/#unit-testing-a-route-handler","title":"Unit Testing a Route Handler","text":"<p>Practice Question</p> <p>Consider the route definition below and identify:</p> <ol> <li>What is the function name we will unit test?</li> <li>What arguments will our unit test provide as user input?</li> <li>What arguments are dependency injected that our unit test will need to mock?</li> <li>What will the unit test prove?</li> </ol> <p>Click on the + icons below to reveal hints </p> main.py<pre><code>@app.post(\"/play\")\ndef play( # (1)!\n    user_choice: Annotated[  # (2)!\n        GamePlay,\n        Body(description=\"User's choice of rock, paper, or scissors.\"),\n    ],\n    game_svc: GameServiceDI, # (3)!\n) -&gt; GameResult: # (4)!\n    return game_svc.play(user_choice) #(5)!\n</code></pre> <ol> <li><code>play</code> is the function name we are testing.</li> <li><code>user_choice</code> is a user input parameter we will need to provide.</li> <li><code>game_svc</code> is a dependency injected parameter we will mock.</li> <li><code>GameResult</code> is the type of object we expect returned back.</li> <li>The logic of this route handler is intentionally simple and merely delegates to <code>game_svc</code>. Our test needs to prove that we successfully delegated to the injected <code>GameService</code> and that its result was faithfully returned.</li> </ol>"},{"location":"threads/system/backend/2-testing-apis/#setting-up-a-unit-test","title":"Setting up a Unit Test","text":"<p>Since this tutorial is for a very simple demonstration, all of our modules are defined at the top-level. In larger, real projects you would use directory structure to organize related modules together. Go ahead and create a test module in the project's top-level directory, next to the others, named <code>test_play_unit.py</code>.</p> test_play_unit.py<pre><code>from unittest.mock import MagicMock\nfrom datetime import datetime\n\nfrom main import play\nfrom models import GamePlay, GameResult, Choice\nfrom services import GameService\n\ndef test_play_returns_game_result():\n    # Arrange\n    ...\n    # Act\n    ...\n    # Assert\n    ...\n</code></pre> <p>Go ahead and try implementing this test and running it to confirm your test passes. You should be able to do so without assistance by reading and reasoning through the imports, but if you need some hints, see the box below.</p> <p>Unit Testing Hints</p> <ol> <li>In the Arrange step, you'll need to setup a mock that specs <code>GameService</code> and a <code>GamePlay</code> instance. You will need to specify the <code>GameResult</code> return value of the method that you expect called on <code>GameService</code>.</li> <li>In the Act step, you'll call the <code>play</code> function and record its result.</li> <li>In the Assert step, you'll confirm the return value is equivalent to the <code>GameResult</code> instance you setup in the mock. You should also confirm that the service method was called once with the correct method(s).</li> </ol>"},{"location":"threads/system/backend/2-testing-apis/#integration-testing-a-route","title":"Integration Testing a Route","text":"<p>Notice in the unit test you implemented above there is no notion of the <code>post</code> method nor the <code>/play</code> route handling that occurs in the FastAPI layer of the backend. Additionally, by design, we mocked a <code>GameService</code> double and avoided testing the integrated behavior of both the route handler and the <code>GameService</code>.</p> <p>If you reread <code>GameService</code>'s implementation in <code>service.py</code>, you will remember that it uses randomization to decide the API's choice among rock, paper, and scissors using the <code>choice</code> function of Python's <code>random</code> library imported as <code>random_choice</code>. We will patch over this behavior to control the side-effect, but otherwise test the integration of the FastAPI route registration, route handler function, and service class. </p> <p>(Note for the careful reader: there is one other side-effect in <code>GameService</code>. Can you identify it? Since it is not integral to the business logic of the application, we will not worry about patching this side-effect in this tutorial. However, if you would like to practice patching, you can practice it here!)</p> <p>Let's get a new test file setup:</p> test_play_integration.py<pre><code>from unittest.mock import MagicMock, patch\n\nimport pytest\nfrom fastapi import status\nfrom fastapi.testclient import TestClient  # (1)!\n\nfrom main import app\nfrom models import Choice\n\n\ndef test_play_integration():\n    # Arrange\n    client = TestClient(app)  # (2)!\n\n    random_choice_mock = MagicMock()\n    random_choice_mock.return_value = \"paper\"\n\n    # Act\n    with patch(\"_____________\", random_choice_mock):  # (3)!\n        response = client.post(\"/play\", json={\"user_choice\": \"rock\"})  # (4)!\n\n    # Assert\n    random_choice_mock.assert_called_once()\n    assert response.status_code == status.HTTP_200_OK  # (5)!\n    response_body = response.json()\n    assert response_body[\"user_choice\"] == user_choice.user_choice\n    assert response_body[\"api_choice\"] == random_choice_mock.return_value\n    assert response_body[\"user_wins\"] == False\n</code></pre> <ol> <li>We are importing the <code>TestClient</code> utility class from FastAPI's library. This will help us test the actual routing layer of our API from an HTTP client's vantage point.</li> <li>When we construct a <code>TestClient</code> we pass in a reference to our FastAPI app where our routes are registered.</li> <li>What name are we patching over to isolate the randomization side-effect? This is your challenge in this particular code listing.</li> <li>Notice here we are making a <code>post</code> request via the <code>client</code> to the <code>\"/play\"</code> route with a \"JSON\" dictionary object as our body.</li> <li>Notice in our assertions, we are working with an actual HTTP response object. </li> </ol> <p>Your task: fill in the <code>patch</code>ed target name string</p> <p>Read through this integration test to understand its general flow. Click on the + icons for explanations of what is happening at new, important steps.</p> <p>Once you have a sense, replace the underscores in the patch string with the name you are patching as it is defined in the module you are applying the patch at.</p> <p>Hint: it is not <code>random.choice</code>. Your test will pass once you target the correct name to patch.</p> <p>Notice with this integration test we are testing all the way from FastAPI's route handling, to our route handler <code>play</code> function, to our <code>GameService</code>'s play method and helpers, only patching minimally over the random choice side-effect.</p>"},{"location":"threads/system/backend/2-testing-apis/#parametrizing-the-integration-test","title":"Parametrizing the Integration Test","text":"<p>Can you identify one unsatisfying quality of this test? What if you wanted to test other choices and outcomes? There would be a lot of redundancy! </p> <p>Let's take a look at using <code>pytest</code>'s parametrization fixture to specify different combinationd of inputs to the same test:</p> test_play_integration.py<pre><code>@pytest.mark.parametrize(\n    \"user_choice_input,api_choice_input,user_wins_expected\",\n    [\n        (Choice.rock, Choice.scissors, True),  # Rock beats scissors\n        (Choice.rock, Choice.paper, False),  # Rock loses to paper\n        (Choice.rock, Choice.rock, False),  # Rock ties rock (API wins)\n        (Choice.paper, Choice.rock, True),  # Paper beats rock\n        (Choice.paper, Choice.scissors, False),  # Paper loses to scissors\n        (Choice.paper, Choice.paper, False),  # Paper ties paper (API wins)\n        (Choice.scissors, Choice.paper, True),  # Scissors beats paper\n        (Choice.scissors, Choice.rock, False),  # Scissors loses to rock\n        (Choice.scissors, Choice.scissors, False),  # Scissors ties scissors (API wins)\n    ],\n)\ndef test_play_integration(user_choice_input, api_choice_input, user_wins_expected):\n    # Arrange\n    client = TestClient(app)\n\n    random_choice_mock = MagicMock()\n    random_choice_mock.return_value = api_choice_input\n\n    # Act\n    with patch(\"services.random_choice\", random_choice_mock):\n        response = client.post(\"/play\", json={\"user_choice\": user_choice_input})\n\n    # Assert\n    random_choice_mock.assert_called_once()\n    assert response.status_code == status.HTTP_200_OK\n    response_body = response.json()\n    assert response_body[\"user_choice\"] == user_choice_input\n    assert response_body[\"api_choice\"] == api_choice_input\n    assert response_body[\"user_wins\"] == user_wins_expected\n</code></pre> <p>On line 11, you are using <code>@pytest.mark.parametrize</code> decorator to configure the fixture parameters of the test function. On line 12, you will notice a funny string that feels a bit off but represents the name matching convention used in <code>parametrize</code>. Compare line 12 with line 25, where the parameters (fixtures!) of the test function are declared and notice there is a 1-to-1 correspondence. What you are doing in the string on line 12 is declaring names for each value that will pass into the test via a parameter of the same name. </p> <p>Then, on lines 13-23 you see a list of 9 tuples declared. Each tuple contains three values, corresponding 1-to-1 with the parameter names defined on line 12. Each of these tuples represents one test case worth of arguments that the test function will be called with. When you run this singular test, nine individual tests will be run individually, one per each of your parametrize case tuples.</p> <p>Inside of the test, rather than hard-coding a string value for the mocked return value of random choice, we substitute the parameter value in on line 30. The same strategy is used on lines 34 and 40-42.</p> <p>The big idea of this kind of integration testing is we are now proving to ourselves that our backend layers are successfully integrated with one another and are implementing the logical outcomes we expect.</p>"},{"location":"threads/system/backend/2-testing-apis/#end-to-end-testing-a-route","title":"End-to-End Testing a Route","text":"<p>For a final exercise, let's write a test that proves the full system is working without any patching of side-effects, at all. This is an end-to-end backend test.</p> test_play_e2e.py<pre><code>import pytest\nfrom fastapi import status\nfrom fastapi.testclient import TestClient\n\nfrom main import app\nfrom models import Choice\n\n\n@pytest.mark.parametrize(\n    \"user_choice_input\",\n    [(Choice.rock), (Choice.paper), (Choice.scissors)] * 100,  # (1)!\n)\ndef test_play_e2e(user_choice_input):\n    # Arrange\n    client = TestClient(app) # (2)!\n\n    # Act\n    response = client.post(\"/play\", json={\"user_choice\": user_choice_input}) # (3)!\n\n    # Assert\n    assert response.status_code == status.HTTP_200_OK\n    response_body = response.json()\n    assert response_body[\"user_choice\"] == user_choice_input\n    assert response_body[\"api_choice\"] in (Choice.rock, Choice.paper, Choice.scissors)\n\n    actual_outcome: tuple[Choice, Choice, bool] = (\n        response_body[\"user_choice\"],\n        response_body[\"api_choice\"],\n        response_body[\"user_wins\"],\n    )\n    expected_outcomes: list[tuple[Choice, Choice, bool]] = [\n        (Choice.rock, Choice.scissors, True),\n        (Choice.rock, Choice.paper, False),\n        (Choice.rock, Choice.rock, False),\n        (Choice.paper, Choice.rock, True),\n        (Choice.paper, Choice.scissors, False),\n        (Choice.paper, Choice.paper, False),\n        (Choice.scissors, Choice.paper, True),\n        (Choice.scissors, Choice.rock, False),\n        (Choice.scissors, Choice.scissors, False),\n    ]\n    assert actual_outcome in expected_outcomes\n</code></pre> <ol> <li>Python's <code>list</code> object has an operator overload for multiplication which produces a new list with the original list repeated <code>N</code> times. Therefore, this parametrization will repeat the sequence of rock, paper, scissors 100 times over for 300 tests.</li> <li>Notice in the arrange steps... we have no mocks or test doubles otherwise. This is a nice feature of end-to-end testing!</li> <li>Notice in the act step... we are not patching anything! This is also a nice feature of end-to-end testing. It is as real of a test of our backend system as we can write.</li> </ol> <p>Expand on the notes of the highlighted lines on lines 11, 15, and 18 for some key insights on this end-to-end test. Notice how straightforward the arrange and act steps are without any mocking or patching. This is one of the benefits of end-to-end API testing: the initial setup is as about as minimal as the API's setup. (One noteworthy caveat: If we used persistence, such as files or a database, or network dependencies, we would either need to do more setup work to be sure the starting state was predictable or relax some of our specificity in assertions.)</p> <p>When you run this end-to-end test you will notice that 300 tests are run!</p> <p>Why run more than three end-to-end tests in this scenario?</p> <p>Why run 300? Since there is entropy in our system via the API's random <code>choice</code> of rock, paper, or scissors, there are 9 possible outcomes. This is not a resource intensive test to run. Therefore, repeating inputs over many tries gives us increasing confidence all cases are likely hit at least once. This is a common strategy and relates to the concept of a \"fuzzing\" test, but fuzzing has broader implications around invalid data we may explore later in the course. I chose the magic number 100 out of convenience sake.</p> <p>This choice is also impacted by what exactly your test is trying to prove. This end-to-end test is looking to show we are exhaustively covering the expected outcomes. That's reasonable when there are only nine outcomes. However, we also proved this exhaustively in the integration test. An engineer could argue for this end-to-end test, given the existence of other subsystem unit and integration tests, all we really need to convince ourselves of is the correct types of data and response code being returned. This is also a sound end-to-end testing strategy.</p>"},{"location":"threads/system/backend/2-testing-apis/#what-isnt-tested-in-the-play-route","title":"What isn't tested in the <code>/play</code> route?","text":"<p>One aspect of the <code>/play</code> route none of our tests have addressed are exceptional cases where clients send invalid data to the API. For example, perhaps their choice is the string \"bazooka\". What happens then? In the version of FastAPI the tutorial uses, the validation error causes a 422 response to be produced automatically. However, this is not code we wrote yet clients of our service may depend on predictable exception behavior. If we wanted to codify this in a test, to help us catch a regression caused by a different decision in a future versions of FastAPI, we could write a test like so:</p> <pre><code>def test_play_exception():\n    # Arrange\n    client = TestClient(app)\n\n    # Act\n    response = client.post(\"/play\", json={\"user_choice\": \"bazooka\"})\n\n    # Assert\n    assert response.status_code is status.HTTP_422_UNPROCESSABLE_ENTITY\n</code></pre> <p>In industrial strength software, it is generally a best practice to have regression tests for exceptional cases of all endpoints that clients of your API may depend upon. While this test adds 0 lines of new code coverage in our project code, it does add meaningful regression resilience for behaviors our clients rely upon.</p> <p>Now that you have completed the tutorial, please submit the responses to questions on Gradescope.</p>"},{"location":"threads/system/backend/2-testing-apis/#answers","title":"Answers","text":"<p>Answers to the open-ended questions posed in sections above.</p>"},{"location":"threads/system/backend/2-testing-apis/#unit-testing-the-play-route-handler","title":"Unit Testing the <code>play</code> Route Handler","text":"<p>You can find one possible correct implementation of a unit test for the <code>play</code> route handler below:</p> test_play_unit.py<pre><code>def test_play_returns_game_result():\n    # Arrange\n    mock_svc: GameService = MagicMock(spec=GameService)\n    user_choice = GamePlay(user_choice=Choice.rock)\n    expected = GameResult(\n        user_choice=Choice.rock,\n        api_choice=Choice.paper,\n        timestamp=datetime.now(),\n        user_wins=False,\n    )\n    mock_svc.play.return_value = expected\n\n    # Act\n    actual: GameResult = play(user_choice, mock_svc)\n\n    # Assert\n    assert actual == expected\n    mock_svc.play.assert_called_once_with(user_choice)\n</code></pre> <p>If you are thinking wow 18 lines of code to unit test a function with one line of delegation, that seems like a lot! You are not wrong. However, be sure you can reason through both the value of what this is verifying in the system and the \"contract\" it is enforcing. </p> <p>The code could be refactored to use fixtures as part of the arrange steps to reduce some redundancy across multiple tests using the same service mock and choice, if needed. If there were other types of responses, such as 404 Not Found, additional tests would need to be added that could reuse the fixtures and shorten the individual test implementations.</p>"},{"location":"threads/system/backend/2-testing-apis/#patching-target","title":"Patching Target","text":"<p>The target you are attempting to patch is <code>services.random_choice</code>. Remember, you patch the name where it is used. If you look in the <code>services.py</code> imports section, you will see <code>from random import choice as random_choice</code>. Within the <code>services</code> module, this is aliasing the <code>choice</code> function of the <code>random</code> package with the name <code>random_choice</code>. The <code>random_choice</code> function is called to produce the API's guess of rock, paper, or scissors, so this is the fully qualified name we are patching over to control the side-effect.</p>"},{"location":"threads/teamwork/","title":"Tools and Processes for Teamwork","text":"<p>This thread focuses on successful collaboration in software contexts. It covers project management software, source code pull request (PR) workflows, and code review (CR) practices.</p> <p>Software engineering is a team sport. Companies value engineers who can collaborate and communicate at high levels of competence. Being a capable team member who can use the tools and practice the processes of collaboration will help you integrate into high-performing teams and thrive.</p> Readings Topic RD18 On Pair Programming"},{"location":"threads/teamwork/rd_pair_programming/","title":"On Pair Programming","text":"<p>N/A</p>"},{"location":"threads/tools/","title":"Tool Bench for Individual Contributors (IC)","text":"<p>You will learn the essential tools that modern software engineers use to write, test, and package their code, including Git, containers (Docker), automated test runners, AI code agents, and more.</p> <p>Mastering a professional tool bench makes you more capable and productive. It helps AI agents successfully contribute to your projects, too. In industry, understanding the tools in your development environment means you can spend more time solving complex problems with the best tools for the job and less time fighting with your developer setup or taking on jobs with the wrong tools for them.</p> <ul> <li>Integrated Development Environment</li> <li><code>git</code> Source Code Mangement</li> <li>Coding Agents</li> </ul>"},{"location":"threads/tools/code-agent/","title":"Coding Agents","text":"Readings Topic RD06 Review AI-generated Code RD13 How AI Impacts Skill Formation Lessons Topic LS02 Dependency Managers, LLMs, and Agentic IDEs Tasks Topic TK02 Implement a Dependency Manager ADR with an Agent TK04 Professionalizing the Developer Environment"},{"location":"threads/tools/git/","title":"git and Source Code Management","text":"<p>Mastering git is essential for collaboration and version control. This thread covers everything from basic commands to advanced branching strategies.</p> Readings Topic RD02 What is Source Code Management and `git`? RD03 `git`: Core Concepts of a Repository RD04 `git`: Fundamental Subcommands RD07 Branching and Merging RD08 Git Collaboration: Working with Remote Repositories Lessons Topic LS03 Conceptually Understanding Common `git` Operations Tasks Topic TK03              `git` Diagram Practice"},{"location":"threads/tools/git/ch0-introduction/","title":"Ch. 0 What is Source Code Management and <code>git</code>?","text":"<p>Imagine you're working on a group project, writing code with your friends or classmates. Things start small and simple, but as the project grows, chaos sneaks in. Who has the latest version of the code? What if two people edit the same file? What if you need to undo something from last week?</p>"},{"location":"threads/tools/git/ch0-introduction/#source-code-management-scm","title":"Source Code Management (SCM)","text":"<p>Software development is messy. Code evolves over time, bugs are fixed, new features are added, and experiments come and go. Without a system to track these changes, you can easily lose your way or overwrite someone else\u2019s work. Source Code Management tools offer a tried and true solution.</p>"},{"location":"threads/tools/git/ch0-introduction/#a-brief-history-of-scm-tools","title":"A Brief History of SCM Tools","text":"<p>Managing project versions on a team has been a challenge since the early days of stored programs. SCM tools have been around for over 50 years that helped developers manage their projects:</p> <ul> <li>SCSS (Source Code Control System, 1973): One of the earliest systems, it introduced basic source code tracking but lacked collaborative features.</li> <li>RCS (Revision Control System, 1982): Inspired by SCSS, a set of UNIX commands for multiple users to work on single files and track changes with diffs.</li> <li>CVS (Concurrent Versions System, 1990): Introduced the concept of a central repository but struggled with merging changes from multiple developers.</li> <li>Subversion (SVN, 2000): Improved on CVS by offering atomic commits and better branching, but still relied on a central server.</li> <li>Distributed Systems (e.g., BitKeeper, 2000): Allowed for more flexible workflows, without a centralized server, but was closed-source, paid software. It was a primary basis of inspiration for <code>git</code>.</li> <li><code>git</code> (2005): Today's leading open source SCM was designed to handle large projects with speed, reliability, and a distributed architecture. Created by Linus Torvalds to become the Linux operating system's SCM, <code>git</code> has an interesting origin story, if you're into computing history. </li> </ul>"},{"location":"threads/tools/git/ch0-introduction/#what-is-git","title":"What is <code>git</code>?","text":"<p><code>git</code> is a command-line tool for managing version control in projects. It allows developers to track changes in a codebase, collaborate with others, and maintain a history of project development. Unlike some tools that rely on a central server, <code>git</code> is distributed, so every user has a full copy of the project\u2019s history on their local machine.</p>"},{"location":"threads/tools/git/ch0-introduction/#what-is-a-git-repository","title":"What is a <code>git</code> Repository?","text":"<p>A <code>git</code> repository (often shortened to \"repo\") is a project's organized history. It stores all the information about a project, including its versions, files, branches, and more. Repositories exist locally on you and your teammates' machines, but can also be synchronized with remote repositories on the internet, via services such as GitHub.</p>"},{"location":"threads/tools/git/ch0-introduction/#what-is-github","title":"What is GitHub?","text":"<p>It\u2019s common to hear <code>git</code> and GitHub mentioned together, often confused with one another, but it is important to recognize they\u2019re not at all the same.</p> <p>GitHub is a cloud-based platform built on top of <code>git</code>. It provides a space to host <code>git</code> repositories online, making it easier to collaborate with others by sharing your code. GitHub adds extra features for software engineering teams you will learn all about in COMP423.</p> <p>While GitHub is one of the most popular platforms, it\u2019s not the only one. Alternatives like GitLab, Bitbucket, and others offer similar functionality.</p>"},{"location":"threads/tools/git/ch0-introduction/#why-engineers-still-choose-git-20-years-later","title":"Why Engineers Still Choose <code>git</code> 20+ Years Later","text":"<p><code>git</code> is the most popular SCM today for many reasons:</p> <ol> <li>Distributed Workflow: Every team member gets their own complete copy of the project's history. This means you can work offline, experiment freely, and share your work when you're ready without blocking your team mates.</li> <li>Fast and Lightweight: <code>git</code> is designed to handle projects of any size efficiently, from small hobby projects to large-scale codebases. Its commands complete shockingly past compared to historical SCMs.</li> <li>Powerful Collaboration: <code>git</code> enables safe collaboration with the ability review a team mate's work and incorporate thier changes when it's ready, without risking loss of your own work.</li> <li>Safety Net: By keeping a history of snapshots of your project, <code>git</code> enables you to undo mistakes or recover something you deleted and later realized you needed.</li> <li>Open Source and Available: <code>git</code> is free, open-source, and widely supported across different platforms.</li> </ol>"},{"location":"threads/tools/git/ch0-introduction/#installing-git-and-checking-its-version","title":"Installing <code>git</code> and Checking its Version","text":"<p>Let's be sure <code>git</code> is installed on your machine! You can check by opening a Terminal, preferably a <code>bash</code> terminal on Windows over PowerShell, and running the command <code>git --version</code>. If a reasonably modern version prints out, you're good to go! If not, continue below.</p>"},{"location":"threads/tools/git/ch0-introduction/#windows","title":"Windows","text":"<ol> <li>Download the <code>git</code> installer from git-scm.com.</li> <li>Run the installer.</li> <li>During setup, use the default options unless you know what you're doing. Key things to confirm:</li> <li>Adjusting your PATH environment: Choose \"Use <code>git</code> from the command line and also from 3rd-party software.\"</li> <li>Default editor: Select Visual Studio Code.</li> <li>After installation, open the Command Prompt and run:    <pre><code>git --version\n</code></pre>    This command checks your <code>git</code> installation by displaying the installed version number. If you see the version number, you're good to go!</li> </ol> <p>Sample output:    <pre><code>git version 2.39.5\n</code></pre>    This confirms that <code>git</code> is properly installed and ready to use. It's okay if your version is different! <code>git</code> takes pride in being highly backwards compatible, so most commands will work the same way regardless of your version.</p>"},{"location":"threads/tools/git/ch0-introduction/#macos","title":"macOS","text":"<ol> <li>Open your terminal.</li> <li>Run the following command:    <pre><code>git --version\n</code></pre>    If <code>git</code> isn\u2019t installed, your Mac will prompt you to install the Command Line Developer Tools. Confirm the installation and follow the prompts.</li> <li>Once installed, verify it works by running:    <pre><code>git --version\n</code></pre>    Sample output:    <pre><code>git version 2.39.5\n</code></pre>    This indicates that <code>git</code> is successfully installed on your macOS system. It's okay if your version is different! <code>git</code> takes pride in being highly backwards compatible, so most commands will work the same way regardless of your version.</li> </ol>"},{"location":"threads/tools/git/ch0-introduction/#what-to-expect-next","title":"What to Expect Next","text":"<p>Now that you know why <code>git</code> is such a valuable tool, it\u2019s time to learn how to use it. In the next chapter we\u2019ll start using its basic commands.</p>"},{"location":"threads/tools/git/ch1-git-structure/","title":"Ch. 1 Core Concepts of a <code>git</code> Repository","text":"<p><code>git</code> becomes much easier to learn and operate with a high-level understanding of its organization and data structure design.</p>"},{"location":"threads/tools/git/ch1-git-structure/#snapshots-not-diffs","title":"Snapshots, Not Diffs","text":"<p><code>git</code> doesn\u2019t store the history of differences (diffs) between versions. Instead, it captures a history of snapshots of your project. Each snapshot is like a photograph of your entire project's contents at the moment you decide to commit to the snapshot.</p> <p>The technical design decisions behind <code>git</code>'s data structures are brilliant. We'll learn more about them in time. For now, let's just appreciate the features its design enables:</p> <ul> <li>Local History: Your entire project history is stored locally on your machine. This means you can explore or revert changes without needing an internet connection or accessing a central server.</li> <li>Quick Navigation: Since snapshots contain the full project state, Git can near instantly move to any commit without recalculating the state from diffs.</li> <li>Efficient Storage: Git only stores files that change between snapshots. Identical files are saved just once and linked back to in later snapshots, which saves space while maintaining integrity.</li> <li>Efficient Comparisons: Git uses hashing to compare files and directories. This allows it to quickly identify changes without scanning the entire file contents.</li> </ul>"},{"location":"threads/tools/git/ch1-git-structure/#what-is-a-commit","title":"What is a Commit?","text":"<p>Building on the concept of snapshots and data integrity, a commit is a saved snapshot of your project at a specific point in time. It contains:</p> <ol> <li>File Contents: The exact state of the files that were committed.</li> <li>Metadata:<ul> <li>Who made the commit (name and email).</li> <li>When the commit was made (timestamp).</li> <li>A message describing the changes.</li> </ul> </li> <li>Parent Pointer(s): Each commit records a reference to one or more parent commits. Most commits have a single parent, but merge commits can have multiple parents.</li> <li>Commit ID Hash: Every commit is assigned a unique cryptographic hash, which acts as its identity. This hash is generated based on the commit\u2019s contents, metadata, and parent pointers. It ensures the integrity of the commit, making it impossible to alter the commit without detection. If even a single byte changes, the hash changes too, signaling that the commit has been tampered with. This immutability is central to Git's reliability.</li> </ol> <p>Here\u2019s an example of what a commit\u2019s metadata might look like:</p> <pre><code>commit 5d41402abc4b2a76b9719d911017c592\nParent: e38ad214bd57c8c7f69a1d8f6326b3d1f3e6a2ef\nAuthor: Your Name &lt;your.email@example.com&gt;\nDate:   Tue Jan 1 12:00:00 2025 +0000\n\n    Add README file\n</code></pre> <p>The hash (<code>5d41402abc4b2a76b9719d911017c592</code>) uniquely identifies the commit and protects its contents from tampering.</p>"},{"location":"threads/tools/git/ch1-git-structure/#commit-history","title":"Commit History","text":"<p>A repository is essentially a timeline of snapshots, with each commit representing a specific state of the project. These snapshots are linked together because each commit holds a reference to its \"parent\" commit's hash ID, forming a chain. </p> <p>In a linear history, this structure is equivalent to a singly linked list. However, as you start working with other team members or experimenting with your own ideas in branches, histories begin to diverge. A diverging history occurs when more than one commit shares the same parent. Merge commits, on the other hand, have multiple parents when branches are combined.</p> <p>These relationships create a directed acyclic graph (DAG)\u2014a concept you\u2019ve seen in COMP210: Data Structures. Here, the DAG ensures that commits are immutable and history is consistent and reliable. Since commits can only reference existing ones and cannot be altered, your entire history remains tamper-proof.</p>"},{"location":"threads/tools/git/ch1-git-structure/#navigating-and-understanding-history","title":"Navigating and Understanding History","text":"<ul> <li>The Commit Log: Think of the commit log as a journal of everything that has happened in your project. Commands like <code>git log</code> let you view this history, showing each commit\u2019s hash, author, date, and message.</li> <li>Revisiting Changes: Each commit is like a bookmark in your project. You can move back and forth in history, inspecting past states or even restoring them.</li> <li>Collaboration Made Easy: Since every commit includes a message and is linked to its parent, your teammates can see not just what changed but why. This transparency makes it easier to review and integrate work.</li> </ul>"},{"location":"threads/tools/git/ch1-git-structure/#the-working-areas-of-a-git-repository","title":"The Working Areas of a <code>git</code> Repository","text":"<p>In addition to the committed history, a <code>git</code> repository organizes files and changes into two additional areas where you will spend most of your time:</p> <ol> <li>Working Directory: This is where you directly interact with your project's files. It reflects your local version of the project, where you edit, create, and delete files.</li> <li>Staging Area: Also called the index, this is a middle ground where you prepare changes from your working directory before committing them to the repository\u2019s history. Think of it as framing your next snapshot, letting you carefully curate and organize the changes you want to commit to your project\u2019s history. </li> </ol>"},{"location":"threads/tools/git/ch1-git-structure/#flow-between-conceptual-areas","title":"Flow Between Conceptual Areas","text":"<p>Here\u2019s a high-level view of how files move through these areas during a typical workflow:</p> <pre><code>sequenceDiagram\n    participant WD as Working Directory\n    participant SA as Staging Area\n    participant CH as Committed History\n\n    WD-&gt;&gt;SA: git add (stage changes)\n    SA-&gt;&gt;CH: git commit (save snapshot)</code></pre> <p>This flow illustrates how the work you do in your version makes its way into your project's local history. You start by editing files in the Working Directory, select specific changes to stage in the Staging Area, and then finalize those changes into the Committed History with a commit.</p> <p>In the next chapter, you will learn all the fundamental <code>git</code> operations necessary to start a new <code>git</code> repository, perform some work, and form a commit history.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/","title":"Ch. 2 Fundamental <code>git</code> Subcommands","text":"<p>This is a hands-on tutorial! Follow along by running all the commands as we go. By the end, you\u2019ll have a solid foundation of the most fundamental <code>git</code> operations.</p> <p>By the end of this chapter, you\u2019ll know what a subcommand is and have worked with several essential <code>git</code> subcommands:</p> <ul> <li><code>git config</code>: Setting up your identity for commits and other configurations.</li> <li><code>git init</code>: Initialize a folder as a new, empty <code>git</code> repository.</li> <li><code>git add</code>: Staging files for your next commit.</li> <li><code>git commit</code>: Creating a commit to record a snapshot of your project.</li> <li><code>git status</code>: Checking the current state of your repository.</li> <li><code>git log</code>: Viewing your project\u2019s history.</li> <li><code>git restore</code>: Reverting changes when something goes awry.</li> <li><code>git checkout</code>: Pulling files, and more, from a project's history of commits.</li> </ul> <p>These subcommands will form the foundation of your <code>git</code> expertise, enabling you to start every project on the right foot and build confidence as you explore deeper features. Let\u2019s jump right in!</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#understanding-git-subcommands","title":"Understanding <code>git</code> Subcommands","text":"<p><code>git</code> is a command-line program that relies on a subcommand convention to perform specific actions to manage your repository. A subcommand is the first string argument following <code>git</code>. Each subcommand has its own purpose, many you will begin to learn in this chapter: initializing a repository (<code>git init</code>), checking the state of your repo (<code>git status</code>), or committing changes (<code>git commit</code>).</p> <p>As you get familiar with <code>git</code>, you\u2019ll find yourself using sequences of subcommands to accomplish tasks. The beauty of <code>git</code> lies in its flexibility, but that can also make it overwhelming at first. Don\u2019t worry\u2014<code>git</code> includes a built-in help system for you.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#getting-help-with-subcommands","title":"Getting Help with Subcommands","text":"<p>If you\u2019re ever unsure about how to use a subcommand or what options it supports, you can ask <code>git</code> for help directly from the command line. For example:</p> <pre><code>git help commit\n</code></pre> <p>This will open a manual page describing what the <code>commit</code> subcommand does, its syntax, and the available options. For example, you\u2019ll see some descriptive, helpful text like this:</p> <pre><code>NAME\n       git-commit - Record changes to the repository\n\nSYNOPSIS\n       git commit [-m &lt;msg&gt;]\n\nDESCRIPTION\n       Create a new commit containing the current contents of the index and\n       the given log message describing the changes.\n</code></pre> <p>You can also use shorthand to view help inline by adding <code>--help</code> after the subcommand, like so:</p> <pre><code>git commit --help\n</code></pre> <p>This approach works for any subcommand. It\u2019s a great way to learn as you go and explore new tools in the <code>git</code> toolbox.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#consulting-with-an-llm","title":"Consulting with an LLM","text":"<p>If you find yourself struggling to remember the exact subcommand or short flag to achieve your goal with <code>git</code>, or how to do something complex, a great use of an LLM like ChatGPT is asking it how to achieve your specific task. Describe what you want to do, ask for the <code>git</code> command(s) you need, and ask it to explain each step to you. Ultimately, you need to know what <code>git</code> can do and how to think in terms of its underlying data structures and conceptual model. That's why we are putting a significant emphasis on mastering <code>git</code> in COMP423!</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#configure-your-name-and-email","title":"Configure your name and email","text":"<p>Before you start using <code>git</code>, set your name and email. This information will be attached to your commits.</p> <p>Run these commands in your terminal, replacing the placeholders with your details:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre> <p>These commands tell <code>git</code> who you are. The <code>--global</code> flag ensures this configuration applies to all your projects.</p> <p>You can check your configuration anytime with:</p> <pre><code>git config --list\n</code></pre> <p>Sample output:</p> <pre><code>user.name=Your Name\nuser.email=your.email@example.com\n</code></pre> <p>This confirms your details are correctly configured. You can also see additional configuration settings. We will not worry ourselves with the depths of <code>git</code> configurability in this course.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#initializing-your-first-repository","title":"Initializing Your First Repository","text":""},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#step-1-create-a-directory","title":"Step 1: Create a Directory","text":"<p>Let\u2019s create a new directory for your project:</p> <pre><code>mkdir git-for-423\ncd git-for-423\n</code></pre> <p>Here\u2019s what these commands do:</p> <ul> <li><code>mkdir git-for-423</code>: Creates a new directory called <code>git-for-423</code>.</li> <li><code>cd git-for-423</code>: Changes into that directory so you can work inside it.</li> </ul>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#step-2-initialize-git","title":"Step 2: Initialize <code>git</code>","text":"<p>Turn this directory into a <code>git</code> repository by running:</p> <pre><code>git init\n</code></pre> <p>This creates a hidden <code>.git</code> directory, which is where all the magic happens. The <code>.git</code> directory contains:</p> <ul> <li>Your entire project history, including every commit.</li> <li>Metadata about your repository.</li> <li>Configuration files and references to other states in your project.</li> </ul> <p>It\u2019s incredible that even for massive projects with thousands of files and decades of history, this one hidden directory contains everything <code>git</code> needs to manage the project.</p> <p>You can peek at it:</p> <pre><code>ls -a\n</code></pre> <p>Sample output:</p> <pre><code>.  ..  .git\n</code></pre> <p>This shows the <code>.git</code> directory alongside the regular and parent directory entries. The <code>.git</code> directory is what transforms a regular directory into a powerful <code>git</code> repository. If you peek further inside, with <code>ls .git</code>, you will see that there are more files and directories in it. The details of this structure are below the layer of abstraction we are focused on in learning <code>git</code>. However, note that your whole repo's history will be right there and there's no magic to it, just data structures and algorithms!</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#making-your-first-commit","title":"Making Your First Commit","text":""},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#step-1-create-a-file","title":"Step 1: Create a File","text":"<p>Let\u2019s start by adding a README file:</p> <pre><code>echo '# Welcome to COMP423!\\n' &gt; README.md\n</code></pre> <p>This command creates a file named <code>README.md</code> with the text `# Welcome to COMP423!` as its content. The <code>&gt;</code> operator redirects the text into the file. You learned about output redirection in COMP211: Systems Fundamentals.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#step-2-check-the-repository-status","title":"Step 2: Check the Repository Status","text":"<p>Use <code>git status</code> to see what\u2019s happening in your repo:</p> <pre><code>git status\n</code></pre> <p>Sample output:</p> <pre><code>On branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    README.md\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n</code></pre> <p>This tells you that <code>README.md</code> is untracked, meaning it isn\u2019t part of version control yet.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#step-3-stage-the-file","title":"Step 3: Stage the File","text":"<p>To tell <code>git</code> you want to include <code>README.md</code> in your next snapshot, stage it:</p> <pre><code>git add README.md\n</code></pre> <p>Sample output:</p> <pre><code>Changes to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    new file:   README.md\n</code></pre> <p>This confirms that <code>README.md</code> is staged and ready to commit.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#step-4-commit-your-changes","title":"Step 4: Commit Your Changes","text":"<p>Create your first commit:</p> <pre><code>git commit -m \"Add README file\"\n</code></pre> <p>The <code>-m</code> flag specifies a commit message directly in the command.</p> <p>Sample output:</p> <pre><code>[main (root-commit) abc1234] Add README file\n 1 file changed, 1 insertion(+)\n create mode 100644 README.md\n</code></pre> <p>This shows:</p> <ul> <li>The branch name (<code>main</code>) and that it's the <code>root-commit</code> where root is the first \"node\" in a repository.</li> <li>The unique commit hash (<code>abc1234</code>). Your hash will be different!</li> <li>A summary of changes: 1 file added with 1 line of content.</li> </ul> <p>Congratulations! You\u2019ve just recorded your first piece of project history.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#adding-another-commit","title":"Adding Another Commit","text":"<p>Modify <code>README.md</code>:</p> <pre><code>echo \"This repository is for learning git.\" &gt;&gt; README.md\n</code></pre> <p>This appends the text to <code>README.md</code>. The <code>&gt;&gt;</code> operator adds text to the file without overwriting it. You may recall learning about this in COMP211: Systems Fundamentals.</p> <p>Check the status:</p> <pre><code>git status\n</code></pre> <p>Sample output:</p> <pre><code>On branch main\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   README.md\n</code></pre> <p>This shows that <code>README.md</code> has been modified but not yet staged.</p> <p>Stage the changes:</p> <pre><code>git add README.md\n</code></pre> <p>Commit them:</p> <pre><code>git commit -m \"Update README with project purpose\"\n</code></pre> <p>Sample output:</p> <pre><code>[main abc5678] Update README with project purpose\n 1 file changed, 1 insertion(+)\n</code></pre> <p>Now your repository has two commits. Use <code>git log</code> to view them:</p> <pre><code>git log\n</code></pre> <p>Sample output:</p> <pre><code>commit abc5678\nAuthor: Your Name &lt;your.email@example.com&gt;\nDate:   Tue Jan 1 12:05:00 2025 +0000\n\n    Update README with project purpose\n\ncommit abc1234\nAuthor: Your Name &lt;your.email@example.com&gt;\nDate:   Tue Jan 1 12:00:00 2025 +0000\n\n    Add README file\n</code></pre> <p>Each commit references its parent(s), forming the DAG of your project history.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#how-git-tracks-file-and-repo-state","title":"How <code>git</code> Tracks File and Repo State","text":"<p>Here\u2019s a diagram of how <code>git</code> tracks files:</p> <pre><code>sequenceDiagram\n    participant WorkingDir as Working Directory&lt;br&gt;(Unchanged)\n    participant Changed as Working Directory&lt;br&gt;(Changed)\n    participant Staged as Staged&lt;br&gt;(Index)\n    participant Committed as Commit&lt;br&gt;(Committed History)\n\n    WorkingDir -&gt;&gt; Changed: Modify File\n    Changed -&gt;&gt; Staged: git add\n    Staged -&gt;&gt; Committed: git commit\n    Changed -&gt;&gt; WorkingDir: git restore &lt;file&gt;\n    Staged -&gt;&gt; Changed: git restore --staged &lt;file&gt;\n    Committed -&gt;&gt; WorkingDir: git checkout &lt;commit&gt; &lt;file&gt;</code></pre> <p>Use <code>git status</code> often to see where your files are in this flow.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#experimenting-with-commits-and-changes","title":"Experimenting with Commits and Changes","text":"<p>Now that you\u2019ve made a few commits, let\u2019s dive deeper into how <code>git</code> helps you manage and explore your project\u2019s history. Follow these steps to get hands-on experience:</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#1-view-your-projects-history","title":"1. View Your Project\u2019s History","text":"<p>Use the <code>git log</code> command to see all your commits:</p> <pre><code>git log\n</code></pre> <p>For a compact summary, try the <code>--oneline</code> option:</p> <pre><code>git log --oneline\n</code></pre> <p>This shows each commit\u2019s hash and its message in a single line, making it easier to navigate.</p> <p>Shortened Hashes in <code>git</code></p> <p>In <code>git</code>, you can refer to commits by shortened versions of their hashes. For example, instead of using the full SHA-1 hash like <code>abc5678abc5678abc5678abc5678abc5678abc5</code>, you can simply use the first few characters, like <code>abc5678</code>. This works as long as the prefix uniquely identifies a commit in the repository.</p> <p>Remember from COMP210 how hash tables rely on unique keys and the odds of collision depend on the hash function and key space? <code>git</code>'s SHA-1 hashes are 160 bits long, providing \\(2^{160}\\) possible values\u2014an astronomically large number. Even with millions of commits, the probability of two full hashes colliding is practically zero.</p> <p>However, when using shortened hashes, you only need enough characters to uniquely identify a commit. For instance, using 5 hexadecimal characters gives \\(16^5 = 1,048,576\\) possible values. In a project like ours with only two commits so far, the chance of ambiguity is nonexistent. As your project grows, <code>git</code> will warn you if a prefix becomes ambiguous and requires more characters to ensure uniqueness.</p> <p>Challenge</p> <p>Look for the hash of your first commit. You\u2019ll use it in the next step!</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#2-revisit-a-previous-commit","title":"2. Revisit a Previous Commit","text":"<p>Imagine you want to see what your project looked like at the time of your first commit.</p> <ol> <li> <p>Use the <code>git checkout</code> command to switch to that specific commit:    <pre><code>git checkout &lt;commit-hash&gt;\n</code></pre>    Replace <code>&lt;commit-hash&gt;</code> with the hash of your first commit (e.g., <code>abc1234</code>).</p> </li> <li> <p>Inspect the file\u2019s contents:    <pre><code>cat README.md\n</code></pre></p> </li> </ol> <p>Notice how the content matches the snapshot from your first commit.</p> <ol> <li>Return to the latest version of your project:    <pre><code>git checkout main\n</code></pre></li> </ol> <p>Tip</p> <p>Always return to a branch (like <code>main</code>) when you\u2019re done exploring past commits to avoid confusion.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#3-undo-a-file-change","title":"3. Undo a File Change","text":"<p>Let\u2019s say you accidentally modify a file and want to undo the changes. For example:</p> <ol> <li> <p>Modify the <code>README.md</code> file:    <pre><code>echo \"Oops, made a mistake!\" &gt;&gt; README.md\n</code></pre></p> </li> <li> <p>Use <code>git restore</code> to revert the file to its previous state:    <pre><code>git restore README.md\n</code></pre></p> </li> <li> <p>Verify the file is back to its original form:    <pre><code>cat README.md\n</code></pre></p> </li> </ol>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#4-unstage-changes","title":"4. Unstage Changes","text":"<p>Suppose you\u2019ve staged a file but then change your mind. Here\u2019s how to unstage it:</p> <ol> <li> <p>Modify and stage the file:    <pre><code>echo \"Temporary change\" &gt;&gt; README.md\ngit add README.md\n</code></pre></p> </li> <li> <p>Unstage the file:    <pre><code>git restore --staged README.md\n</code></pre></p> </li> </ol> <p>Run <code>git status</code> to confirm the file is no longer staged but still modified.</p>"},{"location":"threads/tools/git/ch2-git-fundamental-subcommands/#5-challenge-explore-and-restore","title":"5. Challenge: Explore and Restore","text":"<p>Use what you\u2019ve learned to experiment:</p> <ul> <li>Inspect specific file versions from past commits using <code>git checkout</code>.</li> <li>Revert untracked or staged changes using <code>git restore</code>.</li> <li>Play around with <code>git log</code> to navigate your project history.</li> </ul> <p>By combining these commands, you can confidently manage and explore your repository\u2019s state at any point in its timeline.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/","title":"Ch. 3 Branching and Merging","text":"<p>Imagine this: you're working on a project like a collaborative web app, and you want to test out a bold new feature\u2014maybe integrating a third-party login system\u2014without disrupting the stable version already in use by other team members. In software teams, branching allows experimentation like this without risk. Or maybe you're part of a team, and multiple people are working on different features at the same time. How do you keep everything organized and avoid stepping on each other's toes? That\u2019s where branching and merging come in\u2014two of Git\u2019s most powerful features.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#what-is-a-branch","title":"What is a Branch?","text":"<p>In Git, a branch has very simple, beautiful implementation: a branch is just a named pointer to a commit with the special behavior that when you create a new commit while working on a branch, the branch pointer automatically updates to reference the new commit. No other branch pointers are updated. This implementation idea gives rise to a powerful conceptual abstraction: branches conceptually represent multiple parallel version histories in a repository. </p> <p>When you first create a new branch, no change in your project's history occurs; instead, a new pointer to the last commit on your current branch is created. This means the two branches start off identical, both pointing to the same commit, sharing the same history. As two individual branches have additional commits added to them independently their histories will\u00a0diverge, reflecting the different paths of development taken. When you decide it is time to incorporate work on one branch back into another branch, one branch's changes can be merged\u00a0into another branch.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#why-use-branches","title":"Why Use Branches?","text":"<p>Branches are incredibly useful for:</p> <ul> <li>Experimentation: Try out new ideas without impacting the stable version of your project.</li> <li>Parallel Development: Multiple team members can work on different features or fixes at the same time.</li> <li>Code Review: Branches allow you to submit changes for review before merging them into the main codebase.</li> </ul> <p>By isolating work on separate branches, you reduce the risk of overwriting someone else\u2019s changes or introducing bugs into your <code>main</code> branch.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#branch-early-and-often","title":"Branch Early and Often","text":"<p>One of the best things about Git branches is how lightweight and fast they are. Think about why their implementation makes them lightweight and fast. This speed allows developers to create and switch between branches almost instantaneously, which is especially beneficial in modern workflows. For example, developers can branch off to work on features or fixes, test their changes in isolation, and merge them back quickly without delaying others\u2019 progress. There\u2019s virtually no cost to having as many branches as you need. This makes branches an essential tool for developers.</p> <p>Key idiom: Branch early and often! Instead of making all your changes directly on <code>main</code> or a long-lived branch, create a branch for each new feature, bug fix, or experiment. This approach keeps your work isolated, makes it easier to collaborate, and allows for smoother integration later. Don\u2019t hesitate\u2014branches are free, use them liberally!</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#what-is-head-gits-current-working-branch","title":"What is <code>HEAD</code>? Git's Current Working Branch","text":"<p><code>HEAD</code> is a special pointer in Git that tells you where you are currently working in your project\u2019s history. When <code>HEAD</code> is \"attached\" to a branch, you can think of <code>HEAD</code> as your current working branch. This is analogous to how your shell maintains your current working directory (CWD) such that shell commands you run are relative to your CWD. Git commands are relative to <code>HEAD</code>. </p> <p>Understanding <code>HEAD</code> helps you anticipate how Git commands behave:</p> <ul> <li><code>git commit</code>: Creates a new commit whose parent is the commit <code>HEAD</code> currently refers to. The branch <code>HEAD</code> is attached to is updated to refer to the freshly minted commit. This is how branches stay current with their latest commit.</li> <li><code>git log</code>: By default, displays commit history starting from <code>HEAD</code>.</li> <li><code>git restore</code>: Reverts files to the state they were in at the commit <code>HEAD</code> resolves to, allowing you to discard unwanted changes.</li> <li><code>git switch</code>: Causes <code>HEAD</code> to attach to a different branch and updates your working directory's contents to match the snapshot of that branch\u2019s latest commit. If you have modified files that haven\u2019t been staged, <code>git switch</code> will fail with a message letting you know you have uncommitted changes that are at risk of being overwritten.</li> </ul> <p>Sometimes <code>HEAD</code> is not attached to a branch.</p> <p>You will learn more about a detached <code>HEAD</code> state soon. It sounds spookier than it is. It just means <code>HEAD</code> points to a specific commit rather than to a branch. As soon as you create a new branch, which you will learn how to do next, you will no longer be in a detached <code>HEAD</code> state. This is useful when you want to go back to check out a commit that no branch currently points to, but nothing to concern ourselves with now.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#working-with-branches","title":"Working with Branches","text":"<p>To work with branches in Git, the recommended modern commands are <code>git branch</code> and <code>git switch</code>. While <code>git checkout</code> is still available and widely used, it has a broader scope, which can make it less intuitive for branch-specific tasks. Let\u2019s dive into best practices:</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#creating-a-branch","title":"Creating a Branch","text":"<p>To create a new branch, use the following command:</p> <pre><code>git branch cool-feature\n</code></pre> <p>This creates the branch but doesn\u2019t switch you to it. This command only creates a new pointer to the current commit that the <code>HEAD</code> branch is on. No history has changed, and no parallel history exists yet. At this point, the two branches are exactly equivalent to each other and both point to the exact same commit.</p> <p>To start working on the new branch immediately, use:</p> <pre><code>git switch cool-feature\n</code></pre> <p>Once you understand these two steps independently, you can combine them with one command:</p> <pre><code>git switch --create cool-feature\n</code></pre> <p>The <code>--create</code> flag, whose short variant is <code>-c</code>, combines creating a branch and switching to it in one command. Now, <code>HEAD</code> is pointing to <code>cool-feature</code>, and you\u2019re ready to make changes.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#viewing-branches","title":"Viewing Branches","text":"<p>To see all the branches in your project and which one <code>HEAD</code> is pointing to:</p> <pre><code>git branch\n</code></pre> <p>For example, if you have two branches (<code>main</code> and <code>cool-feature</code>), and you are currently on the <code>cool-feature</code> branch, the output will look like this:</p> <pre><code>* cool-feature\n  main\n</code></pre> <p>The asterisk (<code>*</code>) indicates the branch that <code>HEAD</code> is currently pointing to. Again, think of <code>HEAD</code> as your current working branch.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#adding-a-new-commit-to-a-branch","title":"Adding a New Commit to a Branch","text":"<p>Once you\u2019re on the <code>cool-feature</code> branch and ready to make changes, you can add a new commit as follows:</p> <p>(1) Modify a file in your project, for example, editing <code>README.md</code> to include some additional content.</p> <pre><code># Welcome to COMP423!\nThis repository is for learning git.\nBranching and merging is powerful!\n</code></pre> <p>(2) Stage the changes using <code>git add</code>, review your staged work with <code>git status</code>:</p> <pre><code>git add README.md\ngit status\n</code></pre> <p>(3) Commit the changes with a descriptive message:</p> <pre><code>git commit -m \"Add a note about branching and merging\"\n</code></pre> <p>After committing, Git will output something like:</p> <pre><code>[cool-feature abc1234] Add a note about branching and merging\n 1 file changed, 7 insertions(+)\n</code></pre> <p>(4) Inspect the commit history using <code>git log</code> to see how the branch\u2019s <code>HEAD</code> has moved forward:</p> <pre><code>git log --oneline\n</code></pre> <p>Example output:</p> <pre><code>abc1234 (HEAD -&gt; cool-feature) Add a note about branching and merging\n</code></pre> <p>Here, you can see that <code>HEAD</code> has advanced to the new commit <code>abc1234</code> (your commit ID will be different) on the <code>cool-feature</code> branch.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#how-head-and-branch-updates-work","title":"How <code>HEAD</code> and Branch Updates Work","text":"<p>When you create a commit, Git uses <code>HEAD</code> to determine the parent of the new commit. The new commit will have the commit that <code>HEAD</code> was previously pointing to as its parent. After the commit is created, Git updates the branch that <code>HEAD</code> is attached to so that it points to the new commit. This is why the branch you\u2019re working on moves forward with each commit you make. Yes, this is the second or third time this tutorial has repeated this and for good reason: once your mental model fully internalizes this concept you will find working with branches much, much easier to understand!</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#switching-between-branches","title":"Switching Between Branches","text":"<p>To move between branches, whose histories are now different, use <code>switch</code> again:</p> <pre><code>git switch main\n</code></pre> <p>In practical terms, switching branches allows you to work on completely different features or bug fixes without overwriting or disrupting your current progress. For example, your <code>cool-feature</code> branch included some new text in <code>README.md</code>, switching back to <code>main</code> will revert the working directory, and therefore <code>README.md</code>, to reflect the last commit made in the <code>main</code> branch. You can then easily switch back to <code>cool-feature</code> and be back on its timeline. This separation ensures that changes in progress do not accidentally affect production or stable environments.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#handling-in-progress-changes-when-switching-branches","title":"Handling In-Progress Changes When Switching Branches","text":"<p>Sometimes, you\u2019ll need to switch branches while you have in-progress changes in your working directory or staging. For example, you might need to review a colleague\u2019s work or fix a bug on another branch. </p> <p>In such cases, you have three main strategies to proceed with:</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#1-commit-your-changes-to-your-branch","title":"1. Commit Your Changes to Your Branch","text":"<p>If your current changes are in a good state, you can commit them to the current branch before switching:</p> <pre><code>git add .\ngit commit -m \"WIP: Save progress on feature\"\n</code></pre> <p>This ensures your work is saved and associated with the current branch. Once committed, you can switch branches without any issues:</p> <pre><code>git switch branch-name\n</code></pre> <p>WIP is Work in Progress</p> <p>WIP is a common acronym in the softare engineering world. It is an abbreviation for Work in Progress. We are all WIPs.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#2-stash-your-changes-away-temporarily","title":"2. Stash your Changes away Temporarily","text":"<p>If your changes are not ready to be committed, you can temporarily set them aside using the stash:</p> <pre><code>git stash\n</code></pre> <p>This stores your changes in a separate stash area and reverts your working directory to match the last commit. You can then switch branches:</p> <pre><code>git switch branch-name\n</code></pre> <p>When you return to your original branch, you can restore the stashed changes by \"popping\" them from your stash stack:</p> <pre><code>git stash pop\n</code></pre>"},{"location":"threads/tools/git/ch3-git-branch-merge/#3-discard-your-changes","title":"3. Discard Your Changes","text":"<p>If the changes you\u2019ve made aren\u2019t needed, you can discard them using previously learned commands:</p> <pre><code>git restore --staged .\ngit restore .\n</code></pre> <p>These commands resets your working directory and clear your staging index to match the last commit on the branch, effectively throwing away any modifications. Once reset, you\u2019re free to switch branches:</p> <pre><code>git switch branch-name\n</code></pre>"},{"location":"threads/tools/git/ch3-git-branch-merge/#merging-branches","title":"Merging Branches","text":"<p>Once your work on a branch is complete, you\u2019ll want to combine it with another branch (usually <code>main</code>). Merging into <code>main</code> is a best practice because it keeps the central branch stable and reflects the latest working version of your project. This aligns with workflows like trunk-based development, where small, frequent merges into a shared branch help reduce integration problems and ensure that everyone is working from a reliable codebase. This process is called merging.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#fast-forward-merge-vs-merge-commit","title":"Fast-Forward Merge vs. Merge Commit","text":"<p>When merging, Git uses two main strategies:</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#fast-forward-merge","title":"Fast-Forward Merge","text":"<p>If the branch being merged hasn\u2019t diverged from the target branch (e.g., no new commits were made on <code>main</code> since <code>cool-feature</code> started), Git can simply move the pointer of the target branch forward to the latest commit of the merged branch. This is called a fast-forward merge.</p> <p>For instance, imagine a developer creates a branch for fixing a small bug and completes the fix without any changes occurring on <code>main</code> in the meantime. A fast-forward merge is efficient and keeps the history clean:</p> <pre><code>git merge cool-feature\n</code></pre> <p>After the merge, the history will look like a single line of commits, as if all the work was done directly on the target branch <code>main</code>.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#merge-commit","title":"Merge Commit","text":"<p>If the branches have diverged (e.g., both <code>main</code> and <code>cool-feature</code> have new commits), Git creates a merge commit to combine their histories. A merge commit has two parent commits, representing the tips of the branches being merged.</p> <p>This strategy is particularly useful in larger projects where multiple developers are contributing. For example, if one developer has added a new feature while another has updated documentation on <code>main</code>, a merge commit preserves the distinct contributions:</p> <pre><code>git merge cool-feature\n</code></pre> <p>Here\u2019s how the history looks with a merge commit:</p> <pre><code>*   Merge branch 'cool-feature'\n|\\\n| * Commit on cool-feature\n* | Commit on main\n|/\n</code></pre> <p>To inspect a merge commit and see its parents:</p> <pre><code>git log --graph --oneline\n</code></pre> <p>Merge commits make it easier to trace where specific changes originated, which can be critical for debugging or auditing code.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#step-1-switch-to-the-target-branch","title":"Step 1: Switch to the Target Branch","text":"<p>First, switch to the branch you want to merge into (e.g., <code>main</code>):</p> <pre><code>git checkout main\n</code></pre> <p>This step is crucial because Git applies merge operations to the branch that <code>HEAD</code> is currently pointing to. If you accidentally target the wrong branch, you might unintentionally merge unfinished or experimental changes into a stable branch like <code>main</code>, potentially introducing bugs or breaking the build. For example, imagine merging an in-progress feature branch into <code>main</code> during a product release\u2014this could disrupt the deployment process and create significant headaches for the team.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#step-2-merge-your-feature-branch","title":"Step 2: Merge Your Feature Branch","text":"<p>Next, run the merge command:</p> <pre><code>git merge cool-feature\n</code></pre> <p>If there are no conflicts, Git will combine the changes, and you\u2019re done! \ud83c\udf89</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#handling-merge-conflicts","title":"Handling Merge Conflicts","text":"<p>Sometimes, two branches modify the same part of a file, and Git doesn\u2019t know which version to keep. For example, imagine two developers working on the same function in a file\u2014one optimizes its performance while the other updates its documentation. When these changes are merged, Git identifies a conflict because both developers altered the same section of the file, requiring manual resolution. This is called a merge conflict. When this happens, Git will pause the merge and mark the conflicting sections in your files like this:</p> <pre><code>```plaintext\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nCode from the current branch\n=======\nCode from the branch being merged\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; cool-feature\n```\n</code></pre> <p>To resolve the conflict:</p> <p>(1) Use <code>git status</code> to see which files have conflicts. It will list the files that need attention:</p> <pre><code>git status\n</code></pre> <p>This is especially useful if multiple files are involved.</p> <p>(2) Open each conflicting file and manually edit it to remove the conflict markers and choose the correct content.</p> <p>(3) Add the resolved files:</p> <pre><code>git add &lt;file&gt;\n</code></pre> <p>(4) Complete the merge with a commit:</p> <pre><code>git commit -m \"Resolve merge conflict\"\n</code></pre> <p>If you decide you don\u2019t want to proceed with the merge and want to return to the state before the merge started, you can abort the merge with:</p> <pre><code>git merge --abort\n</code></pre> <p>This will cancel the merge and reset your working directory to the state it was in before the merge began.</p>"},{"location":"threads/tools/git/ch3-git-branch-merge/#cleaning-up-branches","title":"Cleaning Up Branches","text":"<p>Once a branch has been merged, you can delete it to keep your repository tidy:</p> <pre><code>git branch -d cool-feature\n</code></pre> <p>If the branch hasn\u2019t been merged but you still want to delete it, use:</p> <pre><code>git branch -D cool-feature\n</code></pre>"},{"location":"threads/tools/git/ch3-git-branch-merge/#modern-git-why-use-git-switch-rather-than-git-checkout","title":"Modern Git: Why use <code>git switch</code> rather than <code>git checkout</code>?","text":"<p>Many tutorials and older users of <code>git</code> will use <code>checkout</code> where you are learning to use <code>switch</code>. Why?</p> <p><code>git switch</code> was introduced in Git version 2.23 (August 2019) as part of an effort to make Git\u2019s commands more user-friendly and less ambiguous. Historically, <code>git checkout</code> handled many different tasks, from switching branches to checking out individual files or commits. This multitasking nature often led to confusion for new users and even experienced developers.</p> <p>By separating branch-related operations (<code>git switch</code>) from other tasks like checking out specific files or commits (<code>git checkout</code>), Git improved usability and reduced the likelihood of mistakes. </p> <p>For most branching tasks, <code>git switch</code> is the modern and preferred choice. It simplifies workflows and makes commands more intuitive for beginners and teams alike.</p> <ul> <li>Use <code>git switch</code> for creating or moving between branches. It\u2019s explicit and avoids accidentally losing work or entering a detached <code>HEAD</code> state.</li> <li>Use <code>git checkout</code> when you need to:<ul> <li>Recover a specific file from a previous commit:   <pre><code>git checkout &lt;commit-hash&gt; -- &lt;file&gt;\n</code></pre></li> <li>Temporarily view or test a specific commit without creating a new branch (a detached HEAD state):   <pre><code>git checkout &lt;commit-hash&gt;\n</code></pre></li> </ul> </li> </ul>"},{"location":"threads/tools/git/ch3-git-branch-merge/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>A Git branch is a lightweight pointer to a commit. Your current working branch updates to point to new commits added to the branch.</li> <li>Use branches to isolate work, experiment, and collaborate without impacting other branches.</li> <li>Create branches early and often; they\u2019re fast, lightweight, and encourage clean workflows.</li> <li>HEAD points to your current working branch or commit and guides Git commands.</li> <li>Use git switch to create or move between branches; it\u2019s modern and more intuitive than git checkout.</li> <li>Branches can be merged either by fast-forwarding (when no divergence) or merge commits (when histories diverge).</li> <li>Resolve merge conflicts manually by editing files, staging changes, and committing resolutions.</li> <li>Delete merged branches with git branch -d to keep your repository clean.</li> <li>Commit, stash, or discard current changes in your working directory before switching branches to avoid conflicts or lost work.</li> </ul>"},{"location":"threads/tools/git/ch3-git-branch-merge/#subcommands-covered","title":"Subcommands Covered","text":"<ul> <li><code>git branch</code>: Create, list, or delete branches; the core tool for managing branches.  </li> <li><code>git branch -d</code>: Safely delete branches that have been merged. <code>-D</code> is the unsafe variant.</li> <li><code>git switch</code>: Switch between branches or create and switch in one step with <code>--create</code>.  </li> <li><code>git merge</code>: Combine changes from one branch into another, creating a unified history.  </li> <li><code>git merge --abort</code>: Cancel a merge when there are conflicts and return to the pre-merge conflict state.</li> <li><code>git log --graph --oneline</code>: Visualize commit history with branch relationships.</li> <li><code>git stash</code>: Temporarily stash away changes to focus on other tasks or branches.  </li> <li><code>git stash pop</code>: Recover stashed changes to the current working directory.</li> </ul>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/","title":"Ch. 4 Git Collaboration: Working with Remote Repositories","text":"<p>In today's software development world, writing code is rarely a solo endeavor. Whether you're contributing to open source projects, working on a team assignment, or collaborating with others across the globe, you'll need to share your code and coordinate with other developers. This is where Git's collaboration features become essential - they're the foundation of modern software development practices.</p> <p>This tutorial will help you master the core concepts and commands needed for effective collaboration using Git. By the end, you will:</p> <ul> <li>Understand how Git connects and synchronizes with other repositories</li> <li>Be able to share your code with teammates and pull in their latest changes</li> <li>Know how to handle common collaboration scenarios and resolve conflicts</li> <li>Be ready to participate effectively in team projects</li> </ul>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#understanding-remotes-your-gateway-to-collaboration","title":"Understanding Remotes: Your Gateway to Collaboration","text":"<p>Think of your local Git repository as your personal workspace - it's where you write code, make commits, and track changes. But how do you share these changes with your teammates? That's where remote repositories come in. A remote repository acts like a shared storage space that everyone on your team can access. It's similar to how cloud storage lets multiple people share and sync files, but with special features designed for code collaboration.</p> <p>What's a Remote Repository?</p> <p>A remote repository is a version of your project hosted on the internet or a network. It serves as the central reference point that all team members use to share and synchronize their work. Common hosting platforms include GitHub, GitLab, and Bitbucket.</p> <p>Let's start by connecting to a remote repository. We'll use GitHub as our example, but these concepts work the same way with any Git hosting service:</p> <pre><code>git remote add origin https://github.com/username/project.git # (1)!\n</code></pre> <ol> <li>Create a new connection named \"origin\" pointing to your GitHub repository</li> </ol> <p>We're using \"origin\" as the name for our remote repository here. While you can choose any name you like, \"origin\" is the conventional choice for your primary remote repository - think of it as the source of truth for your project.</p> <p>You can manage your remote connections with these helpful commands:</p> <pre><code>git remote --verbose # (1)!\ngit remote show origin # (2)!\n</code></pre> <ol> <li>List all remote repositories with their URLs</li> <li>Display detailed information about the \"origin\" remote</li> </ol> <p>Command Shorthand</p> <p>You can use <code>git remote -v</code> as a shorter version of the verbose flag.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#https-vs-ssh-remote-urls","title":"HTTPS vs SSH Remote URLs","text":"<p>When adding a remote, you have two main options for how to connect: HTTPS or SSH.</p> <pre><code># HTTPS style remote\ngit remote add origin https://github.com/username/repository.git\n\n# SSH style remote\ngit remote add origin git@github.com:username/repository.git\n</code></pre> <p>HTTPS (Hypertext Transfer Protocol Secure) is the protocol that powers secure websites and is GitHub's recommended approach for repository connections. SSH (Secure Shell) is a protocol designed for secure remote access to systems.</p> <p>Both protocols can be set up to remember your credentials, so you won't need to enter them each time you interact with GitHub. With HTTPS, this means configuring a credential manager to store your personal access token. With SSH, you'll set up a key pair that allows for automatic authentication.</p> <p>For most developers, HTTPS is the better choice. It works reliably across different networks and firewall configurations, and GitHub's tooling is optimized for HTTPS connections. Consider SSH as an alternative only if you have specific requirements that HTTPS doesn't meet, or if you're already comfortable managing SSH keys.</p> <p>Authentication Deep Dive</p> <p>GitHub provides comprehensive documentation about authentication methods, security best practices, and troubleshooting tips in their authentication guide. You'll find detailed instructions for setting up both HTTPS and SSH authentication, including how to create and manage access tokens, configure SSH keys, and use two-factor authentication for additional security.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#fetching-preview-changes-before-merging","title":"Fetching: Preview Changes Before Merging","text":"<p>Before you integrate changes from your teammates, it's often helpful to see what they've done first. That's where fetching comes in - it lets you download and inspect changes without immediately applying them to your work. Think of it like previewing changes in a Google Doc before accepting them.</p> <pre><code>git fetch --all # (1)!\n</code></pre> <ol> <li>Download all branches and commits from every configured remote</li> </ol> <p>Command Shorthand</p> <p>The shorter version is <code>git fetch -a</code> once you're comfortable with the operation.</p> <p>When you fetch, Git downloads all new commits but doesn't modify your working files or local branches. This is particularly useful when you want to:</p> <ul> <li>Review your teammates' changes before incorporating them</li> <li>Check if there are any potential conflicts with your work</li> <li>Work with multiple feature branches from different team members</li> </ul> <p>For example, if your teammate pushed changes to a branch named <code>feature-login</code>, here's how you can examine their work:</p> <pre><code>git fetch origin # (1)!\ngit switch feature-login # (2)!\n</code></pre> <ol> <li>Download the latest changes from the remote repository</li> <li>Create and switch to a local branch tracking the remote feature-login branch</li> </ol> <p>Let's look at how this works using a sequence diagram. In sequence diagrams, you see a timeline of how operations unfold between different participants (in our case, different repositories). Each vertical line represents a different participant, and each entry on that line happens in sequence over time as you read from top to bottom. These diagrams are a common tool in software engineering for understanding how different parts of a system interact.</p> <pre><code>sequenceDiagram\n    participant L as Local Repository\n    participant R as Remote Repository\n    participant T as Teammate (feature-login)\n\n    Note over L,R: No feature-login branch locally\n    T-&gt;&gt;T: Creates commits A, B on feature-login\n    T-&gt;&gt;R: Pushes feature-login\n    Note over R: Remote has feature-login at B\n    L-&gt;&gt;R: git fetch origin\n    R--&gt;&gt;L: Updates origin/feature-login to B\n    Note over L: Creates origin/feature-login\n    L-&gt;&gt;L: git switch feature-login\n    Note over L: Creates and switches to&lt;br/&gt;local feature-login tracking&lt;br/&gt;origin/feature-login</code></pre> <p>After the <code>switch</code> command, you are now working in a local branch named <code>feature-login</code> which tracks <code>origin/feature-login</code>. Since it's just a branch, if you think your teammate's work looks good and you'd like to merge it into your project, you can switch back to whatever branch you'd like and, just like any other branch, merge it in!</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#understanding-remote-branches","title":"Understanding Remote Branches","text":"<p>Git uses a special naming system to keep track of branches that exist in remote repositories. Remote branches are prefixed with \"remotes/origin/\" to distinguish them from your local branches. You can see all remote branches using:</p> <pre><code>git branch --remotes # (1)!\n</code></pre> <ol> <li>Display all remote branches known to your local repository</li> </ol> <p>The output might look something like this:</p> <pre><code>  remotes/origin/main\n  remotes/origin/feature-login\n  remotes/origin/bugfix-header\n  remotes/origin/documentation\n  remotes/origin/HEAD -&gt; origin/main\n</code></pre> <p>Remote References</p> <p>Remote branch names can be used in any Git command that accepts a branch name. For example: <code>git diff main origin/main</code> shows differences between your local and remote main branches.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#pulling-combining-fetch-and-merge","title":"Pulling: Combining Fetch and Merge","text":"<ol> <li>Switch to your main branch</li> <li>Download and integrate the latest changes from the remote main branch</li> </ol> <p>Check Your Branch</p> <p>Always verify which branch you're on before pulling. Pulling while on the wrong branch means you'll merge remote changes into the wrong place in your repository.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#pulling-when-remote-branches-havent-diverged","title":"Pulling When Remote Branches Haven't Diverged","text":"<p>If you haven't made any new commits since your last synchronization with the remote repository, pulling performs a fast-forward merge, just like you've seen before when merging branches. The remote branch <code>origin/main</code> is treated exactly like any other branch that has new commits \u2013 Git simply moves your branch pointer forward to match it.</p> <pre><code>sequenceDiagram\n    participant L as Local Repository (main)\n    participant R as Remote Repository (main)\n    participant T as Teammate (main)\n\n    Note over L,T: All main branches at commit B\n    T-&gt;&gt;T: Creates commits C, D on main\n    T-&gt;&gt;R: Pushes main\n    Note over R: Remote main now at D\n    L-&gt;&gt;R: git pull\n    R--&gt;&gt;L: Fetch updates origin/main to D\n    Note over L: Fast-forward local main&lt;br/&gt;from B to D to match origin/main</code></pre>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#pulling-when-remote-branches-have-diverged","title":"Pulling When Remote Branches Have Diverged","text":"<p>If you've made local commits while others have pushed to the remote, your branch and <code>origin/main</code> have diverged \u2013 a situation you're familiar with from merging branches. Just as before, Git needs to create a merge commit to combine these parallel lines of development.</p> <p>Here's the corresponding sequence diagram:</p> <pre><code>sequenceDiagram\n    participant L as Local Repository (main)\n    participant R as Remote Repository (main)\n    participant T as Teammate (main)\n\n    Note over L,T: All main branches at commit B\n    T-&gt;&gt;T: Creates commits C, D on main\n    T-&gt;&gt;R: Pushes main\n    Note over R: Remote main now at D\n    L-&gt;&gt;L: Creates commits E, F on main\n    Note over L: Local main now at F\n    L-&gt;&gt;R: git pull\n    R--&gt;&gt;L: Fetch updates origin/main to D\n    Note over L: Create merge commit G combining&lt;br/&gt;local main (F) and origin/main (D)</code></pre> <p>You might need to resolve conflicts here, just as you would when merging any other diverged branches. Remember that <code>origin/main</code> is just Git's way of tracking what's on the remote repository \u2013 once you have fetched a remote branch, it's just a local branch with a different naming convention and, as such, it behaves exactly like any other branch when it comes to merging.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#cloning-starting-with-an-existing-project","title":"Cloning: Starting with an Existing Project","text":"<p>You've likely started many projects by cloning an existing repository. Now that you understand how Git manages remote repositories, let's explore what actually happens during the cloning process. Cloning is essentially an automated combination of several operations you've just learned about, packaged into a single command for convenience.</p> <p>When you run <code>git clone</code>, Git performs these steps in sequence:</p> <ol> <li>Creates a new directory for your project</li> <li>Initializes a fresh Git repository inside it</li> <li>Adds a remote named \"origin\" pointing to the URL you're cloning from</li> <li>Fetches all branches and history from that remote</li> <li>Sets up tracking relationships between local and remote branches</li> <li>Checks out the default branch (usually <code>main</code>) as your working copy</li> </ol> <p>Let's see this in action:</p> <pre><code>git clone https://github.com/username/project.git # (1)!\n</code></pre> <ol> <li>Download the repository and set up a local copy with remote tracking</li> </ol> <p>This single command accomplishes what would otherwise require several manual steps:</p> <pre><code>mkdir project\ncd project\ngit init\ngit remote add origin https://github.com/username/project.git\ngit fetch origin\ngit switch main\n</code></pre>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#pushing-sharing-your-work","title":"Pushing: Sharing Your Work","text":"<p>Now that you understand how to get changes from your teammates through pulling, let's look at how to share your work with them through pushing. When you push your commits, you're asking the remote repository to integrate your changes into its history.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#setting-up-branch-tracking","title":"Setting Up Branch Tracking","text":"<p>When you're working on a completely new feature or bug fix, you'll often start by creating a local branch. Until you share this branch with others, it exists only in your local repository \u2013 the remote repository doesn't know anything about it yet. In these cases, when you push the branch for the first time, you need to tell Git two things:</p> <ol> <li>Which remote repository should store this branch (usually \"origin\")</li> <li>What name the branch should have on the remote (usually the same as your local branch name)</li> </ol> <p>This is what the <code>--set-upstream</code> flag (or <code>-u</code> for short) does in the following command:</p> <pre><code>git push --set-upstream origin feature-awesome # (1)!\n</code></pre> <ol> <li>Upload your new branch and configure it to track a branch of the same name on the remote</li> </ol> <p>This command creates a connection, or \"tracking relationship,\" between your local <code>feature-awesome</code> branch and a new branch of the same name on the remote repository. This tracking relationship is important \u2013 once established, it lets you use simple <code>git push</code> and <code>git pull</code> commands without having to specify the remote and branch names each time.</p> <p>Local vs Remote Branches</p> <p>This is different from when you <code>fetch</code> and <code>switch</code> to a branch that your teammate has already shared. In that case, Git automatically sets up the tracking relationship for you because it knows the branch came from the remote repository. As such, you can push new commits to the teammate's branch without needing to use <code>--set-upstream</code> the first time.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#creating-new-remote-branches","title":"Creating New Remote Branches","text":"<p>When you push a branch that doesn't exist on the remote repository yet, Git will create it for you.</p> <pre><code>sequenceDiagram\n    participant L as Local Repository (feature)\n    participant R as Remote Repository\n    participant T as Teammate\n\n    Note over L,R: No feature branch exists on remote\n    L-&gt;&gt;L: Creates commits A, B on feature\n    L-&gt;&gt;R: git push -u origin feature\n    Note over R: Creates feature branch&lt;br/&gt;with commits A, B</code></pre> <p>You'll encounter this scenario frequently when starting new work. For example, when you create a new feature branch locally and want to share it with your team, or when you're starting a new bug fix and want to get early feedback.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#pushing-to-existing-branches","title":"Pushing to Existing Branches","text":"<p>When pushing to a branch that already exists on the remote, Git checks if your changes can be merged cleanly thanks to a linear commit history. If no one has pushed new changes to the remote branch since you last pulled, Git can perform a fast-forward merge on the remote repository \u2013 just like the fast-forward merges you're familiar with locally. Your commits are simply added to the end of the remote branch's history:</p> <pre><code>sequenceDiagram\n    participant L as Local Repository (main)\n    participant R as Remote Repository (main)\n    participant T as Teammate (main)\n\n    Note over L,T: All main branches at commit B\n    L-&gt;&gt;L: Creates commits C, D on main\n    L-&gt;&gt;R: git push\n    Note over R: Fast-forward remote main&lt;br/&gt;from B to D</code></pre>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#handling-push-rejections","title":"Handling Push Rejections","text":"<p>Sometimes your push will be rejected because the remote branch has new commits that you don't have locally. This is Git's way of preventing you from accidentally overwriting your teammates' work. When this happens, you need to merge their changes in to your local repository's branch before you can push yours:</p> <pre><code>sequenceDiagram\n    participant L as Local Repository (main)\n    participant R as Remote Repository (main)\n    participant T as Teammate (main)\n\n    Note over L,T: All main branches at commit B\n    L-&gt;&gt;L: Creates commits C, D on main\n    T-&gt;&gt;T: Creates commits E, F on main\n    T-&gt;&gt;R: Pushes main\n    Note over R: Remote main now at F\n    L-&gt;&gt;R: Attempts to push\n    R--&gt;&gt;L: Rejects push (not up to date)\n    L-&gt;&gt;R: git pull\n    R--&gt;&gt;L: Fetch and Merge origin/main\n    Note over L: Create merge commit G combining&lt;br/&gt;local main (D) and origin/main (F)\n    L-&gt;&gt;R: git push\n    Note over R: Uploads commits C, D, G&lt;br/&gt;Fast-forward remote main to G</code></pre> <p>This sequence should look familiar \u2013 it's the same divergent history situation you saw with pulling, just in reverse. Here's what's happening:</p> <ol> <li>The remote rejects your push because accepting it would lose your teammate's work</li> <li>You pull to get your teammate's changes, creating a merge commit</li> <li>Now your history contains everything from the remote, so your push can be accepted</li> </ol> <p>Remember that after the pull creates a merge commit, your subsequent push will be accepted because it's effectively a fast-forward merge from the remote's perspective \u2013 all the commits it has are already part of your history.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#best-practices-for-working-with-remote-repositories","title":"Best Practices for Working with Remote Repositories","text":"<p>Let's discuss some essential habits that will help you collaborate effectively with your team when working with remote repositories.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#fetch-early-and-often","title":"Fetch Early and Often","text":"<p>Make a habit of frequently fetching from your remote repository, just as you regularly save your work. Fetching is safe \u2013 it won't modify your working directory or local branches \u2013 and it helps you stay aware of what your teammates are doing. Many developers start their day by fetching the latest changes and do so again before starting any significant new work. This practice helps them spot potential conflicts early and stay synchronized with their team.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#pull-before-you-push","title":"Pull Before You Push","text":"<p>Always pull before pushing to a shared branch, even if you think your local branch is up to date. This habit ensures you're working with the latest code and helps prevent push rejections. When working on long-running feature branches, it's also good practice to regularly pull from the main branch to keep your feature branch up to date. This prevents your branch from diverging too far from the main line of development.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#communicate-branch-intent","title":"Communicate Branch Intent","text":"<p>When pushing a new branch to the remote repository, use clear, descriptive names that communicate the branch's purpose. A good branch name tells your teammates what work is being done without them having to examine the code. Many teams use prefixes to categorize the type of work and include ticket numbers for tracking:</p> <pre><code>feature-user-authentication\nfeature-issue-123-email-notifications\nbugfix-login-timeout\nbugfix-456-memory-leak\nwip-oauth-integration\n</code></pre> <p>These descriptive names help your team understand at a glance what changes to expect when reviewing your code.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#handle-conflicts-promptly","title":"Handle Conflicts Promptly","text":"<p>If you encounter conflicts during a pull, address them right away while the changes are fresh in your mind. The longer you wait, the harder it becomes to remember the context of the changes and make good decisions about how to resolve the conflicts.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#push-thoughtfully","title":"Push Thoughtfully","text":"<p>While it's important to share your work regularly, avoid pushing broken or incomplete code to shared branches. Before pushing, make sure your changes compile, pass tests, and include any new files your changes depend on. This is especially important for branches that others actively use, like the main branch.</p>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#summary-and-key-points","title":"Summary and Key Points","text":"<p>Working with remote repositories in Git is all about effective collaboration. Remember:</p> <ul> <li>Remote repositories are shared spaces for code collaboration</li> <li>Fetch lets you preview changes before integrating them</li> <li>Pull combines fetching and merging in one step</li> <li>Push shares your work with the team</li> </ul>"},{"location":"threads/tools/git/ch4-git-remote-fetch-push-pull/#essential-commands-reference","title":"Essential Commands Reference","text":"<pre><code>git remote add &lt;name&gt; &lt;url&gt;    # (1)!\ngit fetch -a                   # (2)!\ngit switch &lt;branch&gt;            # (3)!\ngit pull                       # (4)!\ngit push -u origin &lt;branch&gt;    # (5)!\n</code></pre> <ol> <li>Connect to a remote repository</li> <li>Download all remote changes</li> <li>Switch to or create a branch</li> <li>Download and integrate remote changes</li> <li>Upload your changes and set tracking</li> </ol> <p>Further Reading</p> <ul> <li>Git Remote Documentation</li> <li>GitHub Authentication Guide</li> <li>Pro Git Book: Working with Remotes</li> </ul>"},{"location":"threads/tools/git/exercise/","title":"Collaborating with <code>git</code> on a <code>MkDocs</code> Project","text":"<p>In this tutorial, you will work to create </p>"},{"location":"threads/tools/git/exercise/#how-do-collaborative-apps-address-who-can-do-what","title":"How do collaborative apps address \"Who can do what?\"","text":"<p>Think about how you engage with your favorite apps daily. When you post pictures on Instagram, you control who sees them\u2014perhaps just your close friends or maybe everyone. When collaborating on a Google Docs project, you decide whether classmates can edit or just comment. And within your Google Drive, some folders might be shared with your study group, while others remain private. These everyday interactions with social media and collaboration tools are your practical introduction to three pivotal software engineering concepts: authentication, authorization, and access control.</p> <p>As you dive into building your software projects or integrate into development teams, you will encounter these patterns repeatedly. Whether crafting a straightforward web application or developing complex enterprise software, pivotal questions arise: \"How do users prove who they are?\", \"What permissions should different users have?\", and \"How are these permissions enforced consistently?\" Fortunately, the answers to these questions are built on established terminology and conceptual frameworks developed over many decades of software engineering projects.</p>"},{"location":"threads/tools/git/exercise/#authentication","title":"Authentication","text":"<p>Authentication is the process of verifying the identity of a user, system, or entity attempting to access a resource. This verification process involves validating one or more authentication factors, which typically fall into three categories:</p> <ul> <li>Something you know (like passwords or PINs)</li> <li>Something you have (like security tokens or smart cards)</li> <li>Something you are (like fingerprints or facial features)</li> </ul> <p>The authentication process generates a digital identity that the system can then use for subsequent interactions. For example, when you log into a system, an authentication process may create a session token that represents your verified identity.</p>"},{"location":"threads/tools/git/exercise/#authorization-and-access-control","title":"Authorization and Access Control","text":"<p>Authorization is the process of determining whether an authenticated identity has permission to perform specific actions or access particular resources. Authorization defines what an authenticated user can do within a system by evaluating their privileges against a set of rules or policies. Authorization always occurs after authentication - a system must know who you are before it can determine what you're allowed to do.</p> <p>Access Control is the implementation mechanism that enforces authorization decisions and protects resources from unauthorized access. While authorization defines the rules about who can do what, access control provides the technical mechanisms to enforce these rules. Access control systems implement various models such as:</p> <ul> <li>Role-Based Access Control (RBAC): Groups permissions into roles and assigns users to appropriate roles</li> <li>Attribute-Based Access Control (ABAC): Makes access decisions based on attributes of users, resources, and context</li> <li>Mandatory Access Control (MAC): Enforces system-wide policies that users cannot modify</li> <li>Discretionary Access Control (DAC): Allows resource owners to control access to their resources</li> </ul> <p>The Relationship Chain These three concepts work together in a sequential chain:</p> <p>Authentication verifies identity (\"Who are you?\") Authorization determines permissions (\"What are you allowed to do?\") Access Control enforces those permissions (\"Here's how we'll make sure you only do what you're allowed to do\")</p> <p>For example, when you attempt to access a secure file:</p> <ol> <li>The system first authenticates you by validating your login credentials</li> <li>Once authenticated, the authorization system checks your permissions to determine if you should have access to take an action on a given resource</li> <li>If authorized, the access control mechanism enforces this decision by actually allowing or blocking the file access attempt</li> </ol>"},{"location":"threads/tools/git/git-common-fixes/","title":"Git Collaboration: Working with Remote Repositories","text":"<p>Common fixes / rewrites:</p> <p>Amending a commit (changing a commit, adding a file to it)</p> <p>Rebasing a branch</p> <p>Pulling with rebase</p>"},{"location":"threads/tools/ide/","title":"Integrated Development Environment","text":"Tasks Topic TK00 Docker, VSCode/DevContainers, Copilot, and GitHub Education Setup"},{"location":"threads/tools/testing/","title":"Automated Testing Tools","text":"Lessons Topic LS04 Testing Foundations: From Requirements to Automation"}]}